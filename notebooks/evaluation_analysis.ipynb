{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83879783",
   "metadata": {},
   "source": [
    "# Evaluation Analysis\n",
    "\n",
    "This notebook analyses the results produced by RegressionScholar. It loads `evaluation_results.json` and produces publication-quality figures and tables for retrieval and generation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b78f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Optional interactive plotting\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except Exception:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "\n",
    "DATA_DIR = Path('')\n",
    "RESULTS_FILE = DATA_DIR / 'evaluation_results.json'\n",
    "\n",
    "if not RESULTS_FILE.exists():\n",
    "    raise FileNotFoundError(f'{RESULTS_FILE} not found. Run src/evaluation.py first.')\n",
    "\n",
    "with open(RESULTS_FILE, 'r', encoding='utf-8') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Normalize retrieval results into a DataFrame\n",
    "retrieval = results.get('retrieval', {})\n",
    "retrieval_rows = []\n",
    "for k_key, metrics in retrieval.items():\n",
    "    k = int(k_key.split('=')[-1])\n",
    "    retrieval_rows.append({'k': k, 'avg_recall': metrics['avg_recall'], 'avg_precision': metrics['avg_precision'], 'avg_mrr': metrics['avg_mrr'], 'num_queries': metrics['num_queries']})\n",
    "retrieval_df = pd.DataFrame(retrieval_rows).sort_values('k')\n",
    "\n",
    "# Generation results as DataFrame\n",
    "generation = results.get('generation', [])\n",
    "if generation:\n",
    "    gen_df = pd.DataFrame(generation)\n",
    "else:\n",
    "    gen_df = pd.DataFrame(columns=['question_id', 'question', 'concept_coverage', 'has_citations', 'word_count'])\n",
    "\n",
    "print('Loaded results:')\n",
    "print(f\"Retrieval entries: {len(retrieval_df)}\")\n",
    "print(f\"Generation entries: {len(gen_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516a257a",
   "metadata": {},
   "source": [
    "## Retrieval Analysis\n",
    "\n",
    "Plot recall, precision and MRR across k values and examine trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot retrieval metrics\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "sns.lineplot(data=retrieval_df, x='k', y='avg_recall', marker='o', ax=ax[0])\n",
    "ax[0].set_title('Average Recall vs k')\n",
    "ax[0].set_xlabel('k')\n",
    "ax[0].set_ylabel('Average Recall')\n",
    "\n",
    "sns.lineplot(data=retrieval_df, x='k', y='avg_precision', marker='o', ax=ax[1])\n",
    "ax[1].set_title('Average Precision vs k')\n",
    "ax[1].set_xlabel('k')\n",
    "ax[1].set_ylabel('Average Precision')\n",
    "\n",
    "sns.lineplot(data=retrieval_df, x='k', y='avg_mrr', marker='o', ax=ax[2])\n",
    "ax[2].set_title('Average MRR vs k')\n",
    "ax[2].set_xlabel('k')\n",
    "ax[2].set_ylabel('Average MRR')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall vs Precision trade-off plot (merged view)\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.scatterplot(data=retrieval_df, x='avg_precision', y='avg_recall', hue='k', s=100, palette='tab10', ax=ax)\n",
    "for _, r in retrieval_df.iterrows():\n",
    "    ax.text(r['avg_precision'], r['avg_recall'], str(int(r['k'])), fontsize=9, ha='right')\n",
    "ax.set_title('Recall vs Precision (annotated by k)')\n",
    "ax.set_xlabel('Average Precision')\n",
    "ax.set_ylabel('Average Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa39609",
   "metadata": {},
   "source": [
    "### Retrieval: best/worst queries\n",
    "Identify queries with the best and worst retrieval results (requires the original per-query metrics in `evaluation_results.json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the results include per-query data, try to surface best/worst retrievals\n",
    "# generation array may contain question-level metrics; attempt to sort by concept_coverage or other fields\n",
    "if not gen_df.empty:\n",
    "    best = gen_df.sort_values('concept_coverage', ascending=False).head(3)\n",
    "    worst = gen_df.sort_values('concept_coverage', ascending=True).head(3)\n",
    "    display(best[['question_id', 'question', 'concept_coverage', 'has_citations']])\n",
    "    display(worst[['question_id', 'question', 'concept_coverage', 'has_citations']])\n",
    "else:\n",
    "    print('No generation details available to rank questions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3780d2b",
   "metadata": {},
   "source": [
    "## Generation Analysis\n",
    "\n",
    "Analyse concept coverage, citation rate and answer length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e50c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept coverage by category\n",
    "if not gen_df.empty:\n",
    "    # If category is present, group by it; otherwise plot overall distribution\n",
    "    if 'category' in gen_df.columns:\n",
    "        cov = gen_df.groupby('category')['concept_coverage'].mean().reset_index()\n",
    "        fig, ax = plt.subplots(figsize=(8,4))\n",
    "        sns.barplot(data=cov, x='category', y='concept_coverage', palette='muted', ax=ax)\n",
    "        ax.set_title('Average Concept Coverage by Question Category')\n",
    "        ax.set_ylabel('Concept Coverage')\n",
    "        ax.set_ylim(0,1)\n",
    "        plt.xticks(rotation=30)\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(6,4))\n",
    "        sns.histplot(x=gen_df['concept_coverage'], bins=20, kde=True, ax=ax)\n",
    "        ax.set_title('Concept Coverage Distribution')\n",
    "        plt.show()\n",
    "\n",
    "    # Word count distribution\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    sns.histplot(x=gen_df['word_count'], bins=30, kde=True, ax=ax)\n",
    "    ax.set_title('Answer Length (Word Count) Distribution')\n",
    "    plt.show()\n",
    "\n",
    "    # Citation rate\n",
    "    citation_rate = gen_df['has_citations'].mean()\n",
    "    print(f'Citation rate: {citation_rate:.2%}')\n",
    "else:\n",
    "    print('No generation results to analyse.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3dcd6",
   "metadata": {},
   "source": [
    "## Deep-dive Examples\n",
    "\n",
    "Show a few best and worst answers with analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples (best/worst)\n",
    "if not gen_df.empty:\n",
    "    try:\n",
    "        print('\\n' + '='*80)\n",
    "        print('BEST PERFORMING EXAMPLES')\n",
    "        print('='*80)\n",
    "        for _, row in best.iterrows():\n",
    "            print('\\n' + '-'*80)\n",
    "            print(f\"Question ID: {row['question_id']}\")\n",
    "            print(f\"Question: {row['question']}\")\n",
    "            print(f\"Concept coverage: {row['concept_coverage']:.2%}\")\n",
    "            print(f\"\\nAnswer preview:\")\n",
    "            print(row.get('answer_preview', 'No preview available'))\n",
    "        \n",
    "        print('\\n' + '='*80)\n",
    "        print('WORST PERFORMING EXAMPLES')\n",
    "        print('='*80)\n",
    "        for _, row in worst.iterrows():\n",
    "            print('\\n' + '-'*80)\n",
    "            print(f\"Question ID: {row['question_id']}\")\n",
    "            print(f\"Question: {row['question']}\")\n",
    "            print(f\"Concept coverage: {row['concept_coverage']:.2%}\")\n",
    "            print(f\"\\nAnswer preview:\")\n",
    "            print(row.get('answer_preview', 'No preview available'))\n",
    "            \n",
    "    except NameError:\n",
    "        print('Best/worst examples not available; ensure generation analysis ran and produced metrics.')\n",
    "else:\n",
    "    print('No generation examples to show.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d46fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5855ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
