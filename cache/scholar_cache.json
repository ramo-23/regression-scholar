{
  "What is ridge regression?": {
    "answer": "Ridge regression is a statistical learning technique used to estimate regression coefficients, particularly when dealing with issues such as multicollinearity or when the number of covariates ($p$) is large relative to the number of observations ($n$), which can lead to an ill-conditioned or singular $X^TX$ matrix in ordinary least squares (OLS) [1]. It provides an alternative estimator to the OLS estimator, $\\hat{\\beta}$ [1].\n\n### Mathematical Formulations\n\nThe ridge regression estimator, denoted $\\hat{\\beta}_\\lambda$, is mathematically defined as:\n$$ \\hat{\\beta}_\\lambda = (X^TX + \\lambda I_p)^{-1}X^Ty $$\nwhere $X$ is the design matrix, $y$ is the response vector, $I_p$ is the $p \\times p$ identity matrix, and $\\lambda > 0$ is the ridge parameter [1].\n\nThis estimator can be characterized in several equivalent ways:\n\n1.  **Penalized Least-Squares Estimator (L2 Regularization):** The ridge estimator is equivalent to the penalized least-squares estimator, which minimizes the sum of squared residuals subject to an L2 penalty on the coefficient magnitudes:\n    $$ \\hat{\\beta}_\\lambda = \\arg \\min_{\\beta} \\|y - X\\beta\\|^2 + \\lambda \\sum_{k=1}^p \\beta_k^2 $$\n    Here, $\\sum_{k=1}^p \\beta_k^2$ represents the squared L2 norm of the coefficients, $\\|\\beta\\|_2^2$, acting as a **regularization** term [1, 2].\n\n2.  **Constrained Estimation Problem:** Ridge regression can also be interpreted as a constrained estimation problem. It finds the coefficients $\\beta$ that minimize the sum-of-squares criterion, subject to a constraint on the L2 norm of the coefficients:\n    $$ \\hat{\\beta}_\\lambda = \\arg \\min_{\\beta} \\|y - X\\beta\\|^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_2^2 \\le c $$\n    for some constant $c > 0$. The ridge estimate is found at the intersection of the smallest sum-of-squares contour and a circular (or spherical in higher dimensions) constraint centered at the origin, representing $\\{\\beta : \\|\\beta\\|_2^2 \\le c\\}$ [2]. The estimator is always on the boundary of this constraint and never an interior point [2].\n\n### Key Characteristics and Properties\n\n*   **Addressing Ill-Conditioning and Singularity:** Ridge regression addresses the inability of the OLS estimator to handle ill-conditioned or singular $X^TX$ matrices by adding $\\lambda I_p$ to $X^TX$. This ensures that $(X^TX + \\lambda I_p)$ is necessarily non-singular when $\\lambda > 0$, guaranteeing the existence of the inverse and thus the estimator [1]. This is particularly relevant in high-dimensional settings where $p \\ge n$ [1].\n*   **Bias-Variance Trade-off:** Compared to the OLS estimator, the ridge estimator $\\hat{\\beta}_\\lambda$ introduces some bias into the coefficient estimates. In exchange, it achieves lower mean-squared error (MSE($\\hat{\\beta}_\\lambda | \\beta$)) and prediction error (PE($X\\hat{\\beta}_\\lambda | \\beta$)), especially when $X^TX$ is ill-conditioned or singular [1].\n*   **Coefficient Shrinkage:** The ridge parameter $\\lambda > 0$ shrinks the estimate of the coefficients $\\beta$ towards zero. The amount of shrinkage is an increasing function of $\\lambda$ [1]. As $\\lambda$ increases, the size of the spherical ridge parameter constraint shrinks monotonically, eventually collapsing to zero as $\\lambda \\to \\infty$, meaning $\\lim_{\\lambda \\to \\infty} \\|\\hat{\\beta}(\\lambda)\\|_2^2 = 0$ [2].\n*   **Orthogonalized Space Shrinkage:** In an orthogonalized space, the ridge estimator is obtained by shrinking canonical OLS estimates $\\hat{\\alpha}_k$ by factors $\\{d_k^2/(d_k^2 + \\lambda)\\}$, where $d_k^2$ are eigenvalues. This means greater shrinkage is applied to OLS estimates corresponding to relatively small eigenvalues [1].\n*   **Bayesian Interpretation:** From a Bayesian perspective, the ridge estimator $\\hat{\\beta}_\\lambda$ is the mean of the posterior distribution of $\\beta$ under a $p$-variate normal prior distribution, $n_p(\\beta | 0, \\sigma^2\\lambda^{-1}I_p)$, for the conditional random variable $\\beta | \\sigma^2$. It minimizes the posterior expected squared-error loss [1].\n*   **Relationship to OLS:** When $\\lambda = 0$, the ridge estimator $\\hat{\\beta}_\\lambda$ reduces to the OLS estimator $\\hat{\\beta}$ [1].\n*   **Variance Inflation Factor (VIF):** Ridge regression affects the Variance Inflation Factor (VIF), which quantifies the increase in the variance of an estimate due to collinearity [3]. For $\\lambda > 0$ and orthogonal $X$, the ridge VIF, VIF[$\\hat{\\beta}_j(\\lambda)$], becomes less than 1, meaning penalization actively deflates the VIF [3].\n\n### The Ridge Parameter ($\\lambda$)\n\nThe choice of the ridge parameter $\\lambda$ is crucial, as it dictates the quality of coefficient estimates and predictions [1]. Several methods exist for its selection:\n\n*   **Automatic Plug-in Estimation:** The standard Hoerl-Kennard-Baldwin (HKB) plug-in estimator is defined as $\\hat{\\lambda}_{HKB} = p\\hat{\\sigma}^2 / \\hat{\\beta}^T\\hat{\\beta}$. It aims to minimize the model's expected prediction error when $X$ is orthonormal. However, it relies on OLS estimates and does not exist when $X^TX$ is singular. Extensions exist for singular $X^TX$ or when $p \\ge n$ [1]. While quick, it may not be optimal for non-orthonormal $X$ [1].\n*   **Cross-Validation:** This method involves defining a grid of trial $\\lambda$ values. K-fold cross-validation (commonly 10-fold or $n$-fold) is then used to select the $\\hat{\\lambda}$ that provides the smallest prediction error for the ridge regression model. The data is partitioned into K equal-sized sets, and the mean-squared predictive error is measured across all partitions for each trial $\\lambda$ [1].\n*   **Information Criteria Optimization:** Information criteria can also be used for $\\lambda$ selection [1].\n*   **Markov Chain Monte Carlo (MCMC):** MCMC methods are another approach for estimating $\\lambda$ [1].\n\n**Sanity Requirements for $\\lambda$ Selection:**\nTo ensure robust and numerically stable estimates, the choice of $\\lambda$ should be constrained:\n*   **Numerical Accuracy:** $\\lambda$ should not be too close to zero to ensure that the matrix $(X^TX + \\lambda I_p)$ remains well-conditioned (i.e., has a low condition number). A high condition number indicates potential loss of numerical accuracy in the inversion [4].\n*   **Degrees of Freedom:** To prevent overfitting, $\\lambda$ can be chosen such that the degrees of freedom spent on estimating the regression parameter, $df[\\hat{\\beta}(\\lambda)] = tr[H(\\lambda)]$, do not exceed a specified maximum $\\nu$ [4].\n\n### Computational Considerations\n\nThe direct computation of $(X^TX + \\lambda I_p)^{-1}$ can be computationally intensive, especially for large $p$. The Woodbury identity can be applied to re-express the ridge estimator as:\n$$ \\hat{\\beta}(\\lambda) = \\lambda^{-1}X^T(I_n + \\lambda^{-1}XX^T)^{-1}Y $$\nThis formulation replaces the inversion of a $p \\times p$ matrix ($X^TX + \\lambda I_p$) with the inversion of an $n \\times n$ matrix ($I_n + \\lambda^{-1}XX^T$). This is computationally more efficient, particularly when $p > n$, and can avoid numerical errors associated with the singular value decomposition of $X$ [4].\n\n### Practical Implications\n\nRidge regression is a practical remedy for overfitting, especially in high-dimensional data analysis where the number of explanatory variables can exceed the number of observations [2]. Overfitting occurs when the model captures noise rather than just the underlying signal, often indicated by large regression coefficients [2]. By imposing an L2 penalty on the coefficients, ridge regression limits their size, thus preventing the model from becoming too complex and fitting the noise in the data [2]. It is highly relevant in applications where collinearity among covariates is common, such as in gene expression studies where genes work in concert and their expression levels are correlated [3].",
    "chunks": [
      {
        "text": "a result of multicollinearity. For example, multicollinearity can likely occur when the data consist of a large number of covariates (p), or can occur when the data set is constructed by merging multiple data sets that have one or more variables in common. Ridge regression, which deﬁnes the alternative estimator βλ = (X⊺X + λIp)−1X⊺y, pro- vides a solution to the inability of the OLS estimator bβ to handle ill-conditioned or singular X⊺X. Here, λ > 0 is the ridge parameter that shrinks the estimate of the coeﬃcients β towards zero, with the amount of shrinking an increasing function of λ. Compared to the OLS estimator bβ, the ridge estimator βλ introduces some bias in exchange for lower mean-squared MSE(bβλ | β) and prediction error PE(Xbβλ | β), especially when X⊺X is ill- conditioned or singular (Hoerl & Kennard, 1970). However, unlike the OLS estimator, the ridge estimator βλ exists even when X⊺X is singular, because (X⊺X + λIp) is necessarily non-singular when λ > 0. The ridge estimator βλ can be characterized in at least four equivalent ways (e.g., Hastie, et al. 2009). First, the ridge estimator is equivalent to the penalized least-squares estimator, βλ = arg minβ ||y −Xβ||2 + λ Pp k=1 β2 k. Second, in terms of the orthogonalized space, the ridge estimator is given by βλ = Wbαλ, with bαλ,k = {d2 k/(d2 k + λ)}bαk for k = 1, . . . , q. This shows that the ridge estimator βλ is obtained by shrinking the canonical OLS estimator bα = (bα1, . . . , bαq)⊺by the factors {d2 k/(d2 k+λ)} (resp.), and therefore applies a greater amount of shrinkage to OLS estimates bαk having relatively small eigenvalues. Third, from a Bayesian point of view, the ridge estimator βλ is the mean of the posterior distribution of β under a p-variate normal prior distribution, corresponding to normal probability density function (p.d.f.) np(β | 0, σ2λ−1Ip) for the conditional random variable β | σ2. Thus, from a Bayesian decision-theoretic perspective, βλ is the choice of point estimate of β that minimizes the posterior expected squared-error loss. Fourth, when λ = 0, the ridge estimator βλ becomes the OLS estimator bβ. In ridge regression, the quality of coeﬃcient estimates βλ and predictions hinge on the choice of the ridge parameter, λ. As a result, several methods have been proposed to estimate this parameter, based on either automatic plug-in estimation, cross-validation, information criteria optimization, or Markov chain Monte Carlo (MCMC), which we now brieﬂy review. An enormous literature on these methods has developed over the past 45 years, however, the vast majority of them rely on the OLS estimates (bβ, bσ2) (Cule & DeIorio, 2012). In keeping with the general spirit of this paper, we focus our review on ridge methods that can handle data sets where X⊺X is either positive-deﬁnite, ill-conditioned, or singular (including when p ≥n), unless indicated otherwise. The standard, Hoerl-Kennard-Baldwin (HKB) (1975) plug-in estimator for the ridge regression model is deﬁned by bλHKB = pbσ2/bβ ⊺bβ. This estimator is motivated by the fact that λHKB = pσ2/β⊺β is the choice of λ that minimizes the model’s expected prediction error when X is orthonormal (Hoerl & Kennard, 1970). The HKB estimator relies on the OLS estimate bσ2, and therefore does not exist when X⊺X is singular, including when p ≥n. 3 However, this estimator was extended to singular and p ≥n settings (Cule & DeIorio, 2013), by replacing bσ2 with the error variance estimate bσ2 r = 1 n−br||y −Xbαbr||2 obtained from a principal components regression having br ≤q = min(n, p) components with coeﬃcients α = (α1, . . . , αr)⊺. Here, br is the value of r that minimizes r−Pq k=1 d4 k/(d2 k +λr)2. Therefore br best matches the degrees of freedom for variance in the ridge regression model with ridge parameter λbr = brbσ2 br/bα⊺ br bαbr. The HKB plug-in estimator is attractive because it quickly obtains a ridge estimate bλ without iteration. However this estimator may lead to an estimate of λ that is not necessarily optimal for the given data set at hand, especially when X is not orthonormal. The method of cross-validation, for ridge regression, involves ﬁrst specifying a grid of trial values of λ, and then selecting the estimate bλ as the trial value that provides the smallest prediction error for the ridge regression model in K-fold cross-validation, usually 10-fold or n-fold (e.g., Hastie et al. 2009, Ch. 3, 7). The method takes K (at least nearly) equal-sized partitions of the n observations at random, and then for each trial value of λ, measures the model’s overall mean-squared predictive error over all K partitions. Overall mean-squared error is based on obtaining the estimate β (k) λ for one partition and measuring the predictive error",
        "metadata": {
          "paper_title": "Fast Marginal Likelihood Estimation of the Ridge Parameter(s) in Ridge Regression and Generalized Ridge Regression for Big Data",
          "paper_id": "1409.2437v5",
          "authors": "George Karabatsos",
          "section": "introduction",
          "chunk_index": 1
        },
        "id": "chunk_285"
      },
      {
        "text": "fit Figure 1.4: Top panels show examples of convex (left) and nonconvex (right) sets. Middle panels show examples of convex (left) and nonconvex (right) functions. The left bottom panel illustrates the ridge estimation as a constrained estimation problem. The ellipses represent the contours of the ML loss function, with the blue dot at the center the ML estimate. The circle is the ridge parameter constraint. The red dot is the ridge estimate. It is at the intersection of the ridge constraint and the smallest contour with a non-empty intersection with the constraint. The right bottom panel shows the data corresponding to Example 1.7. The grey line represents the ‘true’ relationship, while the black line the fitted one. 20 Ridge regression The second KKT condition (the complementarity condition) requires that ν(∥ˆβ(ν)∥2 2 −c) = 0. If ν = λ and c = ∥ˆβ(λ)∥2 2, the ridge regression estimator β(λ) satisfies both KKT conditions. Hence, both problems have the same solution if c = ∥ˆβ(λ)∥2 2. The constrained estimation interpretation of the ridge regression estimator is illustrated in the left bottom panel of Figure 1.4. It shows the level sets of the sum-of-squares criterion and centered around zero the circular ridge parameter constraint, parametrized by {β : ∥β∥2 2 = β2 1 +β2 2 ≤c} for some c > 0. The ridge regression estimator is then the point where the sum-of-squares’ smallest level set hits the constraint. Exactly at that point the sum- of-squares is minimized over those β’s that live on or inside the constraint. In the high-dimensional setting the ellipsoidal level sets are degenerated. For instance, in the 2-dimensional case of the left bottom panel of Figure 1.4, the ellipsoids would then be lines but the geometric interpretation is unaltered. The ridge regression estimator is always to be found on the boundary of the ridge parameter constraint and is never an interior point. To see this, assume, for simplicity, that the X⊤X matrix is of full rank. The radius of the ridge parameter constraint can then be bounded as follows: ∥ˆβ(λ)∥2 2 = Y⊤X(X⊤X + λIpp)−2X⊤Y ≤Y⊤X(X⊤X)−2X⊤Y = ∥ˆβ ml∥2 2. The inequality in the display above follows from i) X⊤X ≻0 and λIpp ≻0, ii) X⊤X + λIpp ≻X⊤X (due to Lemma 14.2.4 of Harville, 2008), and iii) (X⊤X)−2 ≻(X⊤X + λIpp)−2 (inferring Corollary 7.7.4 of Horn and Johnson, 2012). The ridge regression estimator is thus always on the boundary or in a circular constraint centered around the origin with a radius that is smaller than the size of the maximum likelihood estimator. Moreover, the constrained estimation formulation of the ridge regression estimator then implies that the latter must be on the boundary of the constraint. The size of the spherical ridge parameter constraint shrinks monotonously as λ increases, and eventually, in the λ →∞-limit collapses to zero (as is formalized by Proposition 1.4). Proposition 1.4 The squared norm of the ridge regression estimator satisfies: i) d∥ˆβ(λ)∥2 2/dλ < 0 for λ > 0, ii) limλ→∞∥ˆβ(λ)∥2 2 = 0. Proof. For part i) we need to verify that ∥ˆβ(λ)∥2 2 > ∥ˆβ(λ + h)∥2 2 for h > 0. Hereto substitute the singular value decomposition of the design matrix, X = UxDxV⊤ x , into the display above. Then, after a little algebra, take the derivative with respect to λ, and obtain: d d λ∥ˆβ(λ)∥2 2 = −2 Xn i=1 d2 x,i(d2 x,i + λ)−3(Y⊤Ux,∗,i)2. This is negative for all λ > 0. Indeed, the parameter constraint thus becomes smaller and smaller as λ increases, and so does the size of the estimator. Part ii) follows from limλ→∞ˆβ(λ) = 0p, which has been concluded previously by other means. ■ The relevance of viewing the ridge regression estimator as the solution to a constrained estimation problem be- comes obvious when considering a typical threat to high-dimensional data analysis: overfitting. Overfitting refers to the phenomenon of modelling the noise rather than the signal. In case the true model is parsimonious (few covariates driving the response) and data on many covariates are available, it is likely that a linear combination of all covariates yields a higher likelihood than a combination of the few that are actually related to the response. As only the few covariates related to the response contain the signal, the model involving all covariates then cannot but explain more than the signal alone: it also models the error. Hence, it overfits the data. In high-dimensional settings overfitting is a real threat. The number of explanatory variables exceeds the number of observations. It is thus possible to form a linear combination of the covariates that perfectly explains the response, including the noise. Large estimates of regression coefficients are often an indication of overfitting. Augmentation of the estimation procedure with a constraint on the regression coefficients is a simple remedy to",
        "metadata": {
          "paper_title": "Lecture notes on ridge regression",
          "paper_id": "1509.09169v8",
          "chunk_index": 3,
          "section": "model",
          "authors": "Wessel N. van Wieringen"
        },
        "id": "chunk_226"
      },
      {
        "text": "E[Y⊤X(X⊤X + λIpp)−2X⊤Y] = σ2 tr \b X(X⊤X + λIpp)−2X⊤ + β⊤X⊤X(X⊤X + λIpp)−2X⊤Xβ. In the last step we have used Y ∼N (Xβ, σ2Ipp) and the expectation of the quadratic form of a multivariate random variable ε ∼N (µε, Σε) is E(ε⊤Λ ε) = tr(Λ Σε) + µ⊤ ε Λµε (cf. Mathai and Provost, 1992). The expression for the expectation of the radius of the ridge constraint can now be evaluated for the orthogonal X and the strongly, positively collinear X. It turns out that the latter is larger than the former. This results in a larger ridge constraint. For the larger ridge constraint there is a smaller level set that hits it first. The point of intersection, still on the x = y-line, is now thus closer to β and further from the origin (cf. right panel of Figure 1.8). The resulting estimate is thus larger than that from the orthogonal case. The above needs some attenuation. Among others it depends on: i) the number of covariates in each block, ii) the size of the effects, i.e. regression coefficients of each covariate, and iii) the degree of collinearity. Possibly, there are more factors influencing the behaviour of the ridge regression estimator presented in this subsection. This behaviour of ridge regression is to be understood if a certain (say) clinical outcome is to be predicted from (say) gene expression data. Genes work in concert to fulfil a certain function in the cell. Consequently, one expects their expression levels to be correlated. Indeed, gene expression studies exhibit many co-expressed genes, that is, genes with correlating transcript levels. But also in most other applications with many explanatory variables collinearity will be omnipresent and similar issues are to be considered. 1.9.3 Variance inflation factor The ridge regression estimator was introduced to resolve the undefinedness of its maximum likelihood counterpart in the face of (super)collinearity amoung the explanatory variables. The effect of collinearity on the uncertainty of estimates is often quantified by the Variance Inflation Factor (VIF). The VIF measures the change in the variance of the estimate due to the collinearity. Here we investigate how penalization affects the VIF. This requires a definition of the VIF of the ridge regression estimator. The VIF of the maximum likelihood estimator of the j-th element of the regression parameter is defined as a factor in the following factorization of the variance of ˆβj: Var(ˆβj) = n−1σ2[(X⊤X)−1]jj n−1σ2[Var(Xi,j | Xi,1, . . . , Xi,j−1, Xi,j+1, . . . , Xi,p)]−1 = n−1σ2 Var(Xi,j) · Var(Xi,j) Var(Xi,j | Xi,1, . . . , Xi,j−1, Xi,j+1, . . . , Xi,p) := n−1σ2[Var(Xi,j)]−1VIF(ˆβj), in which it assumed that the Xi,j’s are random and – using the column-wise zero ‘centered-ness’of X – that n−1X⊤X is estimator of their covariance matrix. Moreover, the identity used to arrive at the second line of the display, [(X⊤X)−1]jj = [Var(Xi,j | Xi,1, . . . , Xi,j−1, Xi,j+1, . . . , Xi,p)]−1, originates from Corollary 5.8.1 of Whittaker (1990). Thus, Var(ˆβj) factorizes into σ2[Var(Xi,j)]−1 and the variance inflation factor VIF(ˆβj). When the j-th covariate is orthogonal to the other, i.e. there is no collinearity, then the VIF’s denominator, Var(Xi,j | Xi,1, . . . , Xi,j−1, Xi,j+1, . . . , Xi,p), equals Var(Xi,j). Consequently, VIF(ˆβj) = 1. When there is collinearity among the covariates Var(Xi,j | Xi,1, . . . , Xi,j−1, Xi,j+1, . . . , Xi,p) < Var(Xi,j) and VIF(ˆβj) > 1. The VIF then inflates the variance of the estimator of βj under orthogonality – hence, the name – by a factor attributable to the collinearity. 1.9 Simulations 31 The definition of the VIF needs modification to apply to the ridge regression estimator. In Marquardt (1970) the ‘ridge VIF’ is defined analogously to the above definition of the VIF of the maximum likelihood regression estimator as: Var[ˆβj(λ)] = σ2[(X⊤X + λIpp)−1X⊤X(X⊤X + λIpp)−1]jj = n−1σ2 Var(Xi,j) · nVar(Xi,j)[(X⊤X + λIpp)−1X⊤X(X⊤X + λIpp)−1]jj := n−1σ2[Var(Xi,j)]−1VIF[ˆβj(λ)], where the factorization is forced in line with that of the ‘maximum likelihood VIF’ but lacks a similar interpre- tation. When X is orthogonal, VIF[ˆβj(λ)] = [Var(Xi,j)]2[Var(Xi,j) + λ]−2 < 1 for λ > 0. Penalization then deflates the VIF. An alternative definition of the ‘ridge VIF’ presented by Garc´ıa et al. (2015) for the ‘p = 2’-case, which they motivate from counterintuitive behaviour observed in the ‘ridge VIF’ defined by Marquardt (1970), adopts the Figure 1.9: Contour plots of ‘Marquardt VIFs’ and ‘Garcia VIFs’, left and right columns, respectively. The top panels show these VIFs against the degree of penalization (x-axis) and collinearity (y-axis) for a fixed sample size (n = 100) and dimension (p = 50). The bottom panels show these VIFs against the degree of penalization (x-axis) and the sample size (y-axis) for a fixed dimension (p = 50). 32",
        "metadata": {
          "paper_id": "1509.09169v8",
          "authors": "Wessel N. van Wieringen",
          "section": "model",
          "paper_title": "Lecture notes on ridge regression",
          "chunk_index": 11
        },
        "id": "chunk_234"
      },
      {
        "text": "be used in combination with other loss functions, for instance that of standard generalized linear models (see Chapter 5). This computation is illustrated in Figure 1.5, which shows the substantial gain in computation time of the evaluation of the ridge regression estimator using the efficient over the naive implementation against the dimension p. Details of this figure are provided in Question 1.24. 20 40 60 80 100 0e+00 2e+05 4e+05 6e+05 8e+05 1e+06 dimension (p) computation time regular efficient Figure 1.5: Computation time of the evaluation of the ridge regression estimator using the naive and efficient implementation against the dimension p. The inversion of the p × p-dimensional matrix can be avoided in an other way. Hereto one needs the Woodbury identity. Let A, U and V be p × p-, p × n- and n × p-dimensional matrices, respectively. The (simplified form of the) Woodbury identity then is: (A + UV)−1 = A−1 −A−1U(Inn + VA−1U)−1VA−1. Application of the Woodbury identity to the matrix inverse in the ridge estimator of the regression parameter gives: ˆβ(λ) = (X⊤X + λIpp)−1X⊤Y = [λ−1Ipp −λ−2X⊤(Inn + λ−1XX⊤)−1X]X⊤Y = λ−1X⊤(Inn + λ−1XX⊤)−1Y. (1.12) The inversion of the p × p-dimensional matrix X⊤X + λIpp is thus replaced by that of the n × n-dimensional matrix Inn + λ−1XX⊤. In addition, this expression of the ridge regression estimator avoids the singular value decomposition of X, which may in some cases introduce additional numerical errors (e.g. at the level of machine precision). 1.8 Penalty parameter selection Throughout the introduction of the ridge regression estimator and the subsequent discussion of its properties, we considered the penalty parameter known or ‘given’. In practice, it is unknown and the user needs to make an informed decision on its value. We present several strategies to facilitate such a decision. Prior to that, we discuss 1.8 Penalty parameter selection 23 some sanity requirements one may wish to impose on the ridge regression estimator. Those requirements do not yield a specific choice of the penalty parameter but they specify a domain of sensible penalty parameter values. The evaluation of the ridge regression estimator may be subject to numerical inaccuracy, which ideally is avoided. This numerical inaccuracy results from the ill-conditionedness of X⊤X + λIpp. A matrix is ill- conditioned if its condition number is high. The condition number of a square positive definite matrix A is the ratio of its largest and smallest eigenvalue. If the smallest eigenvalue is zero, the conditional number is unde- fined and so is A−1. Furthermore, a high condition number is indicative of the loss (on a log-scale) in numerical accuracy of the evaluation of A−1. To ensure the numerical accurate evaluation of the ridge regression estimator, the choice of the penalty parameter is thus to be restricted to a subset of the positive real numbers such that it yields a well-conditioned matrix X⊤X + λIpp. Clearly, the penalty parameter λ should then not be too close to zero. There is however no consensus on the criteria on the condition number for a matrix to be well-defined. This depends among others on how much numerical inaccuracy is tolerated by the context. Of course, as pointed out in Section 1.7, inversion of the X⊤X + λIpp matrix can be circumvented. One then still needs to ensure the well- conditionedness of λ−1XX⊤+ Inn, which too results in a lower bound for the penalty parameter. Practically, following Peeters et al. (2019) (who do so in a different context), we suggest to generate a conditional number plot. It plots (on some convenient scale) the condition number of the matrix X⊤X+λIpp against the penalty parameter λ. From this plot, we identify the domain of the penalty parameter value associated with well-conditionedness. To guide in this choice, Peeters et al. (2019) overlay this plot with a curve indicative of the numerical inaccurary. Traditional text books on regression analysis suggest, in order to prevent over-fitting, to limit the number of covariates of the model. This ought to leave enough degrees of freedom to estimate the error and facilitate proper inference. While ridge regression is commonly used for prediction purposes and inference need not be the objective, over-fitting is certainly to be avoided. This can be achieved by limiting the degrees of freedom spent on the estimation of the regression parameter (Harrell, 2001). We thus follow Saleh et al. (2019), who illustrate this in the ridge regression context, and use the degrees of freedom to bound the search domain of the penalty parameter. This requires the specification of a maximum degrees of freedom, denoted by ν with 0 ≤ν ≤n, one wishes to spend on the construction of the ridge regression estimator. Now choose λ such that df[ ˆβ(λ)] ≤ν. To find the bound on the penalty parameter, note that df[ ˆβ(λ)] = tr[H(λ)]",
        "metadata": {
          "paper_id": "1509.09169v8",
          "paper_title": "Lecture notes on ridge regression",
          "chunk_index": 5,
          "section": "model",
          "authors": "Wessel N. van Wieringen"
        },
        "id": "chunk_228"
      },
      {
        "text": "Lecture notes on ridge regression Version 0.60, June 27, 2023. Wessel N. van Wieringen1,2 1 Department of Epidemiology and Data Science, Amsterdam Public Health research institute, Amsterdam UMC, location VUmc P.O. Box 7057, 1007 MB Amsterdam, The Netherlands 2 Department of Mathematics, Vrije Universiteit Amsterdam De Boelelaan 1111, 1081 HV Amsterdam, The Netherlands Email: w.vanwieringen@amsterdamumc.nl License This document is distributed under the Creative Commons Attribution-NonCommercial-ShareAlike license: http: //creativecommons.org/licenses/by-nc-sa/4.0/ arXiv:1509.09169v8 [stat.ME] 27 Jun 2023 Disclaimer This document is a collection of many well-known results on ridge regression. The current status of the document is ‘work-in-progress’ as it is incomplete (more results from literature will be included) and it may contain incon- sistencies and errors. Hence, reading and believing at own risk. Finally, proper reference to the original source may sometimes be lacking. This is regrettable and these references – if ever known to the author – will be included in later versions. Acknowledgements Many people aided in various ways to the construction of these notes. Mark A. van de Wiel commented on various parts of Chapter 2. Jelle J. Goeman clarified some matters behind the method described in Section 6.4.3. Paul H.C. Eilers pointed to helpful references for Chapter 4 and provided parts of the code used in Section 4.3. Harry van Zanten commented on Chapters 1 and ??, while St´ephanie van de Pas made suggestions for the improvement of Chapters ?? and ??. Glenn Andrews thoroughly read Chapter 1 filtering out errors, unclarities, and inaccuracies. Small typo’s or minor errors, that have – hopefully – been corrected in the latest version, were pointed out by (among others): Rikkert Hindriks, Micah Blake McCurdy, Jos´e P. Gonz´alez-Brenes, Hassan Pazira, and numerous students from the High-dimensional data analysis- and Statistics for high-dimensional data-courses taught at Leiden University and the Vrije Universiteit Amsterdam, respectively. Contents 1 Ridge regression 2 1.1 Linear regression 2 1.2 The ridge regression estimator 5 1.3 Eigenvalue shrinkage 9 1.3.1 Principal component regression 10 1.4 Moments 10 1.4.1 Expectation 11 1.4.2 Variance 12 1.4.3 Mean squared error 14 1.4.4 Debiasing 17 1.5 Constrained estimation 17 1.6 Degrees of freedom 21 1.7 Computationally efficient evaluation 21 1.8 Penalty parameter selection 22 1.8.1 Information criterion 23 1.8.2 Cross-validation 24 1.8.3 Generalized cross-validation 25 1.8.4 Randomness 25 1.9 Simulations 26 1.9.1 Role of the variance of the covariates 26 1.9.2 Ridge regression and collinearity 28 1.9.3 Variance inflation factor 30 1.10 Illustration 32 1.10.1 MCM7 expression regulation by microRNAs 33 1.11 Conclusion 37 1.12 Exercises 37 2 Bayesian regression 46 2.1 A minimum of prior knowledge on Bayesian statistics 46 2.2 Relation to ridge regression 47 2.3 Markov chain Monte Carlo 50 2.4 Empirical Bayes 55 2.5",
        "metadata": {
          "section": "unknown",
          "authors": "Wessel N. van Wieringen",
          "paper_id": "1509.09169v8",
          "paper_title": "Lecture notes on ridge regression",
          "chunk_index": 0
        },
        "id": "chunk_223"
      }
    ]
  },
  "Compare ridge and LASSO": {
    "answer": "Ridge and LASSO (Least Absolute Shrinkage and Selection Operator) are both regularization techniques used in statistical learning to improve the out-of-sample prediction accuracy and interpretability of regression models by reducing model complexity. They achieve this by adding a penalty term to the ordinary least squares (OLS) loss function, which constrains the size of the regression coefficients. The primary distinction lies in the type of penalty used: Ridge employs an L2 norm penalty, while LASSO uses an L1 norm penalty [3, 4].\n\n### Ridge Regression\n\n**Definition and Mathematical Formulation:**\nRidge regression incorporates an **L2 regularization** term, also known as a squared L2 norm, into the standard sum-of-squares loss function. This penalty term is proportional to the sum of the squared magnitudes of the regression coefficients.\nThe objective function for Ridge regression can be formulated as:\n$$ \\min_{\\beta \\in \\mathbb{R}^p} \\left\\|Y - X\\beta\\right\\|_2^2 + \\lambda_2 \\left\\|\\beta\\right\\|_2^2 $$\nwhere $Y$ is the response vector, $X$ is the design matrix, $\\beta$ is the vector of regression coefficients, and $\\lambda_2$ (or commonly $\\lambda$) is the **regularization parameter** [3]. A larger $\\lambda_2$ value imposes a stronger penalty, leading to greater shrinkage of coefficients [1].\n\n**Key Properties and Characteristics:**\n*   **Shrinkage:** Ridge regression performs **shrinkage**, reducing the magnitude of coefficients towards zero, but typically does not set them exactly to zero [3]. The shrinkage is \"substantial and little shrinkage (in an absolute sense) for elements of the regression parameter $\\beta$ with a large and small maximum likelihood estimate, respectively\" [4]. It also \"shrinks the estimates of covariates with a large spread less than those with a small spread\" [4].\n*   **No Feature Selection:** Unlike LASSO, Ridge regression does not inherently perform **feature selection** or **dimension reduction** because it does not force coefficients to be exactly zero [2, 3]. All covariates generally contribute to the model.\n*   **Handling Collinearity:** Ridge regression exhibits **joint shrinkage** of strongly collinear covariates. Instead of selecting one, it shrinks their coefficients together [3, 4]. This can be conceived \"as doing some form of averaging, which is relatively robust and reproducible\" [2].\n*   **Uniqueness of Solution:** The L2 penalty term is strictly convex. When combined with the sum-of-squares loss, this strict convexity ensures that the Ridge loss function has a **unique minimizer** [3].\n*   **Geometric Interpretation:** In its equivalent constrained optimization form, the Ridge penalty corresponds to a spherical (or \"circle shaped\" in 2D) constraint region for the coefficients [3, Figure 7.1].\n*   **Implicit Assumption:** Ridge regression implicitly assumes that \"all (or at least many) covariates contribute to the explanation of the variation of the response\" [2].\n*   **Empirical Performance:** Simulations suggest that for larger dimensions ($p \\ge 500$), the \"ridge predictor\" tends to have better predictive performance (measured by Spearman's rank correlation) than LASSO [2, Figure 6.12]. The regularization parameter influences Training MSE, Test MSE, and Adversarial MSE [1, Figures S.8, S.9, S.10].\n\n### LASSO Regression\n\n**Definition and Mathematical Formulation:**\nLASSO regression incorporates an **L1 regularization** term, also known as an L1 norm, into the standard sum-of-squares loss function. This penalty term is proportional to the sum of the absolute magnitudes of the regression coefficients.\nThe objective function for LASSO regression can be formulated as:\n$$ \\min_{\\beta \\in \\mathbb{R}^p} \\left\\|Y - X\\beta\\right\\|_2^2 + \\lambda_1 \\left\\|\\beta\\right\\|_1 $$\nwhere $Y$ is the response vector, $X$ is the design matrix, $\\beta$ is the vector of regression coefficients, and $\\lambda_1$ (or commonly $\\lambda$) is the **regularization parameter** [3, 5]. A larger $\\lambda_1$ value imposes a stronger penalty, leading to more coefficients being set to zero [1].\n\n**Key Properties and Characteristics:**\n*   **Shrinkage:** LASSO also performs shrinkage, but it applies an \"equal amount of shrinkage to each element of $\\beta$, irrespective of the coefficients’ sizes\" [4].\n*   **Feature Selection (Sparsity):** A key distinguishing feature of LASSO is its ability to perform **feature selection** by setting some regression coefficients exactly to zero, thereby producing **sparse** models. This leads to **dimension reduction** [2, 3]. The maximum number of selected variables is bounded by $\\min\\{n, p\\}$ [3].\n*   **Handling Collinearity:** In the presence of strongly collinear covariates, LASSO \"tends to pick one covariates to enters the model while forcing the others out (by setting their estimates to zero)\" [3, 4]. Its regularization paths for collinear covariates \"do not nicely group\" [4, Figure 6.10].\n*   **Uniqueness of Solution:** The L1 penalty term is convex but not strictly convex, especially \"when the dimension $p$ exceeds the sample size $n$\" [3]. This can result in non-unique solutions for the regression coefficients.\n*   **Geometric Interpretation:** In its equivalent constrained optimization form, the LASSO penalty corresponds to a diamond-shaped constraint region (in 2D) for the coefficients. The \"corners\" of this diamond, which fall at the axes, are what endow LASSO with its selection property [3, Figure 7.1].\n*   **Implicit Assumption:** LASSO implicitly assumes that the system under study is \"sparse (and the nonzero elements are clearly distinguishable – in some sense – from zero)\" [2].\n*   **Empirical Performance:** Simulations suggest that for smaller dimensions ($p=50, 250$), LASSO's predictive performance is better than Ridge. However, as the dimension grows ($p \\ge 500$), Ridge often surpasses LASSO [2, Figure 6.12]. The selection process may be \"unstable and thereby less reproducible in high-dimensional settings\" [2]. LASSO also shrinks covariates with the smallest spread most, with a preference for selecting covariates with the largest variance for sufficiently large penalty parameters [4]. The regularization parameter (e.g., set via 5-fold cross-validation) influences Training MSE, Test MSE, and Adversarial MSE [1, Figures S.8, S.9, S.10, S.12].\n\n### Comparison of Ridge and LASSO\n\n| Feature                  | Ridge Regression                                                                   | LASSO Regression                                                                   | Source      |\n| :----------------------- | :--------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------- | :---------- |\n| **Penalty Type**         | L2 norm (sum of squared coefficients)                                              | L1 norm (sum of absolute coefficients)                                             | [3, 4]      |\n| **Mathematical Form.**   | $\\left\\|Y - X\\beta\\right\\|_2^2 + \\lambda_2 \\left\\|\\beta\\right\\|_2^2$              | $\\left\\|Y - X\\beta\\right\\|_2^2 + \\lambda_1 \\left\\|\\beta\\right\\|_1$               | [3, 5]      |\n| **Feature Selection**    | No; shrinks coefficients towards zero but rarely to exactly zero.                   | Yes; performs feature selection by setting coefficients exactly to zero (sparsity). | [2, 3]      |\n| **Dimension Reduction**  | No.                                                                                | Yes, through feature selection.                                                    | [2, 3]      |\n| **Handling Collinearity**| Exhibits **joint shrinkage** of strongly collinear covariates.                      | Tends to select one among strongly collinear covariates, forcing others out.         | [3, 4]      |\n| **Uniqueness of Solution**| Unique minimizer due to strict convexity of the L2 penalty.                        | May have non-unique solutions, especially when $p > n$.                             | [3]         |\n| **Shrinkage Mechanism**  | \"Substantial and little shrinkage\" for large and small MLEs, respectively.         | \"Equal amount of shrinkage\" to each coefficient, irrespective of size.             | [4]         |\n| **Geometric Constraint** | Circular/spherical parameter constraint.                                           | Diamond-shaped parameter constraint with \"corners\" at the axes.                    | [3, Figure 7.1]|\n| **Implicit Assumption**  | Many (or all) covariates contribute to the response.                               | The true system is sparse (many coefficients are zero).                            | [2]         |\n| **Predictive Performance (vs. p)** | Tends to perform better in higher-dimensional settings ($p \\ge 500$).         | Tends to perform better in lower-dimensional settings ($p=50, 250$).               | [2, Figure 6.12]|\n| **Stability/Reproducibility**| Generally more robust and reproducible due to \"averaging\" effect.                | Can be less stable and reproducible in high-dimensional settings due to selection. | [2]         |\n\nIn summary, Ridge regression is preferred when all features are expected to contribute to the response and when dealing with multicollinearity, as it stabilizes coefficients without discarding variables. LASSO is advantageous when feature selection and a sparse, interpretable model are desired, as it can effectively identify and eliminate irrelevant features [2, 3]. The Elastic Net estimator combines both penalties to leverage the strengths of both methods [3].",
    "chunks": [
      {
        "text": "Fourier features 10−1 100 101 102 103 104 105 1/훿 10−20 10−15 10−10 10−5 100 Train. MSE ℓ2-adv. train ridge 10−1 100 101 102 103 104 105 1/훿 10−24 10−19 10−14 10−9 10−4 101 Train. MSE ℓ∞-adv. train lasso (c) Phenotype prediction Figure S.8: Training MSE vs regularization parameter. Left: for ridge and ℓ2-adversarial training. Right: for Lasso and ℓ∞-adversarial training The error bars give the median and the 0.25 and 0.75 quantiles obtained from numerical experiment (5 realizations). xiv 10−1 100 101 102 103 104 105 1/훿 0.0 2.5 5.0 Test MSE ℓ2-adv. train ridge 10−1 100 101 102 103 104 105 1/훿 0.0 2.5 5.0 Test MSE ℓ∞-adv. train lasso (a) Gaussian features 10−1 100 101 102 103 104 105 1/훿 0.0 2.5 5.0 Test MSE ℓ2-adv. train ridge 10−1 100 101 102 103 104 105 1/훿 0.0 2.5 5.0 Test MSE ℓ∞-adv. train lasso (b) Latent-space features 10−1 100 101 102 103 104 105 1/훿 0.0 2.5 5.0 Test MSE ℓ2-adv. train ridge 10−1 100 101 102 103 104 105 1/훿 0.0 2.5 5.0 Test MSE ℓ∞-adv. train lasso (c) Random Fourier features 10−1 100 101 102 103 104 105 1/훿 0.0 2.5 5.0 Test MSE ℓ2-adv. train ridge 10−1 100 101 102 103 104 105 1/훿 0.0 2.5 5.0 Test MSE ℓ∞-adv. train lasso (d) Phenotype prediction Figure S.9: Test MSE vs regularization parameter. Left: for ridge and ℓ2-adversarial training. Right: for Lasso and ℓ∞-adversarial training. The error bars give the median and the 0.25 and 0.75 quantiles obtained from numerical experiment (5 realizations). xv 10−1 100 101 102 103 104 105 1/훿 0 10 20 Adv. MSE ℓ2-adv. train ridge 10−1 100 101 102 103 104 105 1/훿 0 10 20 Adv. MSE ℓ∞-adv. train lasso (a) Gaussian features 10−1 100 101 102 103 104 105 1/훿 0 10 20 Adv. MSE ℓ2-adv. train ridge 10−1 100 101 102 103 104 105 1/훿 0 10 20 Adv. MSE ℓ∞-adv. train lasso (b) Latent-space features 10−1 100 101 102 103 104 105 1/훿 0 10 20 Adv. MSE ℓ2-adv. train ridge 10−1 100 101 102 103 104 105 1/훿 0 10 20 Adv. MSE ℓ∞-adv. train lasso (c) Random Fourier features 10−1 100 101 102 103 104 105 1/훿 0 10 20 Adv. MSE ℓ2-adv. train ridge 10−1 100 101 102 103 104 105 1/훿 0 10 20 Adv. MSE ℓ∞-adv. train lasso (d) Phenotype prediction Figure S.10: Adversarial Test MSE vs regularization parameter. Left: for ridge and ℓ2-adversarial training. Right: for Lasso and ℓ∞-adversarial training. The error bars give the median and the 0.25 and 0.75 quantiles obtained from numerical experiment (5 realizations). xvi 200 400 600 800 1000 # features, 푝 0.5 1.0 1.5 ¯훿 min. ℓ2-norm 200 400 600 800 1000 # features, 푝 0.025 0.050 0.075 0.100 ¯훿 min. ℓ1-norm (a) Threshold ¯δ 100 102 104 1/훿 10−19 10−12 10−5 Train MSE ℓ2-adv. train 100 102 104 1/훿 10−10 10−6 10−2 Train MSE ℓ∞-adv. train (b) Train MSE 100 102 104 1/훿 2.0 2.5 Test MSE ℓ2-adv. train 100 102 104 1/훿 2.0 2.5 Test MSE ℓ∞-adv. train (c) Test MSE Figure S.11: Random projections Left: the results for ℓ∞-adversarial training. Right: the results for ℓ2-adversarial attacks. In (a) we show the threshold as a function of the number of features. Unlike Figures 2 and S.5, we do not give a reference, that is because the input x is fixed, so it does make sense to consider δ in absolute terms. (b) the train MSE as a function of 1/δ for the number of features fixed p = 200. (c) the test MSE as a function of 1/δ. We consider an input of dimension d = 1000. 400 1000 2000 4000 8000 # features, 푝 1.0 1.5 2.0 2.5 Test MSE min ℓ1-norm lasso CV √ lasso adv. train ℓ∞ Figure S.12: Test MSE (MSE) under fixed adversarial radius in MAGIC dataset. We study a choice of adversarial radius inspired by Theorem 2. For ℓ∞-adversarial training, we use δ = 0.5∥Xξ∥∞/∥ξ∥1 for ξ a vector with zero-mean normal entries. We use a random ξ, since we do not know the true additive noise. Even with this approximation, ℓ∞-adversarial training performs comparably with Lasso with the regularization parameter set using 5-fold cross-validation doing a full search among the hyperparameter space. We use a similar method for setting the square-root Lasso parameter, setting λ = 0.1∥Xξ∥∞/∥ξ∥2. The value 0.5 and 0.1 were set empirically, after finding out that the value 3 used in the theorem was too conservative. xvii E",
        "metadata": {
          "paper_title": "Regularization properties of adversarially-trained linear regression",
          "paper_id": "2310.10807v1",
          "chunk_index": 12,
          "section": "model in this latent space",
          "authors": "Antônio H. Ribeiro, Dave Zachariah, Francis Bach, Thomas B. Schön"
        },
        "id": "chunk_125"
      },
      {
        "text": "thresholding-type procedure. We have repeated the simulation but fix the sparsity in a relative sense instead of an absolute one. That is, the relative sparsity is fixed at 10%, i.e. ⌈p/10⌉elements of the regression parameter are non-zero, instead of the five non-zero elements irrespective of the dimension. The elements of the regression parameter are set to βj = 500 j p−7/4 for j = 1, . . . , ⌈p/10⌉and βj = 0 for j = ⌈p/10⌉+ 1, . . . , p. The particular dependence of the nonzero elements of the regression parameter on the dimension is a clumsy attempt to fix the variance of the response over the employed range of dimensions. The latter now ranges over p ∈{50, 250, 500, 750, . . . , 2500}. Figure 6.12 shows the violinplots of the resulting thousand Spearman’s rank correlations between the predictions and observations of the lasso and ridge estimates for the various dimensions. For small dimensions, p = 50 and p = 250, the predictive performance of the ridge regression estimates falls behind that of its lasso counterpart. But as the dimension grows beyond p = 500, the roles reverse and the ridge regression estimate performs better than its lasso counterpart. Let us close with some intuition why we see those results. The lasso regression estimator selects, and thereby performs dimension reduction. This may be unstable and thereby less reproducible in high-dimensional settings. The ridge regression estimator can be conceived as doing some form of averaging, which is relatively robust and reproducible. In all, these simulations may suggest that overall the ‘ridge predictor’ tends to have a better predictive perfor- mance than its lasso counterpart. Should we prefer thus the ridge over the lasso regression estimator? No, that would be shortsighted. The simulations are far too limited in nature, they are only meant to illustrate the depen- dence of the behavior of the estimators under various choices of the regression parameter. One may interpret the 126 Lasso regression Spearman correlation of obs. and cv−prediction, 10% sparse β cor(Y,Y^) −1.0 −0.5 0.0 0.5 1.0 p=50 p=250 p=500 p=750 p=1000 p=1250 p=1500 p=1750 p=2000 p=2250 p=2500 p=50 p=250 p=500 p=750 p=1000 p=1250 p=1500 p=1750 p=2000 p=2250 p=2500 lasso ridge Figure 6.12: Violinplots of the predictive performance, operationalized as Spearman’s rank correlation among observation and prediction, of the lasso and ridge regression estimates with fixed relative sparsity at 10% and increasing p. simulations as suggesting that a choice for either estimator comes with a believe on the structure of the regres- sion parameter. E.g., the lasso regression estimator implicitly assumes the system under study is sparse (and the nonzero elements are clearly distinguishable – in some sense – from zero). The validity of this assumption is not directly obvious is, e.g., biological phenomena. Similarly, the ridge regression estimator implicity assumes all (or at least many) covariates contribute to the explanation of the variation of the response. Such an assumption too is difficult to verify. 6.10 Application A seminal high-dimensional data set, that has often been re-analyzed in many articles to illustrate novel (regular- ization) methods, is presented in Van’t Veer et al. (2002); Van De Vijver et al. (2002). It is part of a study into breast cancer and comprises the overall survival information and expression levels of 24158 genes of 291 breast cancer samples. Alongside the high-throughput omics data, clinical information like age and sex is available but discarded here. The data are provided via the breastCancerNKI-package (Schroeder et al., 2022). In the original work of Van’t Veer et al. (2002); Van De Vijver et al. (2002) the data are used to build a linear predictor of overall survival. This resulted in a so-called ‘signature’ of 70 genes – quite a dimension reduction! – that together are predictive of overall survival. Upon this work, a commercial enterprise has been founded that introduced a diagonostic test called the Mammaprint (https://en.wikipedia.org/wiki/ MammaPrint). In a nutshell, the workings of the test (essentially) amount to the evaluation of the linear predictor, which is subsequently compared to a reference value to decide on the expected survival outcome. Within the context of our data set, the reference value will be the median of all linear predictions: \u001a poor prognosis if Xi,∗ˆβ > median{X1,∗ˆβ, . . . , Xn,∗ˆβ}, good prognosis if Xi,∗ˆβ < median{X1,∗ˆβ, . . . , Xn,∗ˆβ}. The prognosis determines the individual’s follow-up treatment. For instance, should the test indicate a ‘good prognosis’, the individual may be spared chemo-therapy without a substantial overall survival reduction but a considerable gain in quality of life. 6.10 Application 127 100 200 500 1000 2000 5000 10000 above median(lp) below median(lp) events only observed survival time 0 200 400 600 800 1000 2000 3000 4000 5000 events only expected survival time observed survival time Figure 6.13: State",
        "metadata": {
          "section": "algorithm.",
          "paper_title": "Lecture notes on ridge regression",
          "paper_id": "1509.09169v8",
          "chunk_index": 38,
          "authors": "Wessel N. van Wieringen"
        },
        "id": "chunk_261"
      },
      {
        "text": "ii) the bound on the number of selected variables, i.e. maximally min{n, p} can be selected, and iii) the observation that strongly (positively) collinear covariates are not shrunken together: the lasso regression estimator selects among them while it is hard to distinguish their contributions to the variation of the response. While it does not select, the ridge regression estimator does not exhibit these less pleasing features. These consid- erations led Zou and Hastie (2005) to combine the strengths of the lasso and ridge regression estimators and form a ‘best-of-both-worlds’ estimator, called the elastic net regression estimator, defined as: ˆβ(λ1, λ2) = arg min β∈Rp ∥Y −Xβ∥2 2 + λ1∥β∥1 + 1 2λ2∥β∥2 2. The elastic net penalty – defined implicitly in the preceeding display – is thus simply a linear combination of the lasso and ridge penalties. Consequently, the elastic net regression estimator encompasses its lasso and ridge counterparts. Hereto just set λ2 = 0 or λ1 = 0, respectively. A novel estimator is defined if both penalties act simultaneously, i.e. if their corresponding penalty parameters are both nonzero. Does this novel elastic net estimator indeed inherit the strengths of the lasso and ridge regression estimators? Let us turn to the aforementioned motivation behind the elastic net estimator. Starting with the uniqueness, the strict convexity of the ridge penalty renders the elastic net loss function strictly convex, as it is a combination of the ridge penalty and the lasso function – notably non-strict convex when the dimension p exceeds the sample size n. This warrants the existence of a unique minimizer of the elastic net loss function. To assess the preservation of the selection property, now without the bound on the maximum number of selectable variables, exploit the equivalent constraint estimation formulation of the elastic net estimator. Figure 7.1 shows the parameter constraint of the elastic net estimator for the ‘p = 2’-case, which is defined by the set: {(β1, β2) ∈R2 : λ1(|β1| + |β2|) + 1 2λ2(β2 1 + β2 2) ≤c(λ1, λ2)}. Visually, the ‘elastic net parameter constraint’ is a compromise between the circle and the diamond shaped con- straints of the ridge and lasso regression estimators. This compromise inherits exactly the right geometrical fea- tures: the strict convexity of the ‘ridge circle’ and the ‘corners’ (referring to points at which the constraint’s boundary is non-smootness/non-differentiability) falling at the axes of the ‘lasso constraint’. The latter feature, by the same argumentation as presented in Section 6.3, endows the elastic net estimator with the selection property. Moreover, it can – in principle – select p features as the point in the parameter space where the smallest level set of the unpenalized loss hits the elastic net parameter constraint need not fall on any axis. For example, in the ‘p = 2, n = 1’-case the level sets of the sum-of-squares loss are straight lines that, if running almost parallel to the edges of the ‘lasso diamond’, are unlikely to first hit the elastic net parameter constraint at one of its corners. Finally, the largest penalty parameter relates (reciprocally) to the volume of the elastic net parameter constraint, while the ratio between λ1 and λ2 determines whether it is closer to the ‘ridge circle’ or to the ‘lasso diamond’. Whether the elastic net regression estimator also delivers on the joint shrinkage property is assessed by simu- lation (not shown). The impression given by these simulations is that the elastic net has joint shrinkage potential. This, however, usually requires a large ridge penalty, which then dominates the elastic net penalty. 134 Generalizing lasso regression β1 β2 −1.0 −0.5 0.0 0.5 1.0 −1.0 −0.5 0.0 0.5 1.0 lasso ridge elastic net Figure 7.1: The left panel depicts the parameter constraint induced by the elastic net penalty and, for reference, those of the lasso and ridge are added. The right panel shows the contour plot of the cross-validated loglikelihood vs. the two penalty parameters of the elastic net estimator. The elastic net regression estimator can be found with procedures similar to those that evaluate the lasso regression estimator (see Section 6.4) as the elastic net loss can be reformulated as a lasso loss. Hereto the ridge part of the elastic net penalty is absorbed into the sum of squares using the data augmentation trick of Exercise 1.6 which showed that the ridge regression estimator is the ML regression estimator of the related regression model with p zeros and rows added to the response and design matrix, respectively. That is, write ˜Y = (Y⊤, 0⊤ p )⊤and ˜X = (X⊤, √λ2 Ipp)⊤. Then: ∥˜Y −˜Xβ∥2 2 = ∥Y −Xβ∥2 2 + λ2∥β∥2 2. Hence, the elastic net loss function can be rewritten to ∥˜Y −˜Xβ∥2 2 + λ1∥β∥1. This is familiar territory and the lasso algorithms of Section 6.4 can be used. Zou",
        "metadata": {
          "paper_id": "1509.09169v8",
          "authors": "Wessel N. van Wieringen",
          "section": "algorithm.",
          "paper_title": "Lecture notes on ridge regression",
          "chunk_index": 43
        },
        "id": "chunk_266"
      },
      {
        "text": "substantial and little shrinkage (in an absolute sense) for elements of the regression parameter β with a large and small maximum likelihood estimate, respectively. In contrast, the lasso regression estimator applies an equal amount of shrinkage to each element of β, irrespective of the coefficients’ sizes. Figure 6.9: Solution path of the lasso and ridge regression estimators, left and right panel, respectively, for data with an orthonormal design matrix. 6.9.3 Simulation I: covariate selection Here it is investigated whether lasso regression exhibits the same behaviour as ridge regression in the presence of covariates with differing variances. Recall: the simulation of Section 1.9.1 showed that ridge regression shrinks the estimates of covariates with a large spread less than those with a small spread. That simulation has been repeated, with the exact same parameter choices and sample size, but now with the ridge regression estimator replaced by the lasso regression estimator. To refresh the memory: in the simulation of Section 1.9.1 the linear regression model is fitted, now with the lasso regression estimator. The (n = 1000) × (p = 50) dimensional design matrix X is sampled from a multivariate normal distribution: X⊤ i,∗∼N (050, Σ) with Σ diagonal and (Σ)jj = j/10 for j = 1, . . . , p. The response Y is generated through Y = Xβ + ε with β a vector of all ones and ε sampled from the multivariate standard normal distribution. Hence, all covariates contribute equally to the response. The results of the simulation are displayed in Figure 6.10, which shows the regularization paths of the p = 50 covariates. The regularization paths are demarcated by color and style to indicate the size of the spread of the corresponding covariate. These regularization paths show that the lasso regression estimator shrinks – like the ridge regression estimator – the covariates with the smallest spread most. For the lasso regression this translates (for sufficiently large values of the penalty parameter) into a preference for the selection of covariates with largest variance. Intuition for this behavior of the lasso regression estimator may be obtained through geometrical arguments analogous to that provided for the similar behaviour of the ridge regression estimator in Section 1.9.1. Alge- braically it is easily seen when assuming an orthogonal design with Var(X1) ≫Var(X2). The lasso regression loss function can then be rewritten, as in Example 6.5, to: ∥Y −Xβ∥2 2 + λ1∥β∥1 = ∥Y −˜Xγ∥2 2 + λ1[Var(X1)]−1/2|γ1| + λ1[Var(X2)]−1/2|γ2|, where γ1 = [Var(X1)]1/2β1 and γ2 = [Var(X2)]1/2β2. The rescaled design matrix ˜X is now orthonormal and analytic expressions of estimators of γ1 and γ2 are available. The former parameter is penalized substantially less than the latter as λ1[Var(X1)]−1/2 ≪λ1[Var(X2)]−1/2. As a result, if for large enough values of λ1 one variable is selected, it is more likely to be γ1. 6.9 Comparison to ridge 123 Figure 6.10: Regularization paths of the lasso regression estimator related to the simulations presented in Sections 6.9.3 and 6.9.4 in the left and right panel, respectively. 6.9.4 Simulation II: correlated covariates The behaviour of the lasso regression estimator is now studied in the presence of collinearity among the covariates. Previously, in simulation, Section 1.9.2, the ridge regression estimator was shown to exhibit the joint shrinkage of strongly collinear covariates. This simulation is repeated for the lasso regression estimator. The details of the simulation are recapped. The linear regression model is fitted by means of the lasso regression estimator. The (n = 1000) × (p = 50) dimensional design matrix X is samples from a multivariate normal distribution: X⊤ i,∗∼N (050, Σ) with a block-diagonal Σ. The k-the, k = 1, . . . , 5, diagonal block, denoted Σkk comprises ten covariates and equals k−1 5 110×10 + 6−k 5 I10×10 for k = 1, . . . , 5. The response vector Y is then generated by Y = Xβ + ε, with ε sampled from the multivariate standard normal distribution and β containing only ones. Again, all covariates contribute equally to the response. The results of the above simulation results are captured in Figure 6.10. It shows the lasso regularization paths for all elements of the regression parameter β. The regularization paths of covariates corresponding to the same block of Σ (indicative of the degree of collinearity) are now marcated by different colors and styles. Whereas the ridge regularization paths nicely grouped per block, the lasso counterparts do not. The selection property spoils the party. Instead of shrinking the regression parameter estimates of collinear covariates together, the lasso regression estimator (for sufficiently large values of its penalty parameter λ1) tends to pick one covariates to enters the model while forcing the others out (by setting their estimates to zero). 6.9.5 Simulation III: sparsity Our next simulation investigates the effect of sparsity on the predictive performance",
        "metadata": {
          "chunk_index": 36,
          "section": "algorithm.",
          "paper_id": "1509.09169v8",
          "authors": "Wessel N. van Wieringen",
          "paper_title": "Lecture notes on ridge regression"
        },
        "id": "chunk_259"
      },
      {
        "text": "Finally, a note of caution in similar spirit as that which concludes Chapter 3. It is a joy to play with the penalty and see how it encourages the regression parameter estimate to exhibit hypothesized behaviour. Nonetheless, for a deeper and profound understanding of the data generating mechanism, such knowledge is ideally incorporated explicitly in the model itself. 7.7 Exercises Question 7.1 Augment the lasso penalty with the sum of the absolute differences all pairs of successive regression coefficients: λ1 Xp j=1 |βj| + λ1,f Xp j=2 |βj −βj−1|. This augmented lasso penalty is referred to as the fused lasso penalty. a) Consider the standard multiple linear regression model: Yi = Pp j=1 Xij βj + εi. Estimation of the regres- sion parameters takes place via minimization of penalized sum of squares, in which the fused lasso penalty is used with λ1 = 0. Rewrite the corresponding loss function to the standard lasso problem by application of the following change-of-variables: γ1 = β1 and γj = βj −βj−1. b) Investigate on simulated data the effect of the second summand of the fused lasso penalty on the parameter estimates. In this, temporarily set λ1 = 0. c) Let λ1 equal zero still. Compare the regression estimates of part b) to the ridge estimates with a first-order autoregressive prior. What is qualitatively the difference in the behavior of the two estimates? Hint: plot the full solution path for the penalized estimates of both estimation procedures. d) How do the estimates of part b) of this question change if we allow λ1 > 0? Question 7.2 Consider the standard linear regression model Yi = Xi,∗β +εi for i = 1, . . . , n and with εi ∼i.i.d. N (0, σ2). The rows of the design matrix X are of length 2, neither column represents the intercept. Relevant summary statistics from the data on the response Y and the covariates are: X⊤X = \u0012 40 −20 −20 10 \u0013 and X⊤Y = \u0012 26 −13 \u0013 . a) Use lasso regression to regress without intercept the response on the first covariate. Draw (i.e. not sketch!) the regularization path of lasso regression estimator. b) The two covariates are perfectly collinear. However, their regularization paths do not coincide. Why? c) Fit the linear regression model with both covariates (and still without intercept) by means of the fused lasso, i.e. ˆβ(λ1, λf) = arg minβ ∥Y −Xβ∥2 2 + λ1∥β∥1 + λf|β1 −β2| with λ1 = 10 and λF = ∞. Hint: at some point in your answer you may wish to write X = (X∗,1 cX∗,1) and deduce c from X⊤X. Question 7.3 Consider the linear regression model Y = Xβ + ε with ε ∼N (0n, σ2Inn). This model (without intercept) is fitted to data using the lasso regression estimator ˆβ(λ1) = arg minβ ∥Y −Xβ∥2 2 + λ1∥β∥1. The relevant summary statistics of the data are: X = \u0012 1 −1 −1 1 \u0013 , Y = \u0012 −5 4 \u0013 , X⊤X = \u0012 2 −2 −2 2 \u0013 , and X⊤Y = \u0012 −9 9 \u0013 . a) Specify the full set of lasso regression estimates with λ1 = 2 that minimize the lasso loss function for these data. b) Now consider fitting the linear regression model with the fused lasso estimator ˆβ(λf) = arg minβ ∥Y − Xβ∥2 2 + λf|β1 −β2|. Determine λ(0) f > 0 such that ˆβ(λf) = (0, 0)⊤for all λf > λ(0) f . Question 7.4 Consider the linear regression model Y = Xβ + ε with ε ∼N (0n, σ2 εInn). This model (without intercept) is fitted to data using the lasso regression estimator ˆβ(λ1, λf) = arg minβ ∥Y −Xβ∥2 2 +λ1∥β∥1 +λf Pp j=2 |βj − βj−1|. The data are: X = \u0012 3 −1 1 1 \u0013 , and Y = \u0012 −2 3 \u0013 . 140 Generalizing lasso regression a) Evaluate the fused lasso regression estimator ˆβ(λ1, λf) for λ1 = 1 and λf = ∞. b) How many covariates can the fused lasso regression estimator select? Motivate. Question 7.5 (The elastic net regression estimator) Consider fitting the linear regression model by means of the elastic net regression estimator. a) Recall the data augmentation trick of Question 1.6 of the ridge regression exercises. Use the same trick to show that the elastic net least squares loss function can be reformulated to the form of the traditional lasso function. Hint: absorb the ridge part of the elastic net penalty into the sum of squares. b) The elastic net regression estimator can be evaluated by a coordinate descent procedure outlined in Section 6.4.4. Show that in such a procedure at each step the j-th element of the elastic net regression estimate is updated according to: ˆβj(λ1, λ2) = (∥X∗,j∥2 2 +",
        "metadata": {
          "authors": "Wessel N. van Wieringen",
          "chunk_index": 1,
          "paper_title": "Lecture notes on ridge regression",
          "section": "conclusion",
          "paper_id": "1509.09169v8"
        },
        "id": "chunk_224"
      }
    ]
  },
  "What is the bias-variance tradeoff?": {
    "answer": "The bias-variance tradeoff is a fundamental concept in statistical learning concerning the relationship between a model's complexity, its fitting to training data, and its ability to generalize to new, unseen data. It manifests as an opposition between two sources of error: bias and variance, which together contribute to the expected prediction error (also known as risk) of a model [3].\n\nA modeling approach is a mapping from a training set to a model $\\hat{\\mu}$, evaluated by an error or loss criterion, such as the squared-error criterion: $E(\\hat{\\mu}, y_{new}) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{\\mu}_i - y_{new, i})^2$ [3]. The goal in model selection is to choose an approach with small risk, i.e., small expected prediction error [3].\n\n**Bias**\nBias refers to the error introduced by approximating a real-world problem, which may be complicated, by a simplified model. It represents the difference between the expected prediction of our model and the true value [5]. High bias corresponds to \"underfitting,\" where the model fails to capture the underlying patterns in the training data, wasting information that could be gained [3]. For instance, choosing a very low degree polynomial in a least squares fitting problem leads to underfitting and is undesirable [3]. The bias of ridge regression has an asymptotic formulation given by:\n$\\lambda^2(\\mathbf{b}\\Sigma + \\lambda I_p)^{-1}A(\\mathbf{b}\\Sigma + \\lambda I_p)^{-1} \\simeq (v(-\\lambda; \\gamma, \\Sigma)\\Sigma + I_p)^{-1}(\\text{evb}(-\\lambda; \\gamma, \\Sigma, A)\\Sigma + A)(v(-\\lambda; \\gamma, \\Sigma)\\Sigma + I_p)^{-1}$ [5],\nwhere $\\lambda$ is the regularization parameter, $\\mathbf{b}\\Sigma = X^\\top X/n$, $A$ is a matrix (often $\\Sigma$), and $v(-\\lambda; \\gamma, \\Sigma)$ and $\\text{evb}(-\\lambda; \\gamma, \\Sigma, A)$ are terms defined by fixed-point equations related to the data characteristics and regularization strength [5].\n\n**Variance**\nVariance refers to the amount that the model's prediction changes when trained on different training datasets. High variance corresponds to \"overfitting,\" where the model learns too much detail and noise from the training data, performing well on the training set but poorly on new data [3]. A high degree polynomial, for example, often leads to high variance and overfitting [3]. The best linear unbiased estimator (BLUE) is defined as having minimal variance [2]. The variance of ridge regression also has an asymptotic formulation:\n$(\\mathbf{b}\\Sigma + \\lambda I_p)^{-2} \\mathbf{b}\\Sigma A \\simeq \\text{evv}(-\\lambda; \\gamma, \\Sigma)(v(-\\lambda; \\gamma, \\Sigma)\\Sigma + I_p)^{-2}\\Sigma A$ [5],\nwhere $\\text{evv}(-\\lambda; \\gamma, \\Sigma)$ is another term defined by fixed-point equations related to the data and regularization [5].\n\n**The Tradeoff**\nBoth underfitting (high bias) and overfitting (high variance) are undesirable, as they lead to poor model performance [3]. The bias-variance tradeoff implies that as model complexity increases, bias generally decreases (the model fits the training data better), but variance increases (the model becomes more sensitive to the specific training data). Conversely, as model complexity decreases, bias increases, and variance decreases. The goal is to find a balance that minimizes the total expected prediction error on unseen data.\n\n**Regularization and its Role**\nRegularization techniques, such as ridge regression (which employs $\\ell_2$ regularization), are often used to manage this tradeoff by controlling the amount of fitting to training data via a tuning parameter [3, 4].\nRidge regression, for instance, penalizes the size of the regression coefficients to prevent them from becoming too large, especially in the presence of collinear covariates [1]. The objective function for ridge regression is typically given by:\n$\\min_{\\beta \\in \\mathbb{R}^p} \\|Y - X\\beta\\|^2_2 + \\lambda\\|\\beta\\|^2_2$ [1].\nThe regularization parameter $\\lambda > 0$ determines the strength of the penalty.\n*   A larger $\\lambda$ imposes a stronger penalty, shrinking the regression coefficients towards zero, which increases bias but decreases variance [1, 5]. This can be seen in the formulation of the ridge regression estimator: $\\beta(\\lambda) = [\\text{Var}(X) + \\lambda I_{50 \\times 50}]^{-1}\\text{Cov}(X, Y)$ [1]. As $\\lambda$ increases, the inverse term becomes smaller, leading to smaller coefficient estimates.\n*   A smaller $\\lambda$ allows coefficients to be larger, reducing bias but potentially increasing variance [1, 5].\n\nRidge regression can also be viewed as covariate-wise penalization where scaling covariates by a diagonal matrix $A$ results in a modified penalty: $\\min_{\\gamma} \\|Y - X\\gamma\\|^2_2 + \\sum_{j=1}^p \\lambda[(A)_{jj}]^{-2}\\gamma_j^2$ [1]. This demonstrates how the regularization effectively introduces a bias in favor of certain covariates (e.g., those with larger variances) in the unscaled context, to achieve a reduction in variance [1]. The asymptotic bias and variance of ridge regression explicitly show their dependence on $\\lambda$ and other data characteristics like $\\gamma=p/n$ (ratio of features to samples) [5].",
    "chunks": [
      {
        "text": "variances. Color and line type indicated the grouping of the covariates by their variance. Bottom panels: Graphical illustration of the effect of a covariate’s variance on the ridge regression estimator. The grey circle depicts the ridge parameter constraint. The dashed black ellipsoids are the level sets of the least squares loss function. The red dot is the ridge regression estimate. Left and right panels represent the cases with equal and unequal, respectively, variances of the covariates. Ridge regression’s preference of covariates with a large variance can intuitively be understood as follows. First note that the ridge regression estimator now can be written as: β(λ) = [Var(X) + λI50×50]−1Cov(X, Y) = (Σ + λI50×50)−1Σ[Var(X)]−1Cov(X, Y) = (Σ + λI50×50)−1Σβ. Plug in the employed parametrization of Σ, which gives: [β(λ)]j = j(j + 50λ)−1(β)j. Hence, the larger the 28 Ridge regression covariate’s variance (corresponding to the larger j), the larger its ridge regression coefficient estimate. Ridge regression thus prefers, among a set of covariates with comparable effect sizes, those with larger variances. The reformulation of ridge penalized estimation as a constrained estimation problem offers a geometrical interpretation of this phenomenon. Let p = 2 and the design matrix X be orthogonal, while both covariates contribute equally to the response. Contrast the cases with Var(X1) ≈Var(X2) and Var(X1) ≫Var(X2). The level sets of the least squares loss function associated with the former case are circular, while that of the latter are strongly ellipsoidal (see Figure 1.7). The diameters along the principal axes (that – due to the orthogonality of X – are parallel to that of the β1- and β2-axes) of both circle and ellipsoid are reciprocals of the variance of the covariates. When the variances of both covariates are equal, the level sets of the loss function expand equally fast along both axes. With the two covariates having the same regression coefficient, the point of these level sets closest to the parameter constraint is to be found on the line β1 = β2 (Figure 1.7, left panel). Consequently, the ridge regression estimator satisfies ˆβ1(λ) ≈ˆβ2(λ). With unequal variances between the covariates, the ellipsoidal level sets of the loss function have diameters of rather different sizes. In particular, along the β1-axis it is narrow (as Var(X1) is large), and – vice versa – wide along the β2-axis. Consequently, the point of these level sets closest to the circular parameter constraint will be closer to the β1- than to the β2-axis (Figure 1.7, left panel). For the ridge estimator of the regression parameter this implies 0 ≪ˆβ1(λ) < 1 and 0 < ˆβ2(λ) ≪1. Hence, the covariate with a larger variance yields the larger ridge regression estimator. Should one thus standardize the covariates prior to ridge regression analysis? When dealing with gene expres- sion data from microarrays, the data have been subjected to a series of pre-processing steps (e.g. quality control, background correction, within- and between-normalization). The purpose of these steps is to make the expression levels of genes comparable both within and between hybridizations. The preprocessing should thus be considered an inherent part of the measurement. As such, it is to be done independently of whatever down-stream analysis is to follow and further tinkering with the data is preferably to be avoided (as it may mess up the ‘comparable-ness’ of the expression levels as achieved by the preprocessing). For other data types different considerations may apply. Among the considerations to decide on standardization of the covariates, one should also include the fact that ridge regression estimates prior and posterior to scaling do not simply differ by a factor. To see this assume that the covariates have been centered. Scaling of the covariates amounts to post-multiplication of the design matrix by a p × p-dimensional diagonal matrix A with the reciprocals of the covariates’ scale estimates on its diagonal (Sardy, 2008). Hence, the ridge regression estimator (for the rescaled data) is then given by: minβ∈Rp ∥Y −XAβ∥2 2 + λ∥β∥2 2. Apply the change-of-variable γ = Aβ and obtain: min γ ∥Y −Xγ∥2 2 + λ∥A−1γ∥2 2 = min γ ∥Y −Xγ∥2 2 + Xp j=1 λ[(A)jj]−2γ2 j . Effectively, the scaling is equivalent to covariate-wise penalization (see Chapter 3 for more on this). The ‘scaled’ ridge regression estimator may then be derived along the same lines as before in Section 1.5: ˆβ (scaled)(λ) = A−1ˆγ(λ) = A−1(X⊤X + λA−2)−1X⊤Y. In general, this is unequal to the ridge regression estimator without the rescaling of the columns of the design matrix. Moreover, it should be clear that ˆβ(scaled)(λ) ̸= A ˆβ(λ). 1.9.2 Ridge regression and collinearity Initially, ridge regression was motivated as an ad-hoc fix of (super)-collinear covariates in order to obtain a well- defined estimator. We now study the effect of this ad-hoc fix on the regression coefficient estimates of collinear covariates. In particular,",
        "metadata": {
          "chunk_index": 9,
          "paper_title": "Lecture notes on ridge regression",
          "authors": "Wessel N. van Wieringen",
          "section": "model",
          "paper_id": "1509.09169v8"
        },
        "id": "chunk_232"
      },
      {
        "text": "1, 2). Theorem 3.1 below shows that the best pair of linear unbiased estimators is the pair (ˆθ1,BLUE, ˆθ2,BLUE) of 5 the best linear unbiased estimators in the individual models. For its proof we ﬁrst establish the following lemma which is given as exercise 14 of Chapter 4.3 in Borwein (2000). Lemma 3.1. Let g : [a, b] × L →R be a function on the non empty set L ⊂Rm1+m2. If the point (¯t, ¯θ) ∈[a, b] × L is a saddlepoint, that is g(t, ¯θ) ≤g(¯t, ¯θ) ≤g(¯t, θ) for all t ∈[a, b], θ ∈L, (3.5) the following equalities hold inf θ∈L sup t∈[a,b] g(t, θ) = g(¯t, ¯θ) = sup t∈[a,b] inf θ∈L g(t, θ). (3.6) Proof of Lemma 3.1: Note that (¯t, ¯θ) ∈[a, b] × L is a saddlepoint if and only if g(¯t, ¯θ) = inf θ∈L g(¯t, θ) and g(¯t, ¯θ) = sup t∈[a,b] g(t, ¯θ). Using this formulation we have that g(¯t, ¯θ) = supt∈[a,b] g(t, ¯θ) ≥infθ∈L supt∈[a,b] g(t, θ) and also g(¯t, ¯θ) = infθ∈L g(¯t, θ) ≤supt∈[a,b] infθ∈L g(t, θ). Hence inf θ∈L sup t∈[a,b] g(t, θ) ≤sup t∈[a,b] inf θ∈L g(t, θ). (3.7) On the other hand, we have that g(t, θ) ≤supt∈[a,b] g(t, θ) for all t ∈[a, b] and θ ∈L and thus inf θ∈L g(t, θ) ≤inf θ∈L sup t∈[a,b] g(t, θ) ⇒sup t∈[a,b] inf θ∈L g(t, θ) ≤inf θ∈L sup t∈[a,b] g(t, θ). Hence inf θ∈L sup t∈[a,b] g(t, θ) ≥sup t∈[a,b] inf θ∈L g(t, θ), (3.8) and by combining (3.7) and (3.8) the equality (3.6) follows. □ Theorem 3.1. Let ˆθi,BLUE be the best linear unbiased estimator, deﬁned in (3.3) and (3.4), in the corresponding continuous time model (3.1) for i = 1, 2. Then for any p ∈[1, ∞], ˆθBLUE = (ˆθT 1,BLUE, ˆθT 2,BLUE)T is the best pair of linear unbiased estimators minimising the Lp-norm (3.2) of the variance of the estimate of the diﬀerence between the parametric curves in the class L = \bˆθ = (ˆθT 1 , ˆθT 2 )T|ˆθT i linear unbiased estimator for model (3.1) for i = 1, 2 . Proof of Theorem 3.1: Since both ˆθi,BLUE have minimal variance with respect to the Loewner ordering, it follows that ˆθBLUE = (ˆθT 1,BLUE, ˆθT 2,BLUE)T minimises the variance of the diﬀerence between the estimated curves. That is, for any t ∈[a, b] g(t, ˆθBLUE) ≤g(t, ˆθ), for all ˆθ ∈L. 6 When p ∈[1, ∞), it is straightforward to check that ˆθBLUE minimises µp(ˆθ) = ∥g(t, ˆθ)∥p using the fact that the Lp-norm is an increasing function. Now let p = ∞and also let ˆt ∈arg supt∈[a,b] g(t, ˆθBLUE). It follows from the deﬁnition of ˆt that g(t, ˆθBLUE) ≤g(ˆt, ˆθBLUE), for all t ∈[a, b]. Therefore, g(t, ˆθBLUE) ≤g(ˆt, ˆθBLUE) ≤g(ˆt, ˆθ), for all t ∈[a, b] and ˆθ ∈L. This means that (ˆt, ˆθBLUE) ∈[a, b] × L is a saddlepoint and using Lemma 3.1 we obtain inf ˆθ∈L sup t∈[a,b] g(t, ˆθ) = g(ˆt, ˆθBLUE) = sup t∈[a,b] inf ˆθ∈L g(t, ˆθ). Thus ˆθBLUE minimises µ∞(ˆθ) in the class L of pairs of linear unbiased estimators. □ Remark 3.1. Brownian motion is a special case of the general class of triangular kernels which are of the form Ki(t, t′) = ui(t)vi(t′), for t ≤t′ ; i = 1, 2, for each group and the simple kernel KB(t, t′) = t ∧t′ corresponding to the Brownian motion is obtained by choosing ui(t) = t and vi(t) = 1. Dette, Pepelyshev and Zhigljavsky (2016) showed that a representation of the BLUE as a stochastic integral can be obtained for any continuous time model of the form (3.1) with a general triangular kernel. This is achieved by means of a simple non-linear transformation that reduces the model with triangular covariance kernel to a diﬀerent continuous time model with Brownian motion as an error process. In particular, any model of the form (3.1) with εi(t) having a general triangular covariance kernel is equivalent to ˜Yi(˜t) = ˜f T i (˜t)θ + ˜εi(˜t), ˜t ∈[˜a,˜b]; i = 1, 2 , (3.9) where ˜εi(˜t) = εi(t)/vi(t), i = 1, 2, are Brownian motions on the interval [˜a,˜b] and ˜t = qi(t) := ui(t) vi(t), ˜fi(˜t) = fi(t) vi(t), ˜Yi(˜t) = Yi(t) vi(t), i = 1, 2. Hence the ˆθi,BLUE for any continuous time model (3.1) with a general triangular covariance kernel can be obtained from the ˆθi,BLUE in the corresponding model (3.9) by the transformation ˜t = q(t). Therefore, although throughout the theoretical part of this paper we focus on the covariance kernel of the Brownian motion, our methodology is valid for all triangular kernels which in fact represent the majority of covariance kernels considered in the literature. Some examples of kernels other than that of the Brownian",
        "metadata": {
          "authors": "Holger Dette, Kirsten Schorning, Maria Konstantinou",
          "section": "models with dependent observations given by",
          "paper_id": "1601.06722v2",
          "chunk_index": 3,
          "paper_title": "Optimal designs for comparing regression models with correlated observations"
        },
        "id": "chunk_129"
      },
      {
        "text": "Let y ∈Rn be a random data set generated according to a probability distribution f(y; µ), where µ = E(y) is a parameter that we wish to model. A modeling approach M : Rn 7→Rn is a mapping from a training set y to a model ˆµ. Models are evaluated according to an error or loss criterion E(ˆµ, ynew), where ynew (the test set) is also drawn from f, independently of y. Here we focus on the squared-error criterion: E(ˆµ, ynew) = 1 n n X i=1 \u0000ˆµi −ynew i \u00012, (1) although as will be discussed in Section 5, other choices are possible, with most of the theory that follows intact. We follow previous work in examining the in-sample error, where any covariate values are the same for the training and testing data (Efron, 1983; Hastie et al., 2009). In the model selection problem we are given a collection of candidate modeling approaches, and our goal is to select an approach having small risk, i.e., small expected prediction error. A typical setting in which this problem arises is regularization, where a family of nested modeling approaches is considered (Hastie et al., 2009). To apply a modeling approach from the family, ∗shachark@post.tau.ac.il †saharon@post.tau.ac.il 1 arXiv:1311.2791v1 [math.ST] 12 Nov 2013 one ﬁrst has to specify the value of a tuning parameter which controls the amount of ﬁtting to training data. Consider for example the problem of estimating µ by ﬁtting a polynomial to a set of observations {(yi, xi)}n i=1 with least squares. Here the degree of the polynomial, p, plays the role of the tuning parameter, where a higher degree leads to more ﬁtting to training data than does a lower degree. Having speciﬁed p, the least squares optimization problem over the set of polynomials of this degree constitutes a modeling approach, and the solution given speciﬁc training data is a model. Choosing a very low degree (underﬁtting) is undesirable, as some of the information that could be gained from the training data is wasted. A high degree often leads to high variance and overﬁtting, and is also undesirable. Both underﬁtting and overﬁtting lead to",
        "metadata": {
          "chunk_index": 0,
          "section": "introduction",
          "paper_id": "1311.2791v1",
          "authors": "Shachar Kaufman, Saharon Rosset",
          "paper_title": "When Does More Regularization Imply Fewer Degrees of Freedom? Sufficient Conditions and Counter Examples from Lasso and Ridge Regression"
        },
        "id": "chunk_211"
      },
      {
        "text": "Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Conference on Machine Learning, pp. 5301–5310. PMLR, 2019. Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge (less) regression under general source condition. In International Conference on Artificial Intelligence and Statistics, pp. 3889– 3897. PMLR, 2021. 15 Jascha Sohl-Dickstein, Roman Novak, Samuel S Schoenholz, and Jaehoon Lee. On the infinite width limit of neural networks with a standard parameterization. arXiv preprint arXiv:2001.07301, 2020. Peter Sollich. Learning curves for gaussian processes. Advances in Neural Information Processing Systems, pp. 344–350, 1999. Peter Sollich. Gaussian process regression with mismatched models. In Advances in Neural Information Processing Systems, pp. 519–526. MIT Press, 2001. Peter Sollich and Anason Halees. Learning curves for gaussian process regression: Approximations and bounds. Neural computation, 14(6):1393–1428, 2002. Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods: empirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and Experiment, 2020 (12):124001, 2020. Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approximation perspective. arXiv preprint arXiv:1905.10826, 2019. Umberto M Tomasini, Antonio Sclocchi, and Matthieu Wyart. Failure and success of the spectral bias prediction for laplace kernel ridge regression: the case of low-dimensional data. In International Conference on Machine Learning, pp. 21548–21583. PMLR, 2022. Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the parameter- function map is biased towards simple functions. arXiv preprint arXiv:1805.08522, 2018. Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real- world neural representations generalize. In International Conference on Machine Learning, Proceedings of Machine Learning Research, 2022. David H Wolpert. The lack of a priori distinctions between learning algorithms. Neural computation, 8(7): 1341–1390, 1996. Denny Wu and Ji Xu. On the optimal weighted ℓ2 regularization in overparameterized linear regression. Advances in Neural Information Processing Systems, 33:10112–10123, 2020. Lechao Xiao. Eigenspace restructuring: a principle of space and frequency in neural networks. arXiv preprint arXiv:2112.05611, 2021. Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle: Fourier analysis sheds light on deep neural networks. arXiv preprint arXiv:1901.06523, 2019a. Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network in frequency domain. In International Conference on Neural Information Processing, pp. 264–274. Springer, 2019b. Zhiqin John Xu. Understanding training and generalization in deep learning by fourier analysis. arXiv preprint arXiv:1808.04295, 2018. Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks. arXiv preprint arXiv:1907.10599, 2019. 16 A Notation dictionary and comparison with related works Tables 1, 2 and 3 provide a dictionary between the notations of our paper and the nearest related works. Rows should be read in comparison with the top row and interpreted as “when the present paper writes X, the other paper would write Y for the same quantity.\" Comparing expressions for test MSE in Table 2 and predicted function covariance in Table 3 makes it very clear that our learnability framework permits simpler and much more interpretable expression of these results than previously given. Table 1: Notation dictionary between the present paper and related works (part 1). Paper # samples ridge noise eigenvalues eigenfns eigencoeffs (ours) n δ ϵ2 λi ϕi vi Bordelon et al. (2020) p λ NA λρ λ−1/2 ρ ψρ λ1/2 ρ ¯wρ Canatar et al. (2021) P λ σ2 ηρ η−1/2 ρ ψρ η1/2 ρ ¯wρ Jacot et al. (2020) N Nλ ϵ2 dk f (k) ⟨f (k), f ∗⟩ Cui et al. (2021)9 n nλ σ2 ηk ϕk η1/2 k ϑ∗ k Table 2: Notation dictionary between the present paper and related works (part 2). Paper eff. reg. overfitting coeff. test MSE (ours) κ E0 E(f) = E0 \u0000P i(1 −Li)2v2 i + ϵ2\u0001 Bordelon et al. (2020) t+λ n \u0010 1 − pγ (t+λ)2 \u0011−1 Eg = P ρ ¯ w2 ρ λρ \u0010 1 λρ + p λ+t \u0011−2 \u0010 1 − pγ (λ+t)2 \u0011−1 Canatar et al. (2021) κ n 1 1−γ Eg = 1 1−γ P ρ ηρ (κ+P ηρ)2 \u0000κ2 ¯w2 ρ + σ2Pηρ \u0001 Jacot et al. (2020) ϑ ∂λϑ ˜Rϵ = ∂λϑ \u0010 (Ic −˜Aϑ)f ∗ 2 + ϵ2\u0011 Cui et al. (2021)10 z n 1 1−1 n pP k=1 η2 k (z/n+ηk)2 ϵg = z2 n2 ∞ P k=1 ϑ∗ k 2ηk (z/n+ηk)2 +σ2 1−1 n ∞ P k=1 η2 k (z/n+ηk)2 9Cui et al. (2021) use simplifications of the expressions",
        "metadata": {
          "paper_title": "The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks",
          "section": "conclusions",
          "authors": "James B. Simon, Madeline Dickens, Dhruva Karkada, Michael R. DeWeese",
          "chunk_index": 11,
          "paper_id": "2110.03922v6"
        },
        "id": "chunk_500"
      },
      {
        "text": "(from Lemma E.3), and provides deterministic equivalents for resolvents needed to obtain asymptotic bias and variance of stan- dard ridge regression. It is adapted from Lemma S.6.10 of [51]. Lemma E.5 (Deterministic equivalents for ridge resolvents associated with generalized bias and variance). Suppose xi ∈Rp, 1 ≤i ≤n, are i.i.d. random vectors with each xi = ziΣ1/2, where zi ∈Rp is a random vector that contains i.i.d. random variables zij, 1 ≤j ≤p, each with E[zij] = 0, E[z2 ij] = 1, and E[|zij|4+α] ≤Mα for some constants α > 0 and Mα < ∞, and Σ ∈Rp×p is a positive semidefinite matrix with rminIp ⪯Σ ⪯rmaxIp for some constants rmin > 0 and rmax < ∞(independent of p). Let X ∈Rn×p 38 be the concatenated random matrix with xi, 1 ≤i ≤n, as its rows, and define bΣ = X⊤X/n ∈Rp×p. Let γ = p/n. Then, for λ > 0, as n, p →∞with 0 < lim inf γ ≤lim sup γ < ∞, the following statements hold: (1) Bias of ridge regression: λ2(bΣ + λIp)−1A(bΣ + λIp)−1 ≃(v(−λ; γ, Σ)Σ + Ip)−1(evb(−λ; γ, Σ, A)Σ + A)(v(−λ; γ, Σ)Σ + Ip)−1. (2) Variance of ridge regression: (bΣ + λIp)−2 bΣA ≃evv(−λ; γ, Σ)(v(−λ; γ, Σ)Σ + Ip)−2ΣA. Here v(−λ; γ, Σ) > 0 is the unique solution to the fixed-point equation 1 v(−λ; γ, Σ) = λ + Z γr 1 + v(−λ; γ, Σ)r dHn(r; Σ), (40) and evb(−λ; γ, Σ) and evv(−λ; γ, Σ) are defined through v(−λ; γ, Σ) by the following equations: evb(−λ; γ, Σ, A) = γ tr[AΣ(v(−λ; γ, Σ)Σ + Ip)−2]/p v(−λ; γ, Σ)−2 − Z γr2(1 + v(−λ; γ, Σ)r)−2 dHn(r; Σ) , (41) evv(−λ; γ, Σ)−1 = v(−λ; γ, Σ)−2 − Z γr2(1 + v(−λ; γ, Σ)r)−2 dHn(r; Σ), (42) where Hn(·; Σ) is the empirical distribution (supported on [rmin, rmax]) of the eigenvalues of Σ. Though Lemma E.5 states the dependency explicitly, we will simply write Hp(r), v(−λ; γ), evb(−λ; γ, A), and evv(−λ; γ) to denote Hn(r; Σ), v(−λ; γ, Σ), evb(−λ; γ, Σ, A), and evv(−λ; γ, Σ), respectively, for simplifying various notations when it is clear from the context. When A = Σ, we simply write evb(−λ; γ) = evb(−λ; γ, A). F Experiment details F.1 Reproducibility and compute details The source code for generating all experimental figures in this paper can be accessed at: code repository. The source code also includes details about the computational resources used to run the code and other timing details. F.2 Simulation details The covariance matrix of an auto-regressive process of order 1 (AR(1)) is given by Σar1, where (Σar1)ij = ρ|i−j| ar1 for some parameter ρar1 ∈(0, 1). Define β0 = 1 5 P5 j=1 w(j) where w(j) is the eigenvector of Σar1 associated with the top jth eigenvalue r(j). We generated data (xi, yi) for i = 1, . . . , n from a nonlinear",
        "metadata": {
          "paper_title": "Generalized equivalences between subsampling and ridge regularization",
          "chunk_index": 22,
          "section": "results (used in appendices a–d).",
          "paper_id": "2305.18496v2",
          "authors": "Pratik Patil, Jin-Hong Du"
        },
        "id": "chunk_209"
      }
    ]
  },
  "When should I use LASSO vs ridge regression?": {
    "answer": "When deciding between LASSO (Least Absolute Shrinkage and Selection Operator) and Ridge regression, several factors related to the underlying data structure, the desired properties of the model, and computational considerations come into play. Both are regularization techniques applied to linear regression models, aiming to prevent overfitting and improve model generalization by adding a penalty term to the ordinary least squares (OLS) loss function.\n\n### Mathematical Formulations and Definitions\n\nBoth LASSO and Ridge regression extend the standard OLS problem by adding a penalty term to the sum of squared residuals, known as the loss function. The objective is to find the regression parameter vector $\\beta$ that minimizes this penalized loss function:\n\nThe standard linear regression model is given by $Y = X\\beta + \\epsilon$, where $Y$ is the response vector, $X$ is the design matrix, $\\beta$ is the vector of regression parameters, and $\\epsilon$ is the error term. The unpenalized OLS loss function is $\\frac{1}{2n} \\|Y - X\\beta\\|_2^2$, or more generally, $\\|Y - X\\beta\\|_2^2$.\n\n1.  **Ridge Regression**:\n    *   **Penalty Type**: L2 penalty (sum of squared magnitudes of coefficients).\n    *   **Mathematical Formulation**: The Ridge regression estimator $\\hat{\\beta}_{\\text{Ridge}}$ is given by:\n        $$ \\hat{\\beta}_{\\text{Ridge}}(\\lambda_2) = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\|Y - X\\beta\\|_2^2 + \\lambda_2 \\|\\beta\\|_2^2 $$\n        where $\\|\\beta\\|_2^2 = \\sum_{j=1}^p \\beta_j^2$ is the L2-norm squared of the regression coefficients, and $\\lambda_2 \\geq 0$ is the regularization parameter (penalty parameter) controlling the strength of the L2 regularization [3, 5].\n    *   **Effect**: Ridge regression shrinks coefficients towards zero but *does not* set them exactly to zero. This is referred to as \"pure shrinkage\" [2, 5].\n\n2.  **LASSO Regression**:\n    *   **Penalty Type**: L1 penalty (sum of absolute magnitudes of coefficients).\n    *   **Mathematical Formulation**: The LASSO regression estimator $\\hat{\\beta}_{\\text{Lasso}}$ is given by:\n        $$ \\hat{\\beta}_{\\text{Lasso}}(\\lambda_1) = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\|Y - X\\beta\\|_2^2 + \\lambda_1 \\|\\beta\\|_1 $$\n        where $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ is the L1-norm of the regression coefficients, and $\\lambda_1 \\geq 0$ is the regularization parameter controlling the strength of the L1 regularization [2, 3, 5].\n    *   **Effect**: LASSO regression shrinks coefficients and can shrink some coefficients *exactly* to zero, effectively performing variable selection and dimension reduction [1, 5].\n\n### Key Properties and Characteristics\n\n#### LASSO Regression\n*   **Sparsity and Variable Selection**: The L1 penalty encourages coefficients to be exactly zero, leading to a sparse model where \"insignificant\" predictors are eliminated. This is a thresholding-type procedure [1, 5].\n*   **Shrinkage Behavior**: Applies an equal amount of shrinkage to each element of $\\beta$, irrespective of the coefficients' sizes [2].\n*   **Handling Correlated Covariates**: In the presence of strongly collinear covariates, LASSO tends to pick one covariate to enter the model while forcing the others out (by setting their estimates to zero) [2, 3].\n*   **Covariates with Differing Variances**: Shrinks covariates with the smallest spread the most. For sufficiently large penalty parameters, it shows a preference for selecting covariates with the largest variance [2].\n*   **Stability in High Dimensions**: In high-dimensional settings (large $p$), the variable selection performed by LASSO can be unstable and less reproducible [1]. It has a bound on the number of selected variables, maximally $\\min\\{n, p\\}$ [3].\n*   **Convexity**: The LASSO loss function is notably non-strictly convex when the dimension $p$ exceeds the sample size $n$ [3].\n\n#### Ridge Regression\n*   **No Variable Selection (Pure Shrinkage)**: Ridge regression shrinks coefficients towards zero but never precisely to zero. It does not eliminate predictors from the model [2, 5].\n*   **Joint Shrinkage of Correlated Covariates**: Exhibits joint shrinkage of strongly collinear covariates. It does not select among them but rather shrinks them together [2, 3].\n*   **Shrinkage Behavior**: Shrinks the estimates of covariates with a large spread less than those with a small spread, in contrast to LASSO's equal shrinkage [2].\n*   **Robustness and Reproducibility**: Can be conceived as doing some form of averaging, which is relatively robust and reproducible, especially in high-dimensional settings [1].\n*   **Convexity**: The Ridge penalty introduces strict convexity to the loss function, which warrants the existence of a unique minimizer [3].\n*   **Computational Efficiency**: Ridge regression is easily computed using a closed-form expression [4], offering computational gains in terms of speed and simplicity compared to sparse shrinkage methods [5]. It can efficiently calculate posterior mean and covariance estimates of $\\beta$ and auxiliary statistics with the help of the singular value decomposition (SVD) of $X$, avoiding computationally expensive matrix operations [5].\n\n### When to Use LASSO vs. Ridge Regression\n\nThe choice between LASSO and Ridge regression often depends on assumptions about the underlying data structure, particularly the sparsity of the true regression parameters and the presence of collinearity.\n\n1.  **Sparsity and Variable Selection Requirement**:\n    *   **Use LASSO when**: You believe the system under study is *sparse*, meaning only a small subset of the covariates truly contribute to the response, and you want to perform variable selection, shrinking irrelevant coefficients to exactly zero [1, 5]. This is useful for model interpretability or when identifying a parsimonious set of predictors is a goal (e.g., finding a \"signature\" of genes in breast cancer research) [1].\n    *   **Use Ridge when**: You assume that *all or many covariates contribute* to the explanation of the variation of the response, and no coefficient is precisely zero [1, 5]. If it is argued that no continuous quantity like a slope coefficient is ever precisely zero, pure shrinkage (Ridge) may be preferred over sparse shrinkage (LASSO) [5].\n\n2.  **High-Dimensional Settings ($p > n$) and Predictive Performance**:\n    *   **Use Ridge when**: The dimension $p$ is very large (e.g., $p > 500$ in some simulations) [1]. As the dimension grows, the predictive performance of Ridge regression estimates tends to surpass that of LASSO [1]. Ridge regression is robust and reproducible in high-dimensional settings, effectively handling cases where $p$ is much larger than $n$ [1, 5]. Ridge regression methods also tend to have lower prediction error compared to sparse shrinkage methods [5].\n    *   **Use LASSO when**: The dimension $p$ is relatively small (e.g., $p = 50$ or $p = 250$ in simulations), where its predictive performance might be superior or comparable to Ridge [1]. However, in very high-dimensional settings, LASSO's selection property can become unstable [1].\n\n3.  **Presence of Collinearity Among Covariates**:\n    *   **Use Ridge when**: You have groups of highly correlated covariates, and you want to keep all of them in the model, shrinking their coefficients jointly. Ridge regression is effective at handling collinearity by spreading the impact across all correlated variables [2, 3].\n    *   **Use LASSO when**: You have collinear covariates but prefer to select only one representative from a highly correlated group, effectively discarding the others [2, 3].\n\n4.  **Computational Considerations**:\n    *   **Use Ridge when**: Computational speed and simplicity are high priorities, as Ridge regression has a closed-form solution and offers computational gains [4, 5].\n\n5.  **Robustness to Parameter Choice**:\n    *   **Improved Ridge**: An \"improved\" (debiased and thresholded) Ridge regression method has been shown to have comparable performance to threshold LASSO while being quite robust against a non-optimal choice of its regularization parameter $\\rho_n$ [4]. This suggests that with modern enhancements, Ridge regression can be very competitive even when sparsity is desired.\n\nIn summary, LASSO is generally preferred when true sparsity is believed to exist and variable selection is a primary goal, especially in lower-to-moderate dimensional settings. Ridge regression is often preferred in high-dimensional settings, when many covariates are expected to contribute to the response, or when dealing with highly correlated predictors, due to its robustness, joint shrinkage properties, and often superior predictive performance in such scenarios.\n\nIt is important to note that these insights are based on simulations and theoretical properties. As highlighted in one paper, simulations are \"far too limited in nature\" and only illustrate the dependence of behavior under various choices of the regression parameter. The choice for either estimator comes with an implicit belief on the structure of the regression parameter [1]. For situations where both sparsity and grouped selection of correlated variables are desired, the **Elastic Net** regression estimator, which combines both L1 and L2 penalties, can be a valuable alternative, encompassing both LASSO ($\\lambda_2=0$) and Ridge ($\\lambda_1=0$) as special cases [3].",
    "chunks": [
      {
        "text": "thresholding-type procedure. We have repeated the simulation but fix the sparsity in a relative sense instead of an absolute one. That is, the relative sparsity is fixed at 10%, i.e. ⌈p/10⌉elements of the regression parameter are non-zero, instead of the five non-zero elements irrespective of the dimension. The elements of the regression parameter are set to βj = 500 j p−7/4 for j = 1, . . . , ⌈p/10⌉and βj = 0 for j = ⌈p/10⌉+ 1, . . . , p. The particular dependence of the nonzero elements of the regression parameter on the dimension is a clumsy attempt to fix the variance of the response over the employed range of dimensions. The latter now ranges over p ∈{50, 250, 500, 750, . . . , 2500}. Figure 6.12 shows the violinplots of the resulting thousand Spearman’s rank correlations between the predictions and observations of the lasso and ridge estimates for the various dimensions. For small dimensions, p = 50 and p = 250, the predictive performance of the ridge regression estimates falls behind that of its lasso counterpart. But as the dimension grows beyond p = 500, the roles reverse and the ridge regression estimate performs better than its lasso counterpart. Let us close with some intuition why we see those results. The lasso regression estimator selects, and thereby performs dimension reduction. This may be unstable and thereby less reproducible in high-dimensional settings. The ridge regression estimator can be conceived as doing some form of averaging, which is relatively robust and reproducible. In all, these simulations may suggest that overall the ‘ridge predictor’ tends to have a better predictive perfor- mance than its lasso counterpart. Should we prefer thus the ridge over the lasso regression estimator? No, that would be shortsighted. The simulations are far too limited in nature, they are only meant to illustrate the depen- dence of the behavior of the estimators under various choices of the regression parameter. One may interpret the 126 Lasso regression Spearman correlation of obs. and cv−prediction, 10% sparse β cor(Y,Y^) −1.0 −0.5 0.0 0.5 1.0 p=50 p=250 p=500 p=750 p=1000 p=1250 p=1500 p=1750 p=2000 p=2250 p=2500 p=50 p=250 p=500 p=750 p=1000 p=1250 p=1500 p=1750 p=2000 p=2250 p=2500 lasso ridge Figure 6.12: Violinplots of the predictive performance, operationalized as Spearman’s rank correlation among observation and prediction, of the lasso and ridge regression estimates with fixed relative sparsity at 10% and increasing p. simulations as suggesting that a choice for either estimator comes with a believe on the structure of the regres- sion parameter. E.g., the lasso regression estimator implicitly assumes the system under study is sparse (and the nonzero elements are clearly distinguishable – in some sense – from zero). The validity of this assumption is not directly obvious is, e.g., biological phenomena. Similarly, the ridge regression estimator implicity assumes all (or at least many) covariates contribute to the explanation of the variation of the response. Such an assumption too is difficult to verify. 6.10 Application A seminal high-dimensional data set, that has often been re-analyzed in many articles to illustrate novel (regular- ization) methods, is presented in Van’t Veer et al. (2002); Van De Vijver et al. (2002). It is part of a study into breast cancer and comprises the overall survival information and expression levels of 24158 genes of 291 breast cancer samples. Alongside the high-throughput omics data, clinical information like age and sex is available but discarded here. The data are provided via the breastCancerNKI-package (Schroeder et al., 2022). In the original work of Van’t Veer et al. (2002); Van De Vijver et al. (2002) the data are used to build a linear predictor of overall survival. This resulted in a so-called ‘signature’ of 70 genes – quite a dimension reduction! – that together are predictive of overall survival. Upon this work, a commercial enterprise has been founded that introduced a diagonostic test called the Mammaprint (https://en.wikipedia.org/wiki/ MammaPrint). In a nutshell, the workings of the test (essentially) amount to the evaluation of the linear predictor, which is subsequently compared to a reference value to decide on the expected survival outcome. Within the context of our data set, the reference value will be the median of all linear predictions: \u001a poor prognosis if Xi,∗ˆβ > median{X1,∗ˆβ, . . . , Xn,∗ˆβ}, good prognosis if Xi,∗ˆβ < median{X1,∗ˆβ, . . . , Xn,∗ˆβ}. The prognosis determines the individual’s follow-up treatment. For instance, should the test indicate a ‘good prognosis’, the individual may be spared chemo-therapy without a substantial overall survival reduction but a considerable gain in quality of life. 6.10 Application 127 100 200 500 1000 2000 5000 10000 above median(lp) below median(lp) events only observed survival time 0 200 400 600 800 1000 2000 3000 4000 5000 events only expected survival time observed survival time Figure 6.13: State",
        "metadata": {
          "authors": "Wessel N. van Wieringen",
          "paper_title": "Lecture notes on ridge regression",
          "paper_id": "1509.09169v8",
          "section": "algorithm.",
          "chunk_index": 38
        },
        "id": "chunk_261"
      },
      {
        "text": "substantial and little shrinkage (in an absolute sense) for elements of the regression parameter β with a large and small maximum likelihood estimate, respectively. In contrast, the lasso regression estimator applies an equal amount of shrinkage to each element of β, irrespective of the coefficients’ sizes. Figure 6.9: Solution path of the lasso and ridge regression estimators, left and right panel, respectively, for data with an orthonormal design matrix. 6.9.3 Simulation I: covariate selection Here it is investigated whether lasso regression exhibits the same behaviour as ridge regression in the presence of covariates with differing variances. Recall: the simulation of Section 1.9.1 showed that ridge regression shrinks the estimates of covariates with a large spread less than those with a small spread. That simulation has been repeated, with the exact same parameter choices and sample size, but now with the ridge regression estimator replaced by the lasso regression estimator. To refresh the memory: in the simulation of Section 1.9.1 the linear regression model is fitted, now with the lasso regression estimator. The (n = 1000) × (p = 50) dimensional design matrix X is sampled from a multivariate normal distribution: X⊤ i,∗∼N (050, Σ) with Σ diagonal and (Σ)jj = j/10 for j = 1, . . . , p. The response Y is generated through Y = Xβ + ε with β a vector of all ones and ε sampled from the multivariate standard normal distribution. Hence, all covariates contribute equally to the response. The results of the simulation are displayed in Figure 6.10, which shows the regularization paths of the p = 50 covariates. The regularization paths are demarcated by color and style to indicate the size of the spread of the corresponding covariate. These regularization paths show that the lasso regression estimator shrinks – like the ridge regression estimator – the covariates with the smallest spread most. For the lasso regression this translates (for sufficiently large values of the penalty parameter) into a preference for the selection of covariates with largest variance. Intuition for this behavior of the lasso regression estimator may be obtained through geometrical arguments analogous to that provided for the similar behaviour of the ridge regression estimator in Section 1.9.1. Alge- braically it is easily seen when assuming an orthogonal design with Var(X1) ≫Var(X2). The lasso regression loss function can then be rewritten, as in Example 6.5, to: ∥Y −Xβ∥2 2 + λ1∥β∥1 = ∥Y −˜Xγ∥2 2 + λ1[Var(X1)]−1/2|γ1| + λ1[Var(X2)]−1/2|γ2|, where γ1 = [Var(X1)]1/2β1 and γ2 = [Var(X2)]1/2β2. The rescaled design matrix ˜X is now orthonormal and analytic expressions of estimators of γ1 and γ2 are available. The former parameter is penalized substantially less than the latter as λ1[Var(X1)]−1/2 ≪λ1[Var(X2)]−1/2. As a result, if for large enough values of λ1 one variable is selected, it is more likely to be γ1. 6.9 Comparison to ridge 123 Figure 6.10: Regularization paths of the lasso regression estimator related to the simulations presented in Sections 6.9.3 and 6.9.4 in the left and right panel, respectively. 6.9.4 Simulation II: correlated covariates The behaviour of the lasso regression estimator is now studied in the presence of collinearity among the covariates. Previously, in simulation, Section 1.9.2, the ridge regression estimator was shown to exhibit the joint shrinkage of strongly collinear covariates. This simulation is repeated for the lasso regression estimator. The details of the simulation are recapped. The linear regression model is fitted by means of the lasso regression estimator. The (n = 1000) × (p = 50) dimensional design matrix X is samples from a multivariate normal distribution: X⊤ i,∗∼N (050, Σ) with a block-diagonal Σ. The k-the, k = 1, . . . , 5, diagonal block, denoted Σkk comprises ten covariates and equals k−1 5 110×10 + 6−k 5 I10×10 for k = 1, . . . , 5. The response vector Y is then generated by Y = Xβ + ε, with ε sampled from the multivariate standard normal distribution and β containing only ones. Again, all covariates contribute equally to the response. The results of the above simulation results are captured in Figure 6.10. It shows the lasso regularization paths for all elements of the regression parameter β. The regularization paths of covariates corresponding to the same block of Σ (indicative of the degree of collinearity) are now marcated by different colors and styles. Whereas the ridge regularization paths nicely grouped per block, the lasso counterparts do not. The selection property spoils the party. Instead of shrinking the regression parameter estimates of collinear covariates together, the lasso regression estimator (for sufficiently large values of its penalty parameter λ1) tends to pick one covariates to enters the model while forcing the others out (by setting their estimates to zero). 6.9.5 Simulation III: sparsity Our next simulation investigates the effect of sparsity on the predictive performance",
        "metadata": {
          "authors": "Wessel N. van Wieringen",
          "paper_title": "Lecture notes on ridge regression",
          "paper_id": "1509.09169v8",
          "chunk_index": 36,
          "section": "algorithm."
        },
        "id": "chunk_259"
      },
      {
        "text": "ii) the bound on the number of selected variables, i.e. maximally min{n, p} can be selected, and iii) the observation that strongly (positively) collinear covariates are not shrunken together: the lasso regression estimator selects among them while it is hard to distinguish their contributions to the variation of the response. While it does not select, the ridge regression estimator does not exhibit these less pleasing features. These consid- erations led Zou and Hastie (2005) to combine the strengths of the lasso and ridge regression estimators and form a ‘best-of-both-worlds’ estimator, called the elastic net regression estimator, defined as: ˆβ(λ1, λ2) = arg min β∈Rp ∥Y −Xβ∥2 2 + λ1∥β∥1 + 1 2λ2∥β∥2 2. The elastic net penalty – defined implicitly in the preceeding display – is thus simply a linear combination of the lasso and ridge penalties. Consequently, the elastic net regression estimator encompasses its lasso and ridge counterparts. Hereto just set λ2 = 0 or λ1 = 0, respectively. A novel estimator is defined if both penalties act simultaneously, i.e. if their corresponding penalty parameters are both nonzero. Does this novel elastic net estimator indeed inherit the strengths of the lasso and ridge regression estimators? Let us turn to the aforementioned motivation behind the elastic net estimator. Starting with the uniqueness, the strict convexity of the ridge penalty renders the elastic net loss function strictly convex, as it is a combination of the ridge penalty and the lasso function – notably non-strict convex when the dimension p exceeds the sample size n. This warrants the existence of a unique minimizer of the elastic net loss function. To assess the preservation of the selection property, now without the bound on the maximum number of selectable variables, exploit the equivalent constraint estimation formulation of the elastic net estimator. Figure 7.1 shows the parameter constraint of the elastic net estimator for the ‘p = 2’-case, which is defined by the set: {(β1, β2) ∈R2 : λ1(|β1| + |β2|) + 1 2λ2(β2 1 + β2 2) ≤c(λ1, λ2)}. Visually, the ‘elastic net parameter constraint’ is a compromise between the circle and the diamond shaped con- straints of the ridge and lasso regression estimators. This compromise inherits exactly the right geometrical fea- tures: the strict convexity of the ‘ridge circle’ and the ‘corners’ (referring to points at which the constraint’s boundary is non-smootness/non-differentiability) falling at the axes of the ‘lasso constraint’. The latter feature, by the same argumentation as presented in Section 6.3, endows the elastic net estimator with the selection property. Moreover, it can – in principle – select p features as the point in the parameter space where the smallest level set of the unpenalized loss hits the elastic net parameter constraint need not fall on any axis. For example, in the ‘p = 2, n = 1’-case the level sets of the sum-of-squares loss are straight lines that, if running almost parallel to the edges of the ‘lasso diamond’, are unlikely to first hit the elastic net parameter constraint at one of its corners. Finally, the largest penalty parameter relates (reciprocally) to the volume of the elastic net parameter constraint, while the ratio between λ1 and λ2 determines whether it is closer to the ‘ridge circle’ or to the ‘lasso diamond’. Whether the elastic net regression estimator also delivers on the joint shrinkage property is assessed by simu- lation (not shown). The impression given by these simulations is that the elastic net has joint shrinkage potential. This, however, usually requires a large ridge penalty, which then dominates the elastic net penalty. 134 Generalizing lasso regression β1 β2 −1.0 −0.5 0.0 0.5 1.0 −1.0 −0.5 0.0 0.5 1.0 lasso ridge elastic net Figure 7.1: The left panel depicts the parameter constraint induced by the elastic net penalty and, for reference, those of the lasso and ridge are added. The right panel shows the contour plot of the cross-validated loglikelihood vs. the two penalty parameters of the elastic net estimator. The elastic net regression estimator can be found with procedures similar to those that evaluate the lasso regression estimator (see Section 6.4) as the elastic net loss can be reformulated as a lasso loss. Hereto the ridge part of the elastic net penalty is absorbed into the sum of squares using the data augmentation trick of Exercise 1.6 which showed that the ridge regression estimator is the ML regression estimator of the related regression model with p zeros and rows added to the response and design matrix, respectively. That is, write ˜Y = (Y⊤, 0⊤ p )⊤and ˜X = (X⊤, √λ2 Ipp)⊤. Then: ∥˜Y −˜Xβ∥2 2 = ∥Y −Xβ∥2 2 + λ2∥β∥2 2. Hence, the elastic net loss function can be rewritten to ∥˜Y −˜Xβ∥2 2 + λ1∥β∥1. This is familiar territory and the lasso algorithms of Section 6.4 can be used. Zou",
        "metadata": {
          "paper_title": "Lecture notes on ridge regression",
          "paper_id": "1509.09169v8",
          "chunk_index": 43,
          "section": "algorithm.",
          "authors": "Wessel N. van Wieringen"
        },
        "id": "chunk_266"
      },
      {
        "text": "The paper at hand proposes an improved, i.e., debiased and thresholded, ridge regression method that recovers the sparsity of parameters and avoids introducing a large bias. Besides, it derives a consistency result and the Gaussian approximation theorem for the improved ridge estimator. An asymptotically valid conﬁdence region for γ = Mβ and a hypothesis test of γ = γ0 are also constructed based on a wild bootstrap algorithm. In addition, a novel, hybrid resampling procedure was proposed that can be used to perform interval prediction based on the improved ridge regression. Numerical simulations indicate that improved ridge regression has comparable performance to the threshold Lasso while having at least two major advantages: (a) Ridge regression is easily computed using a closed-form expression, and (b) it appears to be quite robust against a non-optimal choice of the ridge parameter ρn. Therefore, ridge regression may be found useful again in applied work using high-dimensional data as long as practitioners make sure to include debiasing and thresholding. 15 References [1] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):267–288, 1996. [2] Nicolai Meinshausen and Peter B¨uhlmann. High-dimensional graphs and variable selection with the lasso. Ann. Statist., 34(3):1436–1462, 06 2006. [3] Nicolai Meinshausen and Bin Yu. Lasso-type recovery of sparse representations for high-dimensional data. Ann. Statist., 37(1):246–270, 02 2009. [4] Sara A. van de Geer. High-dimensional generalized linear models and the lasso. Ann. Statist., 36(2):614–645, 04 2008. [5] A. Chatterjee and S. N. Lahiri. Asymptotic properties of the residual bootstrap for lasso estimators. Proceedings of the American Mathematical Society, 138(12):4497–4509, 2010. [6] A. Chatterjee and S. N. Lahiri. Bootstrapping lasso estimators. Journal of the American Statistical Association, 106(494):608–625, 2011. [7] Xianyang Zhang and Guang Cheng. Simultaneous inference for high-dimensional linear models. Journal of the American Statistical Association, 112(518):757–768, 2017. [8] Ruben Dezeure, Peter B¨uhlmann, and Cun-Hui Zhang. High-dimensional simultaneous inference with the bootstrap. TEST, 26(4):685–719, Dec 2017. [9] Adel Javanmard and Andrea Montanari. Debiasing the lasso: Optimal sample size for gaussian designs. Ann. Statist., 46(6A):2593–2622, 12 2018. [10] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348–1360, 2001. [11] Xi Chen and Wen-Xin Zhou. Robust inference via multiplier bootstrap. Ann. Statist., 48(3):1665–1691, 06 2020. [12] Sara van de Geer, Peter B¨uhlmann, and Shuheng Zhou. The adaptive and the thresholded Lasso for potentially misspeciﬁed models (and a lower bound for the Lasso). Electronic Journal of Statistics, 5(none):688 – 749, 2011. [13] Peter B¨uhlmann and Sara van de Geer. Statistics for High-Dimensional Data: Methods, Theory and Applications. Springer-Verlag Berlin Heidelberg, 1st edition, 2011. [14] Sara van de Geer. On the asymptotic variance of the debiased Lasso. Electronic Journal of Statistics, 13(2):2970 – 3008, 2019. [15] Adel Javanmard and Hamid Javadi. False discovery rate control via debiased lasso. Electronic Journal of Statistics, 13(1):1212 – 1253, 2019. [16] Jason D. Lee, Dennis L. Sun, Yuekai Sun, and Jonathan E. Taylor. Exact post-selection inference, with application to the lasso. Ann. Statist., 44(3):907–927, 06 2016. 16 [17] Hanzhong Liu and Bin Yu. Asymptotic properties of lasso+mls and lasso+ridge in sparse high-dimensional linear regression. Electron. J. Statist., 7:3124–3169, 2013. [18] Ryan J. Tibshirani, Alessandro Rinaldo, Rob Tibshirani, and Larry Wasserman. Uniform asymptotic inference and the bootstrap after model selection. Ann. Statist., 46(3):1255–1287, 06 2018. [19] Jun Shao and Xinwei Deng. Estimation in high-dimensional linear models with deterministic design matrices. Ann. Statist., 40(2):812–831, 04 2012. [20] Linlin Dai, Kani Chen, Zhihua Sun, Zhenqiu Liu, and Gang Li. Broken adaptive ridge regression and its asymptotic properties. Journal of Multivariate Analysis, 168:334 – 351, 2018. [21] Edgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge regression and classiﬁcation. Ann. Statist., 46(1):247–279, 02 2018. [22] Peter B¨uhlmann. Statistical signiﬁcance in high-dimensional linear models. Bernoulli, 19(4):1212–1242, 09 2013. [23] Miles Lopes. A residual bootstrap for high-dimensional regression with near low-rank designs. In Advances in Neural Information Processing Systems 27, pages 3239–3247, 2014. [24] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 2012. [25] C. F. J. Wu. Jackknife, Bootstrap and Other Resampling Methods in Regression Analysis. The Annals of Statistics, 14(4):1261 – 1295, 1986. [26] Regina Y. Liu. Bootstrap Procedures under some Non-I.I.D. Models. The Annals of Statistics, 16(4):1696 – 1708, 1988. [27] Enno Mammen. Bootstrap and Wild Bootstrap for High Dimensional Linear Models. The Annals of Statistics, 21(1):255 – 285, 1993. [28] Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors. Ann. Statist., 41(6):2786–2819, 12 2013. [29] Juan J. Dolado and Helmut L¨utkepohl. Making wald tests work for cointegrated var systems. Econometric Reviews, 15(4):369–386, 1996. [30] Yixiao Sun. Robust trend inference with series variance estimator and testing-optimal smoothing parameter. Journal of Econometrics, 164(2):345 – 366, 2011.",
        "metadata": {
          "paper_id": "2009.08071v2",
          "section": "conclusion",
          "chunk_index": 9,
          "paper_title": "Ridge Regression Revisited: Debiasing, Thresholding and Bootstrap",
          "authors": "Yunyi Zhang, Dimitris N. Politis"
        },
        "id": "chunk_339"
      },
      {
        "text": "can each shrink coeﬃcient estimates to exactly zero to eliminate (zero-out) ”insigniﬁcant” predictors (covariates) in the linear model, according to the penalized least squares estimator βλ,α = arg min β ||y −Xβ||2 + λ Pp k=1 \u0010 (1−α) 2 β2 k + α|βk| \u0011 . (5) Above, 0 ≤α ≤1, and the choice α = 1 deﬁnes the LASSO estimator, and α = 0 deﬁnes a RR estimator, while the choice α = 1/2 is viewed as a reasonable compromise (Zou & Hastie, 2005). For either the LASSO or the EN, the shrinkage parameter λ is usually estimated either as the minimizer of 10-fold cross validated mean-square predictive error, or by minimizing a generalized information criterion GIC(λ) (Fan & Tang, 2013). Sparse shrinkage methods also include Bayesian variable selection linear models that are equipped with indicator (0,1) parameters that set slope coeﬃcients to zero with positive prior probability (e.g., Bottolo & Richardson, 2012). However, from a Bayesian perspective, there are at least three good reasons why shrink- age methods like ridge regression may be preferred over the sparse shrinkage methods. First, it may be argued that no continuous quantity like a slope coeﬃcient βk is ever precisely zero (Draper, 1999). Similarly, for a Bayesian variable selection model, the posterior average of β is surely non-zero, after averaging over the indicator parameters (Polson & Scott 2012). Then point-null hypotheses of the form βk = 0 seem unrealistic, and the posterior average of β obtained under either of these methods may be practically indistinguishable from the posterior average of β obtained under a pure shrinkage method. Second, a pure shrinkage method can provide computational gains over sparse shrinking and Bayesian variable selec- tion (model averaging), in terms of speed and simplicity (Polson & Scott 2012). Moreover, shrinkage regression models, in part because they never entirely eliminate (zero-out) predic- tors, tend to have lower prediction error compared to sparse shrinkage regression methods 8 (e.g., Tibshirani, 1996). In the following sections, we describe and illustrate the marginal maximum likelihood estimation methodology, for each of the Bayesian RR, PRR, and GRR models. In order to provide full context for these models, Section 2.1 provides a brief review of the classical in- ferential theory of the Bayesian linear model (e.g., Denison, et al. 2002; O’Hagan & Forster, 2004). Then in Section 2.2, we deﬁne the Bayesian RR, PRR, and GRR models. There we show that for each of these Bayesian models, the posterior mean and covariance estimates of βλ, and auxiliary statistics such as posterior intervals and SN criteria, can be eﬃciently calcu- lated the help of the s.v.d. of X, without needing to evaluate any computationally-expensive matrix operations such as inverses, determinants, and large-scale matrix multiplications. In practice, this is an important feature when either p or n is large. Then in Section 2.3, we show that for each of the three Bayesian ridge models, the marginal density π(Dn | λ) can be simpliﬁed into an equation involving no matrix operations whatsoever, after taking a s.v.d. of X. We will also show how the marginal maximum likelihood estimate bλ can be obtained for each of the three Bayesian ridge models, under the Bayes empirical Bayes framework (Deely & Lindley, 1981). As mentioned, in this study we will introduce fast iterative algorithms for estimating the parameters of the Bayesian RR and PRR models. For the Bayesian GRR model, we present a set of closed-form equations to provide a non-iterative and automatic plug-in estimator of bλ, leading to estimates of the coeﬃcients and error variance. For each of the three Bayesian ridge models, the estimate bλ provides a basis to directly calculate the posterior mean and covariance estimate of (β, σ2), as well as the auxiliary statistics. In Section 3, we brieﬂy review the literature to support the idea that the Bayesian GRR model, including the Bayesian RR and PRR model, has a large scope for applied statistics, because of the model’s ability to handle data with a very large number of covariates p (e.g., hundreds or thousands) and corresponding slope coeﬃcient parameters. Ridge regression can provide a ﬂexible and approximate Bayesian nonparametric modeling (M¨uller & Quintana, 2004), even for linear classiﬁcation of a binary dependent variable. Section 4 illustrates the marginal likelihood estimation methodology for the three Bayesian ridge models, through the analysis of 10 real data sets. For nearly all these data sets, the number of covariates (p) range from several hundred to around ﬁfteen thousand. Several of these data sets involve more covariates than observations (i.e., p > n). We will also compare the Bayesian RR, PRR, and GRR models under marginal likelihood estimation, with other approaches. They include the Bayesian RR model with ridge parameter λ estimated either by (1) the extended HKB plug-in estimator (Cule & DeIorio, 2013); or (2) by maximizing",
        "metadata": {
          "section": "introduction",
          "paper_title": "Fast Marginal Likelihood Estimation of the Ridge Parameter(s) in Ridge Regression and Generalized Ridge Regression for Big Data",
          "paper_id": "1409.2437v5",
          "chunk_index": 5,
          "authors": "George Karabatsos"
        },
        "id": "chunk_289"
      }
    ]
  },
  "What is elastic net regression?": {
    "answer": "Elastic Net regression is a regularization method that combines the strengths of the Lasso (Least Absolute Shrinkage and Selection Operator) and Ridge regression estimators [2]. It was proposed by Zou and Hastie (2005) to address certain limitations of the Lasso estimator, particularly when dealing with strongly collinear covariates or when the number of covariates `p` exceeds the number of samples `n` [2].\n\n**Mathematical Formulation**\n\nThe elastic net regression estimator, denoted as $\\hat{\\beta}(\\lambda_1, \\lambda_2)$, is defined as the minimizer of the following penalized least-squares loss function:\n$$ \\hat{\\beta}(\\lambda_1, \\lambda_2) = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\|Y - X\\beta\\|_2^2 + \\lambda_1 \\|\\beta\\|_1 + \\frac{1}{2}\\lambda_2 \\|\\beta\\|_2^2 \\quad [2] $$\nHere:\n*   $\\|Y - X\\beta\\|_2^2$ is the ordinary least squares (OLS) sum-of-squares loss function, where $Y$ is the response vector and $X$ is the design matrix.\n*   $\\lambda_1 \\|\\beta\\|_1$ is the L1-norm penalty, also known as the Lasso penalty, which is the sum of the absolute values of the regression coefficients: $\\lambda_1 \\sum_{j=1}^p |\\beta_j|$.\n*   $\\frac{1}{2}\\lambda_2 \\|\\beta\\|_2^2$ is the L2-norm penalty, also known as the Ridge penalty, which is half the sum of the squared values of the regression coefficients: $\\frac{1}{2}\\lambda_2 \\sum_{j=1}^p \\beta_j^2$.\n*   $\\lambda_1$ and $\\lambda_2$ are non-negative penalty parameters that control the amount of regularization [2].\n\nThe elastic net penalty is thus a linear combination of the Lasso and Ridge penalties [2]. By setting $\\lambda_2 = 0$, the estimator reverts to the Lasso regression estimator. By setting $\\lambda_1 = 0$, it becomes the Ridge regression estimator [2]. A novel elastic net estimator is defined when both penalty parameters are non-zero [2].\n\n**Motivation and Properties**\n\nElastic Net regression was developed to inherit desirable properties from both Lasso and Ridge regression:\n\n1.  **Addressing Lasso's Limitations:**\n    *   **Limited Variable Selection:** The Lasso estimator can select at most $\\min\\{n, p\\}$ variables. Elastic Net overcomes this, as it can select `p` variables even when `p > n` [1, 2]. This is evident from its reformulation in terms of augmented data, where the rank of the augmented design matrix is `p` [1].\n    *   **Collinearity Issues:** When covariates are strongly (positively) collinear, Lasso tends to select only one among them and shrink the others to zero, making it difficult to distinguish their contributions. Ridge regression, in contrast, shrinks collinear coefficients together. Elastic Net aims to combine this \"joint shrinkage\" potential [2].\n\n2.  **Uniqueness of Minimizer:** The inclusion of the Ridge penalty (1/2 $\\lambda_2 \\|\\beta\\|_2^2$) makes the elastic net loss function strictly convex. This strict convexity guarantees the existence of a unique minimizer for the elastic net loss function, unlike the Lasso loss function which can be non-strictly convex when `p > n` [2].\n\n3.  **Variable Selection (Sparsity):** The Lasso penalty ($\\lambda_1 \\|\\beta\\|_1$) endows the elastic net estimator with the property of variable selection, meaning it can shrink some coefficients exactly to zero, leading to sparse models [2]. This is graphically represented by the \"corners\" in its parameter constraint, similar to the diamond-shaped constraint of Lasso (Figure 7.1) [2].\n\n4.  **Joint Shrinkage:** Elastic Net has the potential for joint shrinkage of coefficients of collinear covariates, a characteristic inherited from Ridge regression. This typically requires a sufficiently large Ridge penalty ($\\lambda_2$) to dominate the elastic net penalty [1, 2].\n\n5.  **Shrinkage Effect:** Both the Lasso and Ridge penalties contribute to shrinking the size of the estimator. Increasing either $\\lambda_1$ or $\\lambda_2$ will result in a more shrunken elastic net estimator [1]. The \"size\" of $\\beta$ refers to the quantity $\\lambda_1\\|\\beta\\|_1 + \\frac{1}{2}\\lambda_2\\|\\beta\\|_2^2$ [1].\n\n**Parameter Constraints**\n\nGeometrically, the elastic net parameter constraint is a compromise between the circular constraint of Ridge regression and the diamond-shaped constraint of Lasso regression (Figure 7.1). It combines the strict convexity of the \"ridge circle\" with the \"corners\" (non-differentiable points) at the axes of the \"lasso constraint.\" The ratio between $\\lambda_1$ and $\\lambda_2$ determines whether the constraint is closer to the \"ridge circle\" or the \"lasso diamond\" [2].\n\n**Tuning Parameters and Empirical Indeterminacy**\n\nThe penalty parameters $\\lambda_1$ and $\\lambda_2$ (or equivalently, $\\lambda$ and $\\alpha$ in an alternative formulation) require tuning, typically through cross-validation [1].\n\n*   **Mixing Parameter $\\alpha$**: For interpretation purposes, the elastic net penalty can be rewritten as $\\lambda[\\alpha\\|\\beta\\|_1 + \\frac{1}{2}(1-\\alpha)\\|\\beta\\|_2^2]$, where $\\alpha \\in [0, 1]$ is a \"mixing parameter\" and $\\lambda > 0$ is a global penalty parameter [1]. The mixing parameter $\\alpha$ is often set by the user (or tuned), while $\\lambda$ is found through cross-validation [1]. However, no general guidance on choosing $\\alpha$ can be given as it is a tuning parameter itself [1].\n\n*   **Empirical Indeterminacy**: A significant characteristic of elastic net is the \"empirical indeterminacy\" of its penalty parameters. Often, a large range of $(\\lambda_1, \\lambda_2)$ combinations can yield a similar optimal cross-validated performance. This can be seen in contour plots (Figure 7.1) as a \"banana-shaped\" area corresponding to the same optimal performance [1]. This behavior arises because the lasso and ridge penalties, while acting in different norms, have a similar effect on the size of the estimator. Many positive $(\\lambda_1, \\lambda_2)$ combinations can constrain the estimator's size equally, leading to comparable performance [1]. This indeterminacy suggests that for any sparse elastic net estimator, a less sparse one can be found with comparable performance, and vice versa. Thus, caution is advised when concluding on the sparsity of the true underlying linear relationship based solely on the chosen elastic net estimator [1].\n\n**Computational Aspects**\n\nElastic Net regression can be evaluated using algorithms similar to those for Lasso regression. The elastic net loss function can be reformulated as a Lasso loss function through a \"data augmentation trick\" [2].\nThe Ridge penalty part, $\\frac{1}{2}\\lambda_2 \\|\\beta\\|_2^2$, can be absorbed into the sum of squares by augmenting the response vector $Y$ and the design matrix $X$.\nLet $\\tilde{Y} = (Y^\\top, 0_p^\\top)^\\top$ and $\\tilde{X} = (X^\\top, \\sqrt{\\lambda_2} I_{pp})^\\top$. Then the term $\\|Y - X\\beta\\|_2^2 + \\frac{1}{2}\\lambda_2 \\|\\beta\\|_2^2$ can be rewritten as $\\|\\tilde{Y} - \\tilde{X}\\beta\\|_2^2$.\nThus, the elastic net loss function becomes $\\|\\tilde{Y} - \\tilde{X}\\beta\\|_2^2 + \\lambda_1 \\|\\beta\\|_1$, which is in the form of a Lasso loss function, allowing existing Lasso algorithms to be applied [2]. Zou and Hastie (2005) also presented a specific algorithm for elastic net evaluation that is faster and numerically more stable [1].",
    "chunks": [
      {
        "text": "and Hastie (2005) present a different algorithm for the evaluation of the elastic net estimator that is faster and numerically more stable (see also Exercise 7.5). Irrespectively, the reformulation of the elastic net loss in terms of augmented data also reveals that the elastic net regression estimator can select p variables. Even for p > n, which is immediate from the observation that rank( ˜X) = p. The penalty parameters need tuning, e.g. by cross-validation. They are subject to empirically indeterminancy. That is, often a large range of (λ1, λ2)-combinations will yield a similar cross-validated performance as can be witnessed from Figure 7.1. It shows the contourplot of the penalty parameters vs. this performance. There is a yellow ‘banana-shaped’ area that corresponds to the same and optimal performance. Hence, no single (λ1, λ2)- combination can be distinguished as all yield the best performance. This behaviour may be understood intuitively. Reasoning loosely, while the lasso penalty λ1∥β∥1 ensures the selection property and the ridge penalty 1 2λ2∥β∥2 2 warrants the uniqueness and joint shrinkage of coefficients of collinear covariates, they have a similar effect on the size of the estimator. Both shrink, although in different norms. But a reduction in the size of ˆβ(λ1, λ2) in one norm implies a reduction in another. An increase in either the lasso and ridge penalty parameter will have a similar effect on the elastic net estimator: it shrinks. The selection and ‘joint shrinkage’ properties are only consequences of the employed penalty and are not criteria in the optimization of the elastic net loss function. There, only size matters. The size of β refers to λ1∥β∥1 + 1 2λ2∥β∥2 2. As in the size the lasso and ridge penalties appear as a linear combination in the elastic net loss function and have a similar effect on the elastic net estimator: there are many positive (λ1, λ2)-combinations that constrain the size of the elastic net estimator equally. In contrast, for both the lasso and ridge regression estimators, different penalty parameters yield estimators of different sizes (defined accordingly). Moreover, it is mainly the size that determines the cross-validated performance as the size deter- mines the shrinkage of the estimator and, consequently, the size of the errors. But only a fixed size leaves enough freedom to distribute this size over the p elements of the regression parameter estimator ˆβ(λ1, λ2) and, due to the collinearity, among them many that yield a comparable performance. Hence, if a particularly sized elastic net estimator ˆβ(λ1, λ2) optimizes the cross-validated performance, then high-dimensionally there are likely many others with a different (λ1, λ2)-combination but of equal size and similar performance. The empirical indeterminancy of penalty parameters touches upon another issues. In principle, the elastic net 7.2 Fused lasso 135 regression estimator can decide whether a sparse or non-sparse solution is most appropriate. The indeterminancy indicates that for any sparse elastic net regression estimator a less sparse one can be found with comparable performance, and vice versa. Care should be exercised when concluding on the sparsity of the linear relation under study from the chosen elastic net regression estimator. A solution to the indeterminancy of the optimal penalty parameter combination is to fix their ratio. For in- terpretation purposes this is done through the introduction of a ‘mixing parameter’ α ∈[0, 1]. The elastic net penalty is then written as λ[α∥β∥1 + 1 2(1 −α)∥β∥2 2]. The mixing parameter is set by the user while λ > 0 is typically found through cross-validation (cf. the implementation in the glmnet-package) (Friedman et al., 2009). Generally, no guidance on the choice of mixing parameter α can be given. In fact, it is a tuning parameter and as such needs tuning rather then setting out of the blue. 7.2 Fused lasso The fused lasso regression estimator proposed by Tibshirani et al. (2005) is the counterpart of the fused ridge regression estimator encountered in Example 3.1. It is a generalization of the lasso regression estimator for situations where the order of index j, j = 1, . . . , p, of the covariates has a certain meaning such as a spatial or temporal one. The fused lasso regression estimator minimizes the sum-of-squares augmented with the lasso penalty, the sum of the absolute values of the elements of the regression parameter, and the ℓ1-fusion (or simply fusion if clear from the context) penalty, the sum of the first order differences of the regression parameter. Formally, the fused lasso estimator is defined as: ˆβ(λ1, λ1,f) = arg min β∈Rp ∥Y −Xβ∥2 2 + λ1∥β∥1 + λ1,f Xp j=2 |βj −βj−1|, which involves two penalty parameters λ1 and λ1,f for the lasso and fusion penalties, respectively. As a result of adding the fusion penalty the fused lasso regression estimator not only shrinks elements of β towards zero but also",
        "metadata": {
          "chunk_index": 44,
          "paper_id": "1509.09169v8",
          "section": "algorithm.",
          "authors": "Wessel N. van Wieringen",
          "paper_title": "Lecture notes on ridge regression"
        },
        "id": "chunk_267"
      },
      {
        "text": "ii) the bound on the number of selected variables, i.e. maximally min{n, p} can be selected, and iii) the observation that strongly (positively) collinear covariates are not shrunken together: the lasso regression estimator selects among them while it is hard to distinguish their contributions to the variation of the response. While it does not select, the ridge regression estimator does not exhibit these less pleasing features. These consid- erations led Zou and Hastie (2005) to combine the strengths of the lasso and ridge regression estimators and form a ‘best-of-both-worlds’ estimator, called the elastic net regression estimator, defined as: ˆβ(λ1, λ2) = arg min β∈Rp ∥Y −Xβ∥2 2 + λ1∥β∥1 + 1 2λ2∥β∥2 2. The elastic net penalty – defined implicitly in the preceeding display – is thus simply a linear combination of the lasso and ridge penalties. Consequently, the elastic net regression estimator encompasses its lasso and ridge counterparts. Hereto just set λ2 = 0 or λ1 = 0, respectively. A novel estimator is defined if both penalties act simultaneously, i.e. if their corresponding penalty parameters are both nonzero. Does this novel elastic net estimator indeed inherit the strengths of the lasso and ridge regression estimators? Let us turn to the aforementioned motivation behind the elastic net estimator. Starting with the uniqueness, the strict convexity of the ridge penalty renders the elastic net loss function strictly convex, as it is a combination of the ridge penalty and the lasso function – notably non-strict convex when the dimension p exceeds the sample size n. This warrants the existence of a unique minimizer of the elastic net loss function. To assess the preservation of the selection property, now without the bound on the maximum number of selectable variables, exploit the equivalent constraint estimation formulation of the elastic net estimator. Figure 7.1 shows the parameter constraint of the elastic net estimator for the ‘p = 2’-case, which is defined by the set: {(β1, β2) ∈R2 : λ1(|β1| + |β2|) + 1 2λ2(β2 1 + β2 2) ≤c(λ1, λ2)}. Visually, the ‘elastic net parameter constraint’ is a compromise between the circle and the diamond shaped con- straints of the ridge and lasso regression estimators. This compromise inherits exactly the right geometrical fea- tures: the strict convexity of the ‘ridge circle’ and the ‘corners’ (referring to points at which the constraint’s boundary is non-smootness/non-differentiability) falling at the axes of the ‘lasso constraint’. The latter feature, by the same argumentation as presented in Section 6.3, endows the elastic net estimator with the selection property. Moreover, it can – in principle – select p features as the point in the parameter space where the smallest level set of the unpenalized loss hits the elastic net parameter constraint need not fall on any axis. For example, in the ‘p = 2, n = 1’-case the level sets of the sum-of-squares loss are straight lines that, if running almost parallel to the edges of the ‘lasso diamond’, are unlikely to first hit the elastic net parameter constraint at one of its corners. Finally, the largest penalty parameter relates (reciprocally) to the volume of the elastic net parameter constraint, while the ratio between λ1 and λ2 determines whether it is closer to the ‘ridge circle’ or to the ‘lasso diamond’. Whether the elastic net regression estimator also delivers on the joint shrinkage property is assessed by simu- lation (not shown). The impression given by these simulations is that the elastic net has joint shrinkage potential. This, however, usually requires a large ridge penalty, which then dominates the elastic net penalty. 134 Generalizing lasso regression β1 β2 −1.0 −0.5 0.0 0.5 1.0 −1.0 −0.5 0.0 0.5 1.0 lasso ridge elastic net Figure 7.1: The left panel depicts the parameter constraint induced by the elastic net penalty and, for reference, those of the lasso and ridge are added. The right panel shows the contour plot of the cross-validated loglikelihood vs. the two penalty parameters of the elastic net estimator. The elastic net regression estimator can be found with procedures similar to those that evaluate the lasso regression estimator (see Section 6.4) as the elastic net loss can be reformulated as a lasso loss. Hereto the ridge part of the elastic net penalty is absorbed into the sum of squares using the data augmentation trick of Exercise 1.6 which showed that the ridge regression estimator is the ML regression estimator of the related regression model with p zeros and rows added to the response and design matrix, respectively. That is, write ˜Y = (Y⊤, 0⊤ p )⊤and ˜X = (X⊤, √λ2 Ipp)⊤. Then: ∥˜Y −˜Xβ∥2 2 = ∥Y −Xβ∥2 2 + λ2∥β∥2 2. Hence, the elastic net loss function can be rewritten to ∥˜Y −˜Xβ∥2 2 + λ1∥β∥1. This is familiar territory and the lasso algorithms of Section 6.4 can be used. Zou",
        "metadata": {
          "section": "algorithm.",
          "authors": "Wessel N. van Wieringen",
          "paper_id": "1509.09169v8",
          "paper_title": "Lecture notes on ridge regression",
          "chunk_index": 43
        },
        "id": "chunk_266"
      },
      {
        "text": "λ2)−1sign(X⊤ ∗,j ˜Y) \u0002 |X⊤ ∗,j ˜Y| −1 2λ1 \u0003 +. with ˜Y = Y −X∗,\\jβ\\j. Question 7.6 (The elastic net regression estimator) Consider the linear regression model Y = Xβ + ε with ε ∼N (0n, σ2 εInn). This model (without intercept) is fitted to data using the elastic net estimator ˆβ(λ1, λ2) = arg minβ ∥Y −Xβ∥2 2 + λ1∥β∥1 + 1 2λ2∥β∥2 2. The relevant summary statistics of the data are: X =   1 −1 −1  , Y =   −5 4 1  , X⊤X = \u0000 3 \u0001 , and X⊤Y = \u0000 −10 \u0001 . a) Evaluate for (λ1, λ2) = (3, 2) the elastic net regression estimator of the linear regression model. b) Now consider the evaluation the elastic net regression estimator of the linear regression model for the same penalty parameters, (λ1, λ2) = (3, 2), but this time involving two covariates. The first covariate is as in part a), the second is orthogonal to that one. Do you expect the resulting elastic net estimate of the first regression coefficient ˆβ1(λ1, λ2) to be larger, equal or smaller (in an absolute sense) than your answer to part a)? Motivate. c) Now take in part b) the second covariate equal to the first one. Show that the first coefficient of elastic net estimate ˆβ1(λ1, 2λ2) is half that of part a). Note: there is no need to know the exact answer to part a). Question 7.7 * Consider the linear regression model Y = Xβ + ε. It is fitted to data from a study with an orthonormal design matrix by means of the adaptive lasso regression estimator initiated by the OLS/ML regression estimator. Show that the j-th element of the resulting adaptive lasso regression estimator equals: ˆβ adapt j (λ1) = sign(ˆβ ols j )(|ˆβ ols j | −1 2λ1/|ˆβ ols j |)+. *This question is freely copied from B¨uhlmann and Van De Geer (2011): Problem 2.5a, page 43. Bibliography Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6), 716–723. Allen, D. M. (1974). The relationship between variable selection and data agumentation and a method for predic- tion. Technometrics, 16(1), 125–127. Ambs, S., Prueitt, R. L., Yi, M., Hudson, R. S., Howe, T. M., Petrocca, F., Wallace, T. A., Liu, C.-G., Volinia, S., Calin, G. A., Yfantis, H. G., Stephens, R. M., and Croce, C. M. (2008). Genomic profiling of microrna and messenger RNA reveals deregulated microrna expression in prostate cancer. Cancer Research, 68(15), 6162–6170. Anatolyev, S. (2020). A ridge to homogeneity for linear models. Journal of Statistical Computation and Simula- tion, 90(13), 2455–2472. Bartel, D. P. (2004). MicroRNAs: genomics, biogenesis, mechanism, and function. Cell, 116(2), 281–297. Bates, D. and DebRoy, S. (2004). Linear mixed models and penalized least squares. Journal of Multivariate Analysis, 91(1), 1–17. Berger, J. . (2013). Statistical Decision Theory and Bayesian Analysis. Springer Science & Business Media. Bertsekas, D. P. (2014). Constrained Optimization and Lagrange Multiplier Methods. Academic press. Bhattacharya, A., Chakraborty, A., and Mallick, B. K. (2016). Fast sampling with Gaussian scale mixture priors in high-dimensional regression. Biometrika, 103(4), 985–991. Bijma, F., Jonker, M. A., and van der Vaart, A. W. (2017). An Introduction to Mathematical Statistics. Amsterdam University Press. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer. Boyle, E. A., Li, Y. I., and Pritchard, J. K. (2017). An expanded view of complex traits: from polygenic to omnigenic. Cell, 169(7), 1177–1186. B¨uhlmann, P. (2013). Statistical significance in high-dimensional linear models. Bernoulli, 19(4), 1212–1242. B¨uhlmann, P. and Van De Geer, S. (2011). Statistics for High-Dimensional Data: Methods, Theory and Applica- tions. Springer Science & Business Media. Cancer Genome Atlas Network (2011). Integrated genomic analyses of ovarian carcinoma. Nature, 474(7353), 609–615. Cancer Genome Atlas Network (2012). Comprehensive molecular characterization of human colon and rectal cancer. Nature, 487(7407), 330–337. Castillo, I., Schmidt-Hieber, J., and Van der Vaart, A. W. (2015). Bayesian linear regression with sparse priors. The Annals of Statistics, 43(5), 1986–2018. Chaturvedi, N., de Menezes, R. X., and Goeman, J. J. (2014). Fused lasso algorithm for Cox’ proportional hazards and binomial logit models with application to copy number profiles. Biometrical Journal, 56(3), 477–492. Chib, S. and Greenberg, E. (1995). Understanding the metropolis-hastings algorithm. The American Statistician, 49(4), 327–335. de Vlaming, R. and Groenen, P. J. F. (2015). The current and future use of ridge regression for prediction in quantitative genetics. BioMed Research International, page Article ID 143712. Draper, N. R. and Smith, H. (1998). Applied Regression Analysis (3rd edition). John Wiley & Sons. Efron, B. (1986). How biased is the apparent error rate of a prediction rule? Journal of the American statistical Association, 81(394), 461–470. Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004). Least angle regression. The Annals of Statistics, 32(2), 407–499.",
        "metadata": {
          "section": "conclusion",
          "authors": "Wessel N. van Wieringen",
          "paper_id": "1509.09169v8",
          "paper_title": "Lecture notes on ridge regression",
          "chunk_index": 2
        },
        "id": "chunk_225"
      },
      {
        "text": "large parameter estimates. As a consequence it decreases the probability of overfitting. Overfitting is illustrated in the next example. Example 1.7 (Overfitting) Consider an artificial data set comprising of ten observations on a response Yi and nine covariates Xi,j. All covariate data are sampled from the standard normal distribution: Xi,j ∼N (0, 1). The response is generated by Yi = Xi,1 + εi with εi ∼N (0, 1 4). Hence, only the first covariate contributes to the response. 1.6 Degrees of freedom 21 The regression model Yi = P9 j=1 Xi,jβj + εi is fitted to the artificial data using R. This yields the regression parameter estimates: ˆβ⊤ = (0.048, −2.386, −5.528, 6.243, −4.819, 0.760, −3.345, −4.748, 2.136). As β⊤= (1, 0, . . . , 0), many regression coefficient are clearly over-estimated. The fitted values bYi = Xi ˆβ are plotted against the values of the first covariates in the right bottom panel of Figure 1.4. As a reference the line x = y is added, which represents the ‘true’ model. The fitted model follows the ‘true’ relationship. But it also captures the deviations from this line that represent the errors. □ 1.6 Degrees of freedom The degrees of freedom consumed by the ridge regression estimator, which may aid in the choice of the value of the penalty parameter, is derived. A formal definition of the degrees of freedom, which can among others be found in Efron (1986), is df(λ) = Xn i=1[Var(Yi)]−1Cov(bYi, Yi). (1.11) It represents the effective number of parameters used by the estimator. It can also be viewed as the amount of self- explanation by the observations of the fit. Recall from ordinary regression that bY = X(X⊤X)−1X⊤Y = HY where H is the hat matrix. Application of the defintion (1.11) then yields the degrees of freedom used by the maximum likelihood regression estimator and equals tr(H), the trace of H. In particular, if X is of full rank, i.e. rank(X) = p, then tr(H) = p. We adopt the degrees of freedom definition (1.11) for the ridge regression estimator. We then find df(λ) = Xn i=1[Var(Yi)]−1Cov(bYi, Yi) = σ−2 Xn i=1 Cov[Xi,∗ˆβ(λ), Yi] = σ−2 Xn i=1 Cov[Xi,∗(X⊤X + λIpp)−1XY, Yi] = σ−2 Xn i=1 Xi,∗(X⊤X + λIpp)−1XCov(Y, Yi) = tr[X(X⊤X⊤+ λIpp)−1X⊤] = Xp j=1(D⊤ x Dx)jj[(D⊤ x Dx)jj + λ]−1, where we have used the independence among the observations. High-dimensionally, the sum on the right-hand side of the last line of the display above may be limited to n. The degrees of freedom consumed by the ridge regression estimator is monotone decreasing in λ. In particular, limλ→∞df(λ) = 0. That is, in the limit no information from X is used. Indeed, ˆβ(λ) is forced to equal 0p which is not derived from data. Finally, from the derivation in the display above we may deduce a definition of the ‘ridge hat matrix’: H(λ) := X(X⊤X + λIpp)−1X⊤. We can then write, in analogy to the ordinary regression case, df(λ) = tr[H(λ)]. 1.7 Computationally efficient evaluation In the high-dimensional setting the number of covariates p is large compared to the number of samples n. In a microarray experiment p = 40000 and n = 100 is not uncommon. To perform ridge regression in this context, the following expression needs to be evaluated numerically: (X⊤X + λIpp)−1X⊤Y. For p = 40000 this requires the inversion of a 40000 × 40000 dimensional matrix. This is not feasible on most desktop computers. There, however, is a workaround to the computational burden. Revisit the singular value decomposition of X = UxDxV⊤ x . Drop from Dx and Vx the columns that correspond to zero singular values. The resulting Dx and Vx are n × n and p × n-dimensional, respectively. Note that dropping these columns has no effect on the matrix factorization of X, i.e. still X = UxDxV⊤ x but with the last two matrices in this decomposition defined differently from the traditional singular value decomposition. Now write Rx = UxDx. As both Ux and Dx are n×n-dimensional matrices, so is Rx. Consequently, X is now decomposed as X = RxV⊤ x . The ridge regression 22 Ridge regression estimator can be rewritten in terms of Rx and Vx: ˆβ(λ) = (X⊤X + λIpp)−1X⊤Y = (VxR⊤ x RxV⊤ x + λIpp)−1VxR⊤ x Y = (VxR⊤ x RxV⊤ x + λVxV⊤ x )−1VxR⊤ x Y = Vx(R⊤ x Rx + λInn)−1V⊤ x VxR⊤ x Y = Vx(R⊤ x Rx + λInn)−1R⊤ x Y. Hence, the reformulated ridge regressin estimator involves the inversion of an n × n-dimensional matrix. With n = 100 this is feasible on most standard computers. Hastie and Tibshirani (2004) point out that, with the SVD-trick above, the number of computation operations reduces from O(p3) to O(pn2). In addition, they point out that this computational short-cut can",
        "metadata": {
          "authors": "Wessel N. van Wieringen",
          "paper_id": "1509.09169v8",
          "section": "model",
          "paper_title": "Lecture notes on ridge regression",
          "chunk_index": 4
        },
        "id": "chunk_227"
      },
      {
        "text": "a result of multicollinearity. For example, multicollinearity can likely occur when the data consist of a large number of covariates (p), or can occur when the data set is constructed by merging multiple data sets that have one or more variables in common. Ridge regression, which deﬁnes the alternative estimator βλ = (X⊺X + λIp)−1X⊺y, pro- vides a solution to the inability of the OLS estimator bβ to handle ill-conditioned or singular X⊺X. Here, λ > 0 is the ridge parameter that shrinks the estimate of the coeﬃcients β towards zero, with the amount of shrinking an increasing function of λ. Compared to the OLS estimator bβ, the ridge estimator βλ introduces some bias in exchange for lower mean-squared MSE(bβλ | β) and prediction error PE(Xbβλ | β), especially when X⊺X is ill- conditioned or singular (Hoerl & Kennard, 1970). However, unlike the OLS estimator, the ridge estimator βλ exists even when X⊺X is singular, because (X⊺X + λIp) is necessarily non-singular when λ > 0. The ridge estimator βλ can be characterized in at least four equivalent ways (e.g., Hastie, et al. 2009). First, the ridge estimator is equivalent to the penalized least-squares estimator, βλ = arg minβ ||y −Xβ||2 + λ Pp k=1 β2 k. Second, in terms of the orthogonalized space, the ridge estimator is given by βλ = Wbαλ, with bαλ,k = {d2 k/(d2 k + λ)}bαk for k = 1, . . . , q. This shows that the ridge estimator βλ is obtained by shrinking the canonical OLS estimator bα = (bα1, . . . , bαq)⊺by the factors {d2 k/(d2 k+λ)} (resp.), and therefore applies a greater amount of shrinkage to OLS estimates bαk having relatively small eigenvalues. Third, from a Bayesian point of view, the ridge estimator βλ is the mean of the posterior distribution of β under a p-variate normal prior distribution, corresponding to normal probability density function (p.d.f.) np(β | 0, σ2λ−1Ip) for the conditional random variable β | σ2. Thus, from a Bayesian decision-theoretic perspective, βλ is the choice of point estimate of β that minimizes the posterior expected squared-error loss. Fourth, when λ = 0, the ridge estimator βλ becomes the OLS estimator bβ. In ridge regression, the quality of coeﬃcient estimates βλ and predictions hinge on the choice of the ridge parameter, λ. As a result, several methods have been proposed to estimate this parameter, based on either automatic plug-in estimation, cross-validation, information criteria optimization, or Markov chain Monte Carlo (MCMC), which we now brieﬂy review. An enormous literature on these methods has developed over the past 45 years, however, the vast majority of them rely on the OLS estimates (bβ, bσ2) (Cule & DeIorio, 2012). In keeping with the general spirit of this paper, we focus our review on ridge methods that can handle data sets where X⊺X is either positive-deﬁnite, ill-conditioned, or singular (including when p ≥n), unless indicated otherwise. The standard, Hoerl-Kennard-Baldwin (HKB) (1975) plug-in estimator for the ridge regression model is deﬁned by bλHKB = pbσ2/bβ ⊺bβ. This estimator is motivated by the fact that λHKB = pσ2/β⊺β is the choice of λ that minimizes the model’s expected prediction error when X is orthonormal (Hoerl & Kennard, 1970). The HKB estimator relies on the OLS estimate bσ2, and therefore does not exist when X⊺X is singular, including when p ≥n. 3 However, this estimator was extended to singular and p ≥n settings (Cule & DeIorio, 2013), by replacing bσ2 with the error variance estimate bσ2 r = 1 n−br||y −Xbαbr||2 obtained from a principal components regression having br ≤q = min(n, p) components with coeﬃcients α = (α1, . . . , αr)⊺. Here, br is the value of r that minimizes r−Pq k=1 d4 k/(d2 k +λr)2. Therefore br best matches the degrees of freedom for variance in the ridge regression model with ridge parameter λbr = brbσ2 br/bα⊺ br bαbr. The HKB plug-in estimator is attractive because it quickly obtains a ridge estimate bλ without iteration. However this estimator may lead to an estimate of λ that is not necessarily optimal for the given data set at hand, especially when X is not orthonormal. The method of cross-validation, for ridge regression, involves ﬁrst specifying a grid of trial values of λ, and then selecting the estimate bλ as the trial value that provides the smallest prediction error for the ridge regression model in K-fold cross-validation, usually 10-fold or n-fold (e.g., Hastie et al. 2009, Ch. 3, 7). The method takes K (at least nearly) equal-sized partitions of the n observations at random, and then for each trial value of λ, measures the model’s overall mean-squared predictive error over all K partitions. Overall mean-squared error is based on obtaining the estimate β (k) λ for one partition and measuring the predictive error",
        "metadata": {
          "paper_title": "Fast Marginal Likelihood Estimation of the Ridge Parameter(s) in Ridge Regression and Generalized Ridge Regression for Big Data",
          "chunk_index": 1,
          "authors": "George Karabatsos",
          "paper_id": "1409.2437v5",
          "section": "introduction"
        },
        "id": "chunk_285"
      }
    ]
  }
}