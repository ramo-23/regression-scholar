[
  {
    "arxiv_id": "2012.10249v5",
    "title": "Reduced-Rank Tensor-on-Tensor Regression and Tensor-variate Analysis of Variance",
    "authors": [
      "Carlos Llosa-Vite",
      "Ranjan Maitra"
    ],
    "abstract": "Fitting regression models with many multivariate responses and covariates can be challenging, but such responses and covariates sometimes have tensor-variate structure. We extend the classical multivariate regression model to exploit such structure in two ways: first, we impose four types of low-rank tensor formats on the regression coefficients. Second, we model the errors using the tensor-variate normal distribution that imposes a Kronecker separable format on the covariance matrix. We obtain maximum likelihood estimators via block-relaxation algorithms and derive their computational complexity and asymptotic distributions. Our regression framework enables us to formulate tensor-variate analysis of variance (TANOVA) methodology. This methodology, when applied in a one-way TANOVA layout, enables us to identify cerebral regions significantly associated with the interaction of suicide attempters or non-attemptor ideators and positive-, negative- or death-connoting words in a functional Magnetic Resonance Imaging study. Another application uses three-way TANOVA on the Labeled Faces in the Wild image dataset to distinguish facial characteristics related to ethnic origin, age group and gender.",
    "published": "2020-12-18",
    "categories": [
      "stat.ME",
      "math.ST",
      "physics.data-an",
      "stat.CO",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\2012.10249v5.pdf"
  },
  {
    "arxiv_id": "1608.00621v3",
    "title": "Efficient Multiple Incremental Computation for Kernel Ridge Regression with Bayesian Uncertainty Modeling",
    "authors": [
      "Bo-Wei Chen",
      "Nik Nailah Binti Abdullah",
      "Sangoh Park"
    ],
    "abstract": "This study presents an efficient incremental/decremental approach for big streams based on Kernel Ridge Regression (KRR), a frequently used data analysis in cloud centers. To avoid reanalyzing the whole dataset whenever sensors receive new training data, typical incremental KRR used a single-instance mechanism for updating an existing system. However, this inevitably increased redundant computational time, not to mention applicability to big streams. To this end, the proposed mechanism supports incremental/decremental processing for both single and multiple samples (i.e., batch processing). A large scale of data can be divided into batches, processed by a machine, without sacrificing the accuracy. Moreover, incremental/decremental analyses in empirical and intrinsic space are also proposed in this study to handle different types of data either with a large number of samples or high feature dimensions, whereas typical methods focused only on one type. At the end of this study, we further the proposed mechanism to statistical Kernelized Bayesian Regression, so that uncertainty modeling with incremental/decremental computation becomes applicable. Experimental results showed that computational time was significantly reduced, better than the original nonincremental design and the typical single incremental method. Furthermore, the accuracy of the proposed method remained the same as the baselines. This implied that the system enhanced efficiency without sacrificing the accuracy. These findings proved that the proposed method was appropriate for variable streaming data analysis, thereby demonstrating the effectiveness of the proposed method.",
    "published": "2016-08-01",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\1608.00621v3.pdf"
  },
  {
    "arxiv_id": "2210.06831v1",
    "title": "Multi-Target XGBoostLSS Regression",
    "authors": [
      "Alexander M\u00e4rz"
    ],
    "abstract": "Current implementations of Gradient Boosting Machines are mostly designed for single-target regression tasks and commonly assume independence between responses when used in multivariate settings. As such, these models are not well suited if non-negligible dependencies exist between targets. To overcome this limitation, we present an extension of XGBoostLSS that models multiple targets and their dependencies in a probabilistic regression setting. Empirical results show that our approach outperforms existing GBMs with respect to runtime and compares well in terms of accuracy.",
    "published": "2022-10-13",
    "categories": [
      "cs.LG"
    ],
    "pdf_path": "data/papers\\2210.06831v1.pdf"
  },
  {
    "arxiv_id": "1304.5637v1",
    "title": "Tucker Tensor Regression and Neuroimaging Analysis",
    "authors": [
      "Xiaoshan Li",
      "Hua Zhou",
      "Lexin Li"
    ],
    "abstract": "Large-scale neuroimaging studies have been collecting brain images of study individuals, which take the form of two-dimensional, three-dimensional, or higher dimensional arrays, also known as tensors. Addressing scientific questions arising from such data demands new regression models that take multidimensional arrays as covariates. Simply turning an image array into a long vector causes extremely high dimensionality that compromises classical regression methods, and, more seriously, destroys the inherent spatial structure of array data that possesses wealth of information. In this article, we propose a family of generalized linear tensor regression models based upon the Tucker decomposition of regression coefficient arrays. Effectively exploiting the low rank structure of tensor covariates brings the ultrahigh dimensionality to a manageable level that leads to efficient estimation. We demonstrate, both numerically that the new model could provide a sound recovery of even high rank signals, and asymptotically that the model is consistently estimating the best Tucker structure approximation to the full array model in the sense of Kullback-Liebler distance. The new model is also compared to a recently proposed tensor regression model that relies upon an alternative CANDECOMP/PARAFAC (CP) decomposition.",
    "published": "2013-04-20",
    "categories": [
      "stat.ME"
    ],
    "pdf_path": "data/papers\\1304.5637v1.pdf"
  },
  {
    "arxiv_id": "2507.20357v1",
    "title": "Wafer Defect Root Cause Analysis with Partial Trajectory Regression",
    "authors": [
      "Kohei Miyaguchi",
      "Masao Joko",
      "Rebekah Sheraw",
      "Tsuyoshi Id\u00e9"
    ],
    "abstract": "Identifying upstream processes responsible for wafer defects is challenging due to the combinatorial nature of process flows and the inherent variability in processing routes, which arises from factors such as rework operations and random process waiting times. This paper presents a novel framework for wafer defect root cause analysis, called Partial Trajectory Regression (PTR). The proposed framework is carefully designed to address the limitations of conventional vector-based regression models, particularly in handling variable-length processing routes that span a large number of heterogeneous physical processes. To compute the attribution score of each process given a detected high defect density on a specific wafer, we propose a new algorithm that compares two counterfactual outcomes derived from partial process trajectories. This is enabled by new representation learning methods, proc2vec and route2vec. We demonstrate the effectiveness of the proposed framework using real wafer history data from the NY CREATES fab in Albany.",
    "published": "2025-07-27",
    "categories": [
      "cs.LG"
    ],
    "pdf_path": "data/papers\\2507.20357v1.pdf"
  },
  {
    "arxiv_id": "1612.06850v2",
    "title": "Extremal Quantile Regression: An Overview",
    "authors": [
      "Victor Chernozhukov",
      "Iv\u00e1n Fern\u00e1ndez-Val",
      "Tetsuya Kaji"
    ],
    "abstract": "Extremal quantile regression, i.e. quantile regression applied to the tails of the conditional distribution, counts with an increasing number of economic and financial applications such as value-at-risk, production frontiers, determinants of low infant birth weights, and auction models. This chapter provides an overview of recent developments in the theory and empirics of extremal quantile regression. The advances in the theory have relied on the use of extreme value approximations to the law of the Koenker and Bassett (1978) quantile regression estimator. Extreme value laws not only have been shown to provide more accurate approximations than Gaussian laws at the tails, but also have served as the basis to develop bias corrected estimators and inference methods using simulation and suitable variations of bootstrap and subsampling. The applicability of these methods is illustrated with two empirical examples on conditional value-at-risk and financial contagion.",
    "published": "2016-12-20",
    "categories": [
      "stat.ME",
      "econ.EM"
    ],
    "pdf_path": "data/papers\\1612.06850v2.pdf"
  },
  {
    "arxiv_id": "2510.07895v1",
    "title": "Adaptive Optimizable Gaussian Process Regression Linear Least Squares Regression Filtering Method for SEM Images",
    "authors": [
      "D. Chee Yong Ong",
      "I. Bukhori",
      "K. S. Sim",
      "K. Beng Gan"
    ],
    "abstract": "Scanning Electron Microscopy (SEM) images often suffer from noise contamination, which degrades image quality and affects further analysis. This research presents a complete approach to estimate their Signal-to-Noise Ratio (SNR) and noise variance (NV), and enhance image quality using NV-guided Wiener filter. The main idea of this study is to use a good SNR estimation technique and infuse a machine learning model to estimate NV of the SEM image, which then guides the wiener filter to remove the noise, providing a more robust and accurate SEM image filtering pipeline. First, we investigate five different SNR estimation techniques, namely Nearest Neighbourhood (NN) method, First-Order Linear Interpolation (FOL) method, Nearest Neighbourhood with First-Order Linear Interpolation (NN+FOL) method, Non-Linear Least Squares Regression (NLLSR) method, and Linear Least Squares Regression (LSR) method. It is shown that LSR method to perform better than the rest. Then, Support Vector Machines (SVM) and Gaussian Process Regression (GPR) are tested by pairing it with LSR. In this test, the Optimizable GPR model shows the highest accuracy and it stands as the most effective solution for NV estimation. Combining these results lead to the proposed Adaptive Optimizable Gaussian Process Regression Linear Least Squares Regression (AO-GPRLLSR) Filtering pipeline. The AO-GPRLLSR method generated an estimated noise variance which served as input to NV-guided Wiener filter for improving the quality of SEM images. The proposed method is shown to achieve notable success in estimating SNR and NV of SEM images and leads to lower Mean Squared Error (MSE) after the filtering process.",
    "published": "2025-10-09",
    "categories": [
      "cs.LG"
    ],
    "pdf_path": "data/papers\\2510.07895v1.pdf"
  },
  {
    "arxiv_id": "2310.15204v1",
    "title": "Mid-Long Term Daily Electricity Consumption Forecasting Based on Piecewise Linear Regression and Dilated Causal CNN",
    "authors": [
      "Zhou Lan",
      "Ben Liu",
      "Yi Feng",
      "Danhuang Dong",
      "Peng Zhang"
    ],
    "abstract": "Daily electricity consumption forecasting is a classical problem. Existing forecasting algorithms tend to have decreased accuracy on special dates like holidays. This study decomposes the daily electricity consumption series into three components: trend, seasonal, and residual, and constructs a two-stage prediction method using piecewise linear regression as a filter and Dilated Causal CNN as a predictor. The specific steps involve setting breakpoints on the time axis and fitting the piecewise linear regression model with one-hot encoded information such as month, weekday, and holidays. For the challenging prediction of the Spring Festival, distance is introduced as a variable using a third-degree polynomial form in the model. The residual sequence obtained in the previous step is modeled using Dilated Causal CNN, and the final prediction of daily electricity consumption is the sum of the two-stage predictions. Experimental results demonstrate that this method achieves higher accuracy compared to existing approaches.",
    "published": "2023-10-23",
    "categories": [
      "cs.LG"
    ],
    "pdf_path": "data/papers\\2310.15204v1.pdf"
  },
  {
    "arxiv_id": "2503.15039v1",
    "title": "A Note on Local Linear Regression for Time Series in Banach Spaces",
    "authors": [
      "Florian Heinrichs"
    ],
    "abstract": "This work extends local linear regression to Banach space-valued time series for estimating smoothly varying means and their derivatives in non-stationary data. The asymptotic properties of both the standard and bias-reduced Jackknife estimators are analyzed under mild moment conditions, establishing their convergence rates. Simulation studies assess the finite sample performance of these estimators and compare them with the Nadaraya-Watson estimator. Additionally, the proposed methods are applied to smooth EEG recordings for reconstructing eye movements and to video analysis for detecting pedestrians and abandoned objects.",
    "published": "2025-03-19",
    "categories": [
      "math.ST",
      "stat.ME"
    ],
    "pdf_path": "data/papers\\2503.15039v1.pdf"
  },
  {
    "arxiv_id": "2310.10807v1",
    "title": "Regularization properties of adversarially-trained linear regression",
    "authors": [
      "Ant\u00f4nio H. Ribeiro",
      "Dave Zachariah",
      "Francis Bach",
      "Thomas B. Sch\u00f6n"
    ],
    "abstract": "State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the minimum-norm interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\\ell_\\infty$-adversarial training -- as in square-root Lasso -- the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.",
    "published": "2023-10-16",
    "categories": [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "math.OC"
    ],
    "pdf_path": "data/papers\\2310.10807v1.pdf"
  },
  {
    "arxiv_id": "1601.06722v2",
    "title": "Optimal designs for comparing regression models with correlated observations",
    "authors": [
      "Holger Dette",
      "Kirsten Schorning",
      "Maria Konstantinou"
    ],
    "abstract": "We consider the problem of efficient statistical inference for comparing two regression curves estimated from two samples of dependent measurements. Based on a representation of the best pair of linear unbiased estimators in continuous time models as a stochastic integral, an efficient pair of linear unbiased estimators with corresponding optimal designs for finite sample size is constructed. This pair minimises the width of the confidence band for the difference between the estimated curves. We thus extend results readily available in the literature to the case of correlated observations and provide an easily implementable and efficient solution. The advantages of using such pairs of estimators with corresponding optimal designs for the comparison of regression models are illustrated via numerical examples.",
    "published": "2016-01-25",
    "categories": [
      "stat.ME"
    ],
    "pdf_path": "data/papers\\1601.06722v2.pdf"
  },
  {
    "arxiv_id": "1803.04839v3",
    "title": "Optimal estimators in misspecified linear regression model with an application to real-world data",
    "authors": [
      "Manickavasagar Kayanan",
      "Pushpakanthie Wijekoon"
    ],
    "abstract": "In this article, we propose the Sample Information Optimal Estimator (SIOE) and the Stochastic Restricted Optimal Estimator (SROE) for misspecified linear regression model when multicollinearity exists among explanatory variables. Further, we obtain the superiority conditions of proposed estimators over some other existing estimators in the Mean Square Error Matrix (MSEM) criterion in a standard form which can apply to all estimators considered in this study. Finally, a real world example and a Monte Carlo simulation study are presented for the proposed estimators to illustrate the theoretical results.",
    "published": "2018-03-11",
    "categories": [
      "math.ST",
      "stat.ME"
    ],
    "pdf_path": "data/papers\\1803.04839v3.pdf"
  },
  {
    "arxiv_id": "1507.03003v2",
    "title": "High-Dimensional Asymptotics of Prediction: Ridge Regression and Classification",
    "authors": [
      "Edgar Dobriban",
      "Stefan Wager"
    ],
    "abstract": "We provide a unified analysis of the predictive risk of ridge regression and regularized discriminant analysis in a dense random effects model. We work in a high-dimensional asymptotic regime where $p, n \\to \\infty$ and $p/n \\to \u03b3\\in (0, \\, \\infty)$, and allow for arbitrary covariance among the features. For both methods, we provide an explicit and efficiently computable expression for the limiting predictive risk, which depends only on the spectrum of the feature-covariance matrix, the signal strength, and the aspect ratio $\u03b3$. Especially in the case of regularized discriminant analysis, we find that predictive accuracy has a nuanced dependence on the eigenvalue distribution of the covariance matrix, suggesting that analyses based on the operator norm of the covariance matrix may not be sharp. Our results also uncover several qualitative insights about both methods: for example, with ridge regression, there is an exact inverse relation between the limiting predictive risk and the limiting estimation risk given a fixed signal strength. Our analysis builds on recent advances in random matrix theory.",
    "published": "2015-07-10",
    "categories": [
      "math.ST",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\1507.03003v2.pdf"
  },
  {
    "arxiv_id": "1010.3320v2",
    "title": "Exact block-wise optimization in group lasso and sparse group lasso for linear regression",
    "authors": [
      "Rina Foygel",
      "Mathias Drton"
    ],
    "abstract": "The group lasso is a penalized regression method, used in regression problems where the covariates are partitioned into groups to promote sparsity at the group level. Existing methods for finding the group lasso estimator either use gradient projection methods to update the entire coefficient vector simultaneously at each step, or update one group of coefficients at a time using an inexact line search to approximate the optimal value for the group of coefficients when all other groups' coefficients are fixed. We present a new method of computation for the group lasso in the linear regression case, the Single Line Search (SLS) algorithm, which operates by computing the exact optimal value for each group (when all other coefficients are fixed) with one univariate line search. We perform simulations demonstrating that the SLS algorithm is often more efficient than existing computational methods. We also extend the SLS algorithm to the sparse group lasso problem via the Signed Single Line Search (SSLS) algorithm, and give theoretical results to support both algorithms.",
    "published": "2010-10-16",
    "categories": [
      "stat.ML"
    ],
    "pdf_path": "data/papers\\1010.3320v2.pdf"
  },
  {
    "arxiv_id": "1108.4559v2",
    "title": "Optimal Algorithms for Ridge and Lasso Regression with Partially Observed Attributes",
    "authors": [
      "Elad Hazan",
      "Tomer Koren"
    ],
    "abstract": "We consider the most common variants of linear regression, including Ridge, Lasso and Support-vector regression, in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time. We present simple and efficient algorithms for these problems: for Lasso and Ridge regression they need the same total number of attributes (up to constants) as do full-information algorithms, for reaching a certain accuracy. For Support-vector regression, we require exponentially less attributes compared to the state of the art. By that, we resolve an open problem recently posed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art.",
    "published": "2011-08-23",
    "categories": [
      "cs.LG"
    ],
    "pdf_path": "data/papers\\1108.4559v2.pdf"
  },
  {
    "arxiv_id": "2305.18496v2",
    "title": "Generalized equivalences between subsampling and ridge regularization",
    "authors": [
      "Pratik Patil",
      "Jin-Hong Du"
    ],
    "abstract": "We establish precise structural and risk equivalences between subsampling and ridge regularization for ensemble ridge estimators. Specifically, we prove that linear and quadratic functionals of subsample ridge estimators, when fitted with different ridge regularization levels $\u03bb$ and subsample aspect ratios $\u03c8$, are asymptotically equivalent along specific paths in the $(\u03bb,\u03c8)$-plane (where $\u03c8$ is the ratio of the feature dimension to the subsample size). Our results only require bounded moment assumptions on feature and response distributions and allow for arbitrary joint distributions. Furthermore, we provide a data-dependent method to determine the equivalent paths of $(\u03bb,\u03c8)$. An indirect implication of our equivalences is that optimally tuned ridge regression exhibits a monotonic prediction risk in the data aspect ratio. This resolves a recent open problem raised by Nakkiran et al. for general data distributions under proportional asymptotics, assuming a mild regularity condition that maintains regression hardness through linearized signal-to-noise ratios.",
    "published": "2023-05-29",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\2305.18496v2.pdf"
  },
  {
    "arxiv_id": "1311.2791v1",
    "title": "When Does More Regularization Imply Fewer Degrees of Freedom? Sufficient Conditions and Counter Examples from Lasso and Ridge Regression",
    "authors": [
      "Shachar Kaufman",
      "Saharon Rosset"
    ],
    "abstract": "Regularization aims to improve prediction performance of a given statistical modeling approach by moving to a second approach which achieves worse training error but is expected to have fewer degrees of freedom, i.e., better agreement between training and prediction error. We show here, however, that this expected behavior does not hold in general. In fact, counter examples are given that show regularization can increase the degrees of freedom in simple situations, including lasso and ridge regression, which are the most common regularization approaches in use. In such situations, the regularization increases both training error and degrees of freedom, and is thus inherently without merit. On the other hand, two important regularization scenarios are described where the expected reduction in degrees of freedom is indeed guaranteed: (a) all symmetric linear smoothers, and (b) linear regression versus convex constrained linear regression (as in the constrained variant of ridge regression and lasso).",
    "published": "2013-11-12",
    "categories": [
      "math.ST",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\1311.2791v1.pdf"
  },
  {
    "arxiv_id": "1509.09169v8",
    "title": "Lecture notes on ridge regression",
    "authors": [
      "Wessel N. van Wieringen"
    ],
    "abstract": "The linear regression model cannot be fitted to high-dimensional data, as the high-dimensionality brings about empirical non-identifiability. Penalized regression overcomes this non-identifiability by augmentation of the loss function by a penalty (i.e. a function of regression coefficients). The ridge penalty is the sum of squared regression coefficients, giving rise to ridge regression. Here many aspect of ridge regression are reviewed e.g. moments, mean squared error, its equivalence to constrained estimation, and its relation to Bayesian regression. Finally, its behaviour and use are illustrated in simulation and on omics data. Subsequently, ridge regression is generalized to allow for a more general penalty. The ridge penalization framework is then translated to logistic regression and its properties are shown to carry over. To contrast ridge penalized estimation, the final chapters introduce its lasso counterpart and generalizations thereof.",
    "published": "2015-09-30",
    "categories": [
      "stat.ME"
    ],
    "pdf_path": "data/papers\\1509.09169v8.pdf"
  },
  {
    "arxiv_id": "1910.01805v2",
    "title": "On the Duality between Network Flows and Network Lasso",
    "authors": [
      "Alexander Jung"
    ],
    "abstract": "Many applications generate data with an intrinsic network structure such as time series data, image data or social network data. The network Lasso (nLasso) has been proposed recently as a method for joint clustering and optimization of machine learning models for networked data. The nLasso extends the Lasso from sparse linear models to clustered graph signals. This paper explores the duality of nLasso and network flow optimization. We show that, in a very precise sense, nLasso is equivalent to a minimum-cost flow problem on the data network structure. Our main technical result is a concise characterization of nLasso solutions via existence of certain network flows. The main conceptual result is a useful link between nLasso methods and basic graph algorithms such as clustering or maximum flow.",
    "published": "2019-10-04",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\1910.01805v2.pdf"
  },
  {
    "arxiv_id": "2512.10632v1",
    "title": "Lasso-Ridge Refitting: A Two-Stage Estimator for High-Dimensional Linear Regression",
    "authors": [
      "Guo Liu"
    ],
    "abstract": "The least absolute shrinkage and selection operator (Lasso) is a popular method for high-dimensional statistics. However, it is known that the Lasso often has estimation bias and prediction error. To address such disadvantages, many alternatives and refitting strategies have been proposed and studied. This work introduces a novel Lasso--Ridge method. Our analysis indicates that the proposed estimator achieves improved prediction performance in a range of settings, including cases where the Lasso is tuned at its theoretical optimal rate \\(\\sqrt{\\log(p)/n}\\). Moreover, the proposed method retains several key advantages of the Lasso, such as prediction consistency and reliable variable selection under mild conditions. Through extensive simulations, we further demonstrate that our estimator outperforms the Lasso in both prediction and estimation accuracy, highlighting its potential as a powerful tool for high-dimensional linear regression.",
    "published": "2025-12-11",
    "categories": [
      "stat.ME"
    ],
    "pdf_path": "data/papers\\2512.10632v1.pdf"
  },
  {
    "arxiv_id": "1409.2437v5",
    "title": "Fast Marginal Likelihood Estimation of the Ridge Parameter(s) in Ridge Regression and Generalized Ridge Regression for Big Data",
    "authors": [
      "George Karabatsos"
    ],
    "abstract": "Unlike the ordinary least-squares (OLS) estimator for the linear model, a ridge regression linear model provides coefficient estimates via shrinkage, usually with improved mean-square and prediction error. This is true especially when the observed design matrix is ill-conditioned or singular, either as a result of highly-correlated covariates or the number of covariates exceeding the sample size. This paper introduces novel and fast marginal maximum likelihood (MML) algorithms for estimating the shrinkage parameter(s) for the Bayesian ridge and power ridge regression models, and an automatic plug-in MML estimator for the Bayesian generalized ridge regression model. With the aid of the singular value decomposition of the observed covariate design matrix, these MML estimation methods are quite fast even for data sets where either the sample size (n) or the number of covariates (p) is very large, and even when p>n. On several real data sets varying widely in terms of n and p, the computation times of the MML estimation methods for the three ridge models, respectively, are compared with the times of other methods for estimating the shrinkage parameter in ridge, LASSO and Elastic Net (EN) models, with the other methods based on minimizing prediction error according to cross-validation or information criteria. Also, the ridge, LASSO, and EN models, and their associated estimation methods, are compared in terms of prediction accuracy. Furthermore, a simulation study compares the ridge models under MML estimation, against the LASSO and EN models, in terms of their ability to differentiate between truly-significant covariates (i.e., with non-zero slope coefficients) and truly-insignificant covariates (with zero coefficients).",
    "published": "2014-09-08",
    "categories": [
      "stat.ME"
    ],
    "pdf_path": "data/papers\\1409.2437v5.pdf"
  },
  {
    "arxiv_id": "2003.14118v1",
    "title": "A flexible adaptive lasso Cox frailty model based on the full likelihood",
    "authors": [
      "Maike Hohberg",
      "Andreas Groll"
    ],
    "abstract": "In this work a method to regularize Cox frailty models is proposed that accommodates time-varying covariates and time-varying coefficients and is based on the full instead of the partial likelihood. A particular advantage in this framework is that the baseline hazard can be explicitly modeled in a smooth, semi-parametric way, e.g. via P-splines. Regularization for variable selection is performed via a lasso penalty and via group lasso for categorical variables while a second penalty regularizes wiggliness of smooth estimates of time-varying coefficients and the baseline hazard. Additionally, adaptive weights are included to stabilize the estimation. The method is implemented in R as coxlasso and will be compared to other packages for regularized Cox regression. Existing packages, however, do not allow for the combination of different effects that are accommodated in coxlasso.",
    "published": "2020-03-31",
    "categories": [
      "stat.ME",
      "stat.CO"
    ],
    "pdf_path": "data/papers\\2003.14118v1.pdf"
  },
  {
    "arxiv_id": "2408.13474v2",
    "title": "Ridge, lasso, and elastic-net estimations of the modified Poisson and least-squares regressions for binary outcome data",
    "authors": [
      "Takahiro Kitano",
      "Hisashi Noma"
    ],
    "abstract": "Logistic regression is a standard method in multivariate analysis for binary outcome data in epidemiological and clinical studies; however, the resultant odds-ratio estimates fail to provide directly interpretable effect measures. The modified Poisson and least-squares regressions are alternative standard methods that can provide risk-ratio and risk difference estimates without computational problems. However, the bias and invalid inference problems of these regression analyses under small or sparse data conditions (i.e.,the \"separation\" problem) have been insufficiently investigated. We show that the separation problem can adversely affect the inferences of the modified Poisson and least squares regressions, and to address these issues, we apply the ridge, lasso, and elastic-net estimating approaches to the two regression methods. As the methods are not founded on the maximum likelihood principle, we propose regularized quasi-likelihood approaches based on the estimating equations for these generalized linear models. The methods provide stable shrinkage estimates of risk ratios and risk differences even under separation conditions, and the lasso and elastic-net approaches enable simultaneous variable selection. We provide a bootstrap method to calculate the confidence intervals on the basis of the regularized quasi-likelihood estimation. The proposed methods are applied to a hematopoietic stem cell transplantation cohort study and the National Child Development Survey. We also provide an R package, regconfint, to implement these methods with simple commands.",
    "published": "2024-08-24",
    "categories": [
      "stat.ME",
      "stat.AP"
    ],
    "pdf_path": "data/papers\\2408.13474v2.pdf"
  },
  {
    "arxiv_id": "2009.08071v2",
    "title": "Ridge Regression Revisited: Debiasing, Thresholding and Bootstrap",
    "authors": [
      "Yunyi Zhang",
      "Dimitris N. Politis"
    ],
    "abstract": "The success of the Lasso in the era of high-dimensional data can be attributed to its conducting an implicit model selection, i.e., zeroing out regression coefficients that are not significant. By contrast, classical ridge regression can not reveal a potential sparsity of parameters, and may also introduce a large bias under the high-dimensional setting. Nevertheless, recent work on the Lasso involves debiasing and thresholding, the latter in order to further enhance the model selection. As a consequence, ridge regression may be worth another look since -- after debiasing and thresholding -- it may offer some advantages over the Lasso, e.g., it can be easily computed using a closed-form expression. % and it has similar performance to threshold Lasso. In this paper, we define a debiased and thresholded ridge regression method, and prove a consistency result and a Gaussian approximation theorem. We further introduce a wild bootstrap algorithm to construct confidence regions and perform hypothesis testing for a linear combination of parameters. In addition to estimation, we consider the problem of prediction, and present a novel, hybrid bootstrap algorithm tailored for prediction intervals. Extensive numerical simulations further show that the debiased and thresholded ridge regression has favorable finite sample performance and may be preferable in some settings.",
    "published": "2020-09-17",
    "categories": [
      "math.ST",
      "stat.AP",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\2009.08071v2.pdf"
  },
  {
    "arxiv_id": "1907.02242v2",
    "title": "Fair Kernel Regression via Fair Feature Embedding in Kernel Space",
    "authors": [
      "Austin Okray",
      "Hui Hu",
      "Chao Lan"
    ],
    "abstract": "In recent years, there have been significant efforts on mitigating unethical demographic biases in machine learning methods. However, very little is done for kernel methods. In this paper, we propose a new fair kernel regression method via fair feature embedding (FKR-F$^2$E) in kernel space. Motivated by prior works on feature selection in kernel space and feature processing for fair machine learning, we propose to learn fair feature embedding functions that minimize demographic discrepancy of feature distributions in kernel space. Compared to the state-of-the-art fair kernel regression method and several baseline methods, we show FKR-F$^2$E achieves significantly lower prediction disparity across three real-world data sets.",
    "published": "2019-07-04",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\1907.02242v2.pdf"
  },
  {
    "arxiv_id": "2310.13966v3",
    "title": "Transfer Learning for Kernel-based Regression",
    "authors": [
      "Chao Wang",
      "Caixing Wang",
      "Xin He",
      "Xingdong Feng"
    ],
    "abstract": "In recent years, transfer learning has garnered significant attention. Its ability to leverage knowledge from related studies to improve generalization performance in a target study has made it highly appealing. This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space. The aim is to bridge the gap between practical effectiveness and theoretical guarantees. We specifically consider two scenarios: one where the transferable sources are known and another where they are unknown. For the known transferable source case, we propose a two-step kernel-based estimator by solely using kernel ridge regression. For the unknown case, we develop a novel method based on an efficient aggregation algorithm, which can automatically detect and alleviate the effects of negative sources. This paper provides the statistical properties of the desired estimators and establishes the minimax rate. Through extensive numerical experiments on synthetic data and real examples, we validate our theoretical findings and demonstrate the effectiveness of our proposed method.",
    "published": "2023-10-21",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "pdf_path": "data/papers\\2310.13966v3.pdf"
  },
  {
    "arxiv_id": "1710.07004v2",
    "title": "Modal Regression using Kernel Density Estimation: a Review",
    "authors": [
      "Yen-Chi Chen"
    ],
    "abstract": "We review recent advances in modal regression studies using kernel density estimation. Modal regression is an alternative approach for investigating relationship between a response variable and its covariates. Specifically, modal regression summarizes the interactions between the response variable and covariates using the conditional mode or local modes. We first describe the underlying model of modal regression and its estimators based on kernel density estimation. We then review the asymptotic properties of the estimators and strategies for choosing the smoothing bandwidth. We also discuss useful algorithms and similar alternative approaches for modal regression, and propose future direction in this field.",
    "published": "2017-10-19",
    "categories": [
      "stat.ME",
      "econ.EM"
    ],
    "pdf_path": "data/papers\\1710.07004v2.pdf"
  },
  {
    "arxiv_id": "1907.08592v2",
    "title": "Kernel Mode Decomposition and programmable/interpretable regression networks",
    "authors": [
      "Houman Owhadi",
      "Clint Scovel",
      "Gene Ryan Yoo"
    ],
    "abstract": "Mode decomposition is a prototypical pattern recognition problem that can be addressed from the (a priori distinct) perspectives of numerical approximation, statistical inference and deep learning. Could its analysis through these combined perspectives be used as a Rosetta stone for deciphering mechanisms at play in deep learning? Motivated by this question we introduce programmable and interpretable regression networks for pattern recognition and address mode decomposition as a prototypical problem. The programming of these networks is achieved by assembling elementary modules decomposing and recomposing kernels and data. These elementary steps are repeated across levels of abstraction and interpreted from the equivalent perspectives of optimal recovery, game theory and Gaussian process regression (GPR). The prototypical mode/kernel decomposition module produces an optimal approximation $(w_1,w_2,\\cdots,w_m)$ of an element $(v_1,v_2,\\ldots,v_m)$ of a product of Hilbert subspaces of a common Hilbert space from the observation of the sum $v:=v_1+\\cdots+v_m$. The prototypical mode/kernel recomposition module performs partial sums of the recovered modes $w_i$ based on the alignment between each recovered mode $w_i$ and the data $v$. We illustrate the proposed framework by programming regression networks approximating the modes $v_i= a_i(t)y_i\\big(\u03b8_i(t)\\big)$ of a (possibly noisy) signal $\\sum_i v_i$ when the amplitudes $a_i$, instantaneous phases $\u03b8_i$ and periodic waveforms $y_i$ may all be unknown and show near machine precision recovery under regularity and separation assumptions on the instantaneous amplitudes $a_i$ and frequencies $\\dot\u03b8_i$. The structure of some of these networks share intriguing similarities with convolutional neural networks while being interpretable, programmable and amenable to theoretical analysis.",
    "published": "2019-07-19",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.NA"
    ],
    "pdf_path": "data/papers\\1907.08592v2.pdf"
  },
  {
    "arxiv_id": "1701.01218v1",
    "title": "Overlapping Cover Local Regression Machines",
    "authors": [
      "Mohamed Elhoseiny",
      "Ahmed Elgammal"
    ],
    "abstract": "We present the Overlapping Domain Cover (ODC) notion for kernel machines, as a set of overlapping subsets of the data that covers the entire training set and optimized to be spatially cohesive as possible. We show how this notion benefit the speed of local kernel machines for regression in terms of both speed while achieving while minimizing the prediction error. We propose an efficient ODC framework, which is applicable to various regression models and in particular reduces the complexity of Twin Gaussian Processes (TGP) regression from cubic to quadratic. Our notion is also applicable to several kernel methods (e.g., Gaussian Process Regression(GPR) and IWTGP regression, as shown in our experiments). We also theoretically justified the idea behind our method to improve local prediction by the overlapping cover. We validated and analyzed our method on three benchmark human pose estimation datasets and interesting findings are discussed.",
    "published": "2017-01-05",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_path": "data/papers\\1701.01218v1.pdf"
  },
  {
    "arxiv_id": "2002.02561v7",
    "title": "Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks",
    "authors": [
      "Blake Bordelon",
      "Abdulkadir Canatar",
      "Cengiz Pehlevan"
    ],
    "abstract": "We derive analytical expressions for the generalization performance of kernel regression as a function of the number of training samples using theoretical methods from Gaussian processes and statistical physics. Our expressions apply to wide neural networks due to an equivalence between training them and kernel regression with the Neural Tangent Kernel (NTK). By computing the decomposition of the total generalization error due to different spectral components of the kernel, we identify a new spectral principle: as the size of the training set grows, kernel machines and neural networks fit successively higher spectral modes of the target function. When data are sampled from a uniform distribution on a high-dimensional hypersphere, dot product kernels, including NTK, exhibit learning stages where different frequency modes of the target function are learned. We verify our theory with simulations on synthetic data and MNIST dataset.",
    "published": "2020-02-07",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\2002.02561v7.pdf"
  },
  {
    "arxiv_id": "1411.0306v3",
    "title": "Fast Randomized Kernel Methods With Statistical Guarantees",
    "authors": [
      "Ahmed El Alaoui",
      "Michael W. Mahoney"
    ],
    "abstract": "One approach to improving the running time of kernel-based machine learning methods is to build a small sketch of the input and use it in lieu of the full kernel matrix in the machine learning task of interest. Here, we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance. By extending the notion of \\emph{statistical leverage scores} to the setting of kernel ridge regression, our main statistical result is to identify an importance sampling distribution that reduces the size of the sketch (i.e., the required number of columns to be sampled) to the \\emph{effective dimensionality} of the problem. This quantity is often much smaller than previous bounds that depend on the \\emph{maximal degrees of freedom}. Our main algorithmic result is to present a fast algorithm to compute approximations to these scores. This algorithm runs in time that is linear in the number of samples---more precisely, the running time is $O(np^2)$, where the parameter $p$ depends only on the trace of the kernel matrix and the regularization parameter---and it can be applied to the matrix of feature vectors, without having to form the full kernel matrix. This is obtained via a variant of length-squared sampling that we adapt to the kernel setting in a way that is of independent interest. Lastly, we provide empirical results illustrating our theory, and we discuss how this new notion of the statistical leverage of a data point captures in a fine way the difficulty of the original statistical learning problem.",
    "published": "2014-11-02",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "pdf_path": "data/papers\\1411.0306v3.pdf"
  },
  {
    "arxiv_id": "1905.10843v8",
    "title": "Asymptotic learning curves of kernel methods: empirical data v.s. Teacher-Student paradigm",
    "authors": [
      "Stefano Spigler",
      "Mario Geiger",
      "Matthieu Wyart"
    ],
    "abstract": "How many training data are needed to learn a supervised task? It is often observed that the generalization error decreases as $n^{-\u03b2}$ where $n$ is the number of training examples and $\u03b2$ an exponent that depends on both data and algorithm. In this work we measure $\u03b2$ when applying kernel methods to real datasets. For MNIST we find $\u03b2\\approx 0.4$ and for CIFAR10 $\u03b2\\approx 0.1$, for both regression and classification tasks, and for Gaussian or Laplace kernels. To rationalize the existence of non-trivial exponents that can be independent of the specific kernel used, we study the Teacher-Student framework for kernels. In this scheme, a Teacher generates data according to a Gaussian random field, and a Student learns them via kernel regression. With a simplifying assumption -- namely that the data are sampled from a regular lattice -- we derive analytically $\u03b2$ for translation invariant kernels, using previous results from the kriging literature. Provided that the Student is not too sensitive to high frequencies, $\u03b2$ depends only on the smoothness and dimension of the training data. We confirm numerically that these predictions hold when the training points are sampled at random on a hypersphere. Overall, the test error is found to be controlled by the magnitude of the projection of the true function on the kernel eigenvectors whose rank is larger than $n$. Using this idea we predict relate the exponent $\u03b2$ to an exponent $a$ describing how the coefficients of the true function in the eigenbasis of the kernel decay with rank. We extract $a$ from real data by performing kernel PCA, leading to $\u03b2\\approx0.36$ for MNIST and $\u03b2\\approx0.07$ for CIFAR10, in good agreement with observations. We argue that these rather large exponents are possible due to the small effective dimension of the data.",
    "published": "2019-05-26",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "pdf_path": "data/papers\\1905.10843v8.pdf"
  },
  {
    "arxiv_id": "2110.03922v6",
    "title": "The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks",
    "authors": [
      "James B. Simon",
      "Madeline Dickens",
      "Dhruva Karkada",
      "Michael R. DeWeese"
    ],
    "abstract": "We derive simple closed-form estimates for the test risk and other generalization metrics of kernel ridge regression (KRR). Relative to prior work, our derivations are greatly simplified and our final expressions are more readily interpreted. These improvements are enabled by our identification of a sharp conservation law which limits the ability of KRR to learn any orthonormal basis of functions. Test risk and other objects of interest are expressed transparently in terms of our conserved quantity evaluated in the kernel eigenbasis. We use our improved framework to: i) provide a theoretical explanation for the \"deep bootstrap\" of Nakkiran et al (2020), ii) generalize a previous result regarding the hardness of the classic parity problem, iii) fashion a theoretical tool for the study of adversarial robustness, and iv) draw a tight analogy between KRR and a well-studied system in statistical physics.",
    "published": "2021-10-08",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\2110.03922v6.pdf"
  },
  {
    "arxiv_id": "1402.3849v1",
    "title": "Scalable Kernel Clustering: Approximate Kernel k-means",
    "authors": [
      "Radha Chitta",
      "Rong Jin",
      "Timothy C. Havens",
      "Anil K. Jain"
    ],
    "abstract": "Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data. Among various kernel-based clustering algorithms, kernel k-means has gained popularity due to its simple iterative nature and ease of implementation. However, its run-time complexity and memory footprint increase quadratically in terms of the size of the data set, and hence, large data sets cannot be clustered efficiently. In this paper, we propose an approximation scheme based on randomization, called the Approximate Kernel k-means. We approximate the cluster centers using the kernel similarity between a few sampled points and all the points in the data set. We show that the proposed method achieves better clustering performance than the traditional low rank kernel approximation based clustering schemes. We also demonstrate that its running time and memory requirements are significantly lower than those of kernel k-means, with only a small reduction in the clustering quality on several public domain large data sets. We then employ ensemble clustering techniques to further enhance the performance of our algorithm.",
    "published": "2014-02-16",
    "categories": [
      "cs.CV",
      "cs.DS",
      "cs.LG"
    ],
    "pdf_path": "data/papers\\1402.3849v1.pdf"
  },
  {
    "arxiv_id": "2402.06763v3",
    "title": "Scalable Kernel Logistic Regression with Nystr\u00f6m Approximation: Theoretical Analysis and Application to Discrete Choice Modelling",
    "authors": [
      "Jos\u00e9 \u00c1ngel Mart\u00edn-Baos",
      "Ricardo Garc\u00eda-R\u00f3denas",
      "Luis Rodriguez-Benitez",
      "Michel Bierlaire"
    ],
    "abstract": "The application of kernel-based Machine Learning (ML) techniques to discrete choice modelling using large datasets often faces challenges due to memory requirements and the considerable number of parameters involved in these models. This complexity hampers the efficient training of large-scale models. This paper addresses these problems of scalability by introducing the Nystr\u00f6m approximation for Kernel Logistic Regression (KLR) on large datasets. The study begins by presenting a theoretical analysis in which: i) the set of KLR solutions is characterised, ii) an upper bound to the solution of KLR with Nystr\u00f6m approximation is provided, and finally iii) a specialisation of the optimisation algorithms to Nystr\u00f6m KLR is described. After this, the Nystr\u00f6m KLR is computationally validated. Four landmark selection methods are tested, including basic uniform sampling, a k-means sampling strategy, and two non-uniform methods grounded in leverage scores. The performance of these strategies is evaluated using large-scale transport mode choice datasets and is compared with traditional methods such as Multinomial Logit (MNL) and contemporary ML techniques. The study also assesses the efficiency of various optimisation techniques for the proposed Nystr\u00f6m KLR model. The performance of gradient descent, Momentum, Adam, and L-BFGS-B optimisation methods is examined on these datasets. Among these strategies, the k-means Nystr\u00f6m KLR approach emerges as a successful solution for applying KLR to large datasets, particularly when combined with the L-BFGS-B and Adam optimisation methods. The results highlight the ability of this strategy to handle datasets exceeding 200,000 observations while maintaining robust performance.",
    "published": "2024-02-09",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\2402.06763v3.pdf"
  },
  {
    "arxiv_id": "1610.00660v1",
    "title": "Kernel Selection using Multiple Kernel Learning and Domain Adaptation in Reproducing Kernel Hilbert Space, for Face Recognition under Surveillance Scenario",
    "authors": [
      "Samik Banerjee",
      "Sukhendu Das"
    ],
    "abstract": "Face Recognition (FR) has been the interest to several researchers over the past few decades due to its passive nature of biometric authentication. Despite high accuracy achieved by face recognition algorithms under controlled conditions, achieving the same performance for face images obtained in surveillance scenarios, is a major hurdle. Some attempts have been made to super-resolve the low-resolution face images and improve the contrast, without considerable degree of success. The proposed technique in this paper tries to cope with the very low resolution and low contrast face images obtained from surveillance cameras, for FR under surveillance conditions. For Support Vector Machine classification, the selection of appropriate kernel has been a widely discussed issue in the research community. In this paper, we propose a novel kernel selection technique termed as MFKL (Multi-Feature Kernel Learning) to obtain the best feature-kernel pairing. Our proposed technique employs a effective kernel selection by Multiple Kernel Learning (MKL) method, to choose the optimal kernel to be used along with unsupervised domain adaptation method in the Reproducing Kernel Hilbert Space (RKHS), for a solution to the problem. Rigorous experimentation has been performed on three real-world surveillance face datasets : FR\\_SURV, SCface and ChokePoint. Results have been shown using Rank-1 Recognition Accuracy, ROC and CMC measures. Our proposed method outperforms all other recent state-of-the-art techniques by a considerable margin.",
    "published": "2016-10-03",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "pdf_path": "data/papers\\1610.00660v1.pdf"
  },
  {
    "arxiv_id": "1805.00861v1",
    "title": "Modelling cross-dependencies between Spain's regional tourism markets with an extension of the Gaussian process regression model",
    "authors": [
      "Oscar Claveria",
      "Enric Monte",
      "Salvador Torra"
    ],
    "abstract": "This study presents an extension of the Gaussian process regression model for multiple-input multiple-output forecasting. This approach allows modelling the cross-dependencies between a given set of input variables and generating a vectorial prediction. Making use of the existing correlations in international tourism demand to all seventeen regions of Spain, the performance of the proposed model is assessed in a multiple-step-ahead forecasting comparison. The results of the experiment in a multivariate setting show that the Gaussian process regression model significantly improves the forecasting accuracy of a multi-layer perceptron neural network used as a benchmark. The results reveal that incorporating the connections between different markets in the modelling process may prove very useful to refine predictions at a regional level.",
    "published": "2018-05-02",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_path": "data/papers\\1805.00861v1.pdf"
  },
  {
    "arxiv_id": "2203.09179v3",
    "title": "Maximum Likelihood Estimation in Gaussian Process Regression is Ill-Posed",
    "authors": [
      "Toni Karvonen",
      "Chris J. Oates"
    ],
    "abstract": "Gaussian process regression underpins countless academic and industrial applications of machine learning and statistics, with maximum likelihood estimation routinely used to select appropriate parameters for the covariance kernel. However, it remains an open problem to establish the circumstances in which maximum likelihood estimation is well-posed, that is, when the predictions of the regression model are insensitive to small perturbations of the data. This article identifies scenarios where the maximum likelihood estimator fails to be well-posed, in that the predictive distributions are not Lipschitz in the data with respect to the Hellinger distance. These failure cases occur in the noiseless data setting, for any Gaussian process with a stationary covariance function whose lengthscale parameter is estimated using maximum likelihood. Although the failure of maximum likelihood estimation is part of Gaussian process folklore, these rigorous theoretical results appear to be the first of their kind. The implication of these negative results is that well-posedness may need to be assessed post-hoc, on a case-by-case basis, when maximum likelihood estimation is used to train a Gaussian process model.",
    "published": "2022-03-17",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\2203.09179v3.pdf"
  },
  {
    "arxiv_id": "1302.4245v3",
    "title": "Gaussian Process Kernels for Pattern Discovery and Extrapolation",
    "authors": [
      "Andrew Gordon Wilson",
      "Ryan Prescott Adams"
    ],
    "abstract": "Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density -- the Fourier transform of a kernel -- with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that we can reconstruct standard covariances within our framework.",
    "published": "2013-02-18",
    "categories": [
      "stat.ML",
      "cs.AI",
      "stat.ME"
    ],
    "pdf_path": "data/papers\\1302.4245v3.pdf"
  },
  {
    "arxiv_id": "1912.06552v1",
    "title": "Active emulation of computer codes with Gaussian processes -- Application to remote sensing",
    "authors": [
      "Daniel Heestermans Svendsen",
      "Luca Martino",
      "Gustau Camps-Valls"
    ],
    "abstract": "Many fields of science and engineering rely on running simulations with complex and computationally expensive models to understand the involved processes in the system of interest. Nevertheless, the high cost involved hamper reliable and exhaustive simulations. Very often such codes incorporate heuristics that ironically make them less tractable and transparent. This paper introduces an active learning methodology for adaptively constructing surrogate models, i.e. emulators, of such costly computer codes in a multi-output setting. The proposed technique is sequential and adaptive, and is based on the optimization of a suitable acquisition function. It aims to achieve accurate approximations, model tractability, as well as compact and expressive simulated datasets. In order to achieve this, the proposed Active Multi-Output Gaussian Process Emulator (AMOGAPE) combines the predictive capacity of Gaussian Processes (GPs) with the design of an acquisition function that favors sampling in low density and fluctuating regions of the approximation functions. Comparing different acquisition functions, we illustrate the promising performance of the method for the construction of emulators with toy examples, as well as for a widely used remote sensing transfer code.",
    "published": "2019-12-13",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\1912.06552v1.pdf"
  },
  {
    "arxiv_id": "1911.00955v2",
    "title": "Gaussian process surrogate modeling with manipulating factors for carbon nanotube growth experiments",
    "authors": [
      "Chiwoo Park",
      "Rahul Rao",
      "Pavel Nikolaev",
      "Benji Maruyama"
    ],
    "abstract": "This paper presents a new Gaussian process (GP) surrogate modeling for predicting the outcome of a physical experiment where some experimental inputs are controlled by other manipulating factors. Particularly, we are interested in the case where the control precision is not very high, so the input factor values vary significantly even under the same setting of the corresponding manipulating factors. The case is observed in our main application to carbon nanotube growth experiments, where one experimental input among many is manipulated by another manipulating factors, and the relation between the input and the manipulating factors significantly varies in the dates and times of operations. Due to this variation, the standard GP surrogate that directly relates the manipulating factors to the experimental outcome does not provide a great predictive power on the outcome. At the same time, the GP model relating the main factors to the outcome directly is not appropriate for the prediction purpose because the main factors cannot be accurately set as planned for a future experiment. Motivated by the carbon nanotube example, we propose a two-tiered GP model, where the bottom tier relates the manipulating factors to the corresponding main factors with potential biases and variation independent of the manipulating factors, and the top tier relates the main factors to the experimental outcome. Our two-tier model explicitly models the propagation of the control uncertainty to the experimental outcome through the two GP modeling tiers. We present the inference and hyper-parameter estimation of the proposed model. The proposed approach is illustrated with the motivating example of a closed-loop autonomous research system for carbon nanotube growth experiments, and the test results are reported with the comparison to a benchmark method, i.e. a standard GP model.",
    "published": "2019-11-03",
    "categories": [
      "stat.ME"
    ],
    "pdf_path": "data/papers\\1911.00955v2.pdf"
  },
  {
    "arxiv_id": "2002.02826v3",
    "title": "Conditional Deep Gaussian Processes: multi-fidelity kernel learning",
    "authors": [
      "Chi-Ken Lu",
      "Patrick Shafto"
    ],
    "abstract": "Deep Gaussian Processes (DGPs) were proposed as an expressive Bayesian model capable of a mathematically grounded estimation of uncertainty. The expressivity of DPGs results from not only the compositional character but the distribution propagation within the hierarchy. Recently, [1] pointed out that the hierarchical structure of DGP well suited modeling the multi-fidelity regression, in which one is provided sparse observations with high precision and plenty of low fidelity observations. We propose the conditional DGP model in which the latent GPs are directly supported by the fixed lower fidelity data. Then the moment matching method in [2] is applied to approximate the marginal prior of conditional DGP with a GP. The obtained effective kernels are implicit functions of the lower-fidelity data, manifesting the expressivity contributed by distribution propagation within the hierarchy. The hyperparameters are learned via optimizing the approximate marginal likelihood. Experiments with synthetic and high dimensional data show comparable performance against other multi-fidelity regression methods, variational inference, and multi-output GP. We conclude that, with the low fidelity data and the hierarchical DGP structure, the effective kernel encodes the inductive bias for true function allowing the compositional freedom discussed in [3,4].",
    "published": "2020-02-07",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\2002.02826v3.pdf"
  },
  {
    "arxiv_id": "1911.09946v3",
    "title": "Actively Learning Gaussian Process Dynamics",
    "authors": [
      "Mona Buisson-Fenet",
      "Friedrich Solowjow",
      "Sebastian Trimpe"
    ],
    "abstract": "Despite the availability of ever more data enabled through modern sensor and computer technology, it still remains an open problem to learn dynamical systems in a sample-efficient way. We propose active learning strategies that leverage information-theoretical properties arising naturally during Gaussian process regression, while respecting constraints on the sampling process imposed by the system dynamics. Sample points are selected in regions with high uncertainty, leading to exploratory behavior and data-efficient training of the model. All results are finally verified in an extensive numerical benchmark.",
    "published": "2019-11-22",
    "categories": [
      "cs.LG",
      "cs.RO",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\1911.09946v3.pdf"
  },
  {
    "arxiv_id": "1809.04967v3",
    "title": "Gaussian process classification using posterior linearisation",
    "authors": [
      "\u00c1ngel F. Garc\u00eda-Fern\u00e1ndez",
      "Filip Tronarp",
      "Simo S\u00e4rkk\u00e4"
    ],
    "abstract": "This paper proposes a new algorithm for Gaussian process classification based on posterior linearisation (PL). In PL, a Gaussian approximation to the posterior density is obtained iteratively using the best possible linearisation of the conditional mean of the labels and accounting for the linearisation error. PL has some theoretical advantages over expectation propagation (EP): all calculated covariance matrices are positive definite and there is a local convergence theorem. In experimental data, PL has better performance than EP with the noisy threshold likelihood and the parallel implementation of the algorithms.",
    "published": "2018-09-13",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "pdf_path": "data/papers\\1809.04967v3.pdf"
  },
  {
    "arxiv_id": "1110.4411v1",
    "title": "Gaussian Process Regression Networks",
    "authors": [
      "Andrew Gordon Wilson",
      "David A. Knowles",
      "Zoubin Ghahramani"
    ],
    "abstract": "We introduce a new regression framework, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the non-parametric flexibility of Gaussian processes. This model accommodates input dependent signal and noise correlations between multiple response variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both efficient Markov chain Monte Carlo and variational Bayes inference procedures for this model. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on benchmark datasets, including a 1000 dimensional gene expression dataset.",
    "published": "2011-10-19",
    "categories": [
      "stat.ML",
      "q-fin.ST",
      "stat.ME"
    ],
    "pdf_path": "data/papers\\1110.4411v1.pdf"
  },
  {
    "arxiv_id": "2310.19390v2",
    "title": "Implicit Manifold Gaussian Process Regression",
    "authors": [
      "Bernardo Fichera",
      "Viacheslav Borovitskiy",
      "Andreas Krause",
      "Aude Billard"
    ],
    "abstract": "Gaussian process regression is widely used because of its ability to provide well-calibrated uncertainty estimates and handle small or sparse datasets. However, it struggles with high-dimensional data. One possible way to scale this technique to higher dimensions is to leverage the implicit low-dimensional manifold upon which the data actually lies, as postulated by the manifold hypothesis. Prior work ordinarily requires the manifold structure to be explicitly provided though, i.e. given by a mesh or be known to be one of the well-known manifolds like the sphere. In contrast, in this paper we propose a Gaussian process regression technique capable of inferring implicit structure directly from data (labeled and unlabeled) in a fully differentiable way. For the resulting model, we discuss its convergence to the Mat\u00e9rn Gaussian process on the assumed manifold. Our technique scales up to hundreds of thousands of data points, and may improve the predictive performance and calibration of the standard Gaussian process regression in high-dimensional settings.",
    "published": "2023-10-30",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "pdf_path": "data/papers\\2310.19390v2.pdf"
  },
  {
    "arxiv_id": "2304.12923v1",
    "title": "Quantum Gaussian Process Regression for Bayesian Optimization",
    "authors": [
      "Frederic Rapp",
      "Marco Roth"
    ],
    "abstract": "Gaussian process regression is a well-established Bayesian machine learning method. We propose a new approach to Gaussian process regression using quantum kernels based on parameterized quantum circuits. By employing a hardware-efficient feature map and careful regularization of the Gram matrix, we demonstrate that the variance information of the resulting quantum Gaussian process can be preserved. We also show that quantum Gaussian processes can be used as a surrogate model for Bayesian optimization, a task that critically relies on the variance of the surrogate model. To demonstrate the performance of this quantum Bayesian optimization algorithm, we apply it to the hyperparameter optimization of a machine learning model which performs regression on a real-world dataset. We benchmark the quantum Bayesian optimization against its classical counterpart and show that quantum version can match its performance.",
    "published": "2023-04-25",
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "pdf_path": "data/papers\\2304.12923v1.pdf"
  }
]