{
  "What is ridge regression?": {
    "answer": "Ridge regression, also known as ℓ2 regularization, is an alternative estimator for linear regression coefficients designed to address issues arising from multicollinearity or an ill-conditioned design matrix XᵀX [1, 2]. This condition typically occurs when the regression matrix X has a column rank less than p, or its condition number (ratio of largest to smallest singular value) is poor, making the ordinary least squares (OLS) estimator unstable or undefined [2].\n\n### Mathematical Formulation\n\nThe standard OLS estimator is given by $\\hat{\\beta} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$ [2]. Ridge regression modifies this by adding a positive constant $\\lambda$ to the diagonal elements of $\\mathbf{X}^{\\top}\\mathbf{X}$, resulting in the ridge estimator $\\hat{\\beta}_{\\lambda}$:\n\n$\\hat{\\beta}_{\\lambda} = (\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_p)^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$ [1, 2]\n\nHere, $\\mathbf{I}_p$ is the $p \\times p$ identity matrix, and $\\lambda > 0$ is the ridge parameter [1, 2]. Adding $\\lambda\\mathbf{I}_p$ (a \"ridge on the diagonal\") increases all eigenvalues of $\\mathbf{X}^{\\top}\\mathbf{X}$ by $\\lambda$, ensuring that $(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_p)$ is necessarily non-singular and invertible, even if $\\mathbf{X}^{\\top}\\mathbf{X}$ is singular [1, 2].\n\n### Equivalent Characterizations\n\nThe ridge estimator can be characterized in at least four equivalent ways [1]:\n\n1.  **Penalized Least-Squares Estimator:** Ridge regression is equivalent to minimizing the sum of squared residuals with an added penalty term based on the $\\ell_2$ (Euclidean) norm of the coefficients:\n\n    $\\hat{\\beta}_{\\lambda} = \\arg \\min_{\\beta} ||\\mathbf{y} - \\mathbf{X}\\beta||^2_2 + \\lambda ||\\beta||^2_2$ [1, 2]\n\n    This is referred to as the Lagrange form [2]. The term $\\lambda ||\\beta||^2_2 = \\lambda \\sum_{k=1}^p \\beta_k^2$ is the $\\ell_2$ penalty [1, 2].\n\n2.  **Constrained Estimation Problem:** The ridge estimator is also the solution to a constrained optimization problem:\n\n    $\\min_{\\beta} ||\\mathbf{y} - \\mathbf{X}\\beta||^2_2 \\quad \\text{subject to} \\quad ||\\beta||^2_2 \\le c$ [2]\n\n    For every solution $\\hat{\\beta}_{\\lambda}$ to the penalized problem, there is an equivalent solution to the constrained problem with $c = ||\\hat{\\beta}_{\\lambda}||^2_2$ [2]. Geometrically, this means the ridge estimate is the point where the smallest elliptical contour of the OLS loss function (sum-of-squares criterion) intersects with the circular ridge parameter constraint region defined by $||\\beta||^2_2 \\le c$ [3]. The ridge estimator is always found on the boundary of this constraint [3]. This constraint shrinks monotonically as $\\lambda$ increases, eventually collapsing to zero as $\\lambda \\to \\infty$ [3].\n\n3.  **Orthogonalized Space Shrinkage:** In an orthogonalized space, the ridge estimator $\\hat{\\beta}_{\\lambda}$ is obtained by shrinking the canonical OLS estimator $\\hat{\\alpha} = (\\hat{\\alpha}_1, \\dots, \\hat{\\alpha}_q)^{\\top}$ by factors of $\\{d_k^2 / (d_k^2 + \\lambda)\\}$ for $k = 1, \\dots, q$ [1]. This implies that a greater amount of shrinkage is applied to OLS estimates $\\hat{\\alpha}_k$ that have relatively small eigenvalues ($d_k^2$) [1].\n\n4.  **Bayesian Perspective:** From a Bayesian viewpoint, the ridge estimator $\\hat{\\beta}_{\\lambda}$ is the mean of the posterior distribution of $\\beta$ under a $p$-variate normal prior distribution, specifically $N_p(\\beta | \\mathbf{0}, \\sigma^2\\lambda^{-1}\\mathbf{I}_p)$ for the conditional random variable $\\beta | \\sigma^2$. This estimate minimizes the posterior expected squared-error loss [1].\n\n### Key Properties and Characteristics\n\n*   **Addressing Ill-conditioning and Singularity:** Ridge regression provides a solution when $\\mathbf{X}^{\\top}\\mathbf{X}$ is ill-conditioned or singular, which prevents the calculation of the OLS estimator [1, 2]. By adding $\\lambda\\mathbf{I}_p$, the matrix $(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_p)$ becomes non-singular, allowing for inversion [1, 2].\n*   **Bias-Variance Trade-off:** Compared to the OLS estimator, the ridge estimator introduces some bias into the coefficient estimates. In exchange, it achieves lower mean-squared error (MSE) and prediction error (PE), especially when $\\mathbf{X}^{\\top}\\mathbf{X}$ is ill-conditioned or singular [1]. It also has smaller variance than the OLS estimate [2]. The selection of $\\lambda$ amounts to navigating this bias-variance trade-off [2].\n*   **Coefficient Shrinkage:** The ridge parameter $\\lambda > 0$ shrinks the estimates of the coefficients $\\beta$ towards zero [1, 2]. The amount of shrinkage increases as $\\lambda$ increases [1].\n*   **Relationship to OLS:** When $\\lambda = 0$, the ridge estimator $\\hat{\\beta}_{\\lambda}$ reverts to the OLS estimator $\\hat{\\beta}$ [1].\n*   **Effect on Variance Inflation Factor (VIF):** For the maximum likelihood estimator, the VIF quantifies the increase in variance due to collinearity [4]. For the ridge regression estimator, a modified \"ridge VIF\" is defined. When the design matrix X is orthogonal and $\\lambda > 0$, penalization leads to a \"ridge VIF\" that is less than 1, meaning penalization *deflates* the VIF [4].\n*   **Numerical Stability:** The evaluation of the ridge regression estimator can be affected by numerical inaccuracy if $\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_p$ remains ill-conditioned, which occurs if $\\lambda$ is too close to zero [5]. To ensure numerical accuracy, the choice of $\\lambda$ must yield a well-conditioned matrix, often assessed using a condition number plot [5]. Computational efficiency can be improved by using the Woodbury identity to avoid the inversion of the $p \\times p$ matrix $\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_p$, replacing it with an $n \\times n$ matrix inversion:\n\n    $\\hat{\\beta}(\\lambda) = \\lambda^{-1}\\mathbf{X}^{\\top}(\\mathbf{I}_n + \\lambda^{-1}\\mathbf{X}\\mathbf{X}^{\\top})^{-1}\\mathbf{y}$ [5]\n\n### Practical Implications and Applications\n\n*   **Overfitting Prevention:** Ridge regression is a remedy for overfitting, especially in high-dimensional settings where the number of explanatory variables $p$ exceeds the number of observations $n$ ($p \\ge n$) [1, 3]. Large regression coefficient estimates are often an indication of overfitting, and the constraint on coefficient size in ridge regression helps mitigate this [3].\n*   **General Linear Models (GLMs):** Ridge regularization is not limited to standard linear regression. It is applicable to GLMs, such as logistic regression, Cox models, and multiclass logistic regression, where it can stabilize the Newton algorithm by adding $\\lambda\\mathbf{I}$ to the Hessian matrix, preventing instability if the design matrix X is ill-conditioned or if classes are perfectly separated [2]. This involves maximizing a penalized likelihood function: $\\ell(\\beta; \\mathbf{X}, \\mathbf{y}) - \\lambda||\\beta||^2_2$ [2].\n*   **Wide-Data Situations:** In fields like genomics or document classification where $p \\gg n$, regularization, including ridge regression, is essential [2].\n\n### Choice of the Ridge Parameter ($\\lambda$)\n\nThe choice of $\\lambda$ is critical for the quality of coefficient estimates and predictions [1, 5]. It controls the amount of bias introduced and the degree of shrinkage [1, 2]. Various methods exist for selecting $\\lambda$:\n\n*   **Automatic Plug-in Estimation:** An example is the Hoerl-Kennard-Baldwin (HKB) plug-in estimator, $\\hat{\\lambda}_{\\text{HKB}} = p\\hat{\\sigma}^2 / \\hat{\\beta}^{\\top}\\hat{\\beta}$ [1]. This estimator is motivated by minimizing the model's expected prediction error when X is orthonormal [1]. However, the standard HKB estimator relies on OLS estimates and does not exist if $\\mathbf{X}^{\\top}\\mathbf{X}$ is singular, though extensions exist for such cases [1].\n*   **Cross-Validation:** This involves specifying a grid of trial values for $\\lambda$, then selecting the value that yields the smallest prediction error in K-fold cross-validation (e.g., 10-fold or n-fold) [1].\n*   **Information Criteria Optimization** and **Markov Chain Monte Carlo (MCMC)** are other methods [1].\n*   **Degrees of Freedom:** One can bound the search domain of $\\lambda$ by limiting the degrees of freedom spent on estimating the regression parameters, for example, choosing $\\lambda$ such that $df[\\hat{\\beta}(\\lambda)] \\le \\nu$, where $\\nu$ is the maximum desired degrees of freedom [5].\n\n### Comparison to Other Regularization Methods\n\nRidge regression uses an $\\ell_2$ penalty. In contrast, the Lasso (Least Absolute Shrinkage and Selection Operator) uses an $\\ell_1$ penalty: $\\min_{\\beta} ||\\mathbf{y} - \\mathbf{X}\\beta||^2_2 + \\lambda||\\beta||_1$ [2]. While both shrink coefficients, the $\\ell_1$ penalty of Lasso can set some coefficients exactly to zero, thereby performing variable selection, which ridge regression does not [2]. The elastic net combines both $\\ell_1$ and $\\ell_2$ penalties: $\\lambda[(1-\\alpha)||\\beta||^2_2 + \\alpha||\\beta||_1]$ [2].",
    "chunks": [
      {
        "text": "a result of multicollinearity. For example, multicollinearity can likely occur when the data consist of a large number of covariates (p), or can occur when the data set is constructed by merging multiple data sets that have one or more variables in common. Ridge regression, which deﬁnes the alternative estimator βλ = (X⊺X + λIp)−1X⊺y, pro- vides a solution to the inability of the OLS estimator bβ to handle ill-conditioned or singular X⊺X. Here, λ > 0 is the ridge parameter that shrinks the estimate of the coeﬃcients β towards zero, with the amount of shrinking an increasing function of λ. Compared to the OLS estimator bβ, the ridge estimator βλ introduces some bias in exchange for lower mean-squared MSE(bβλ | β) and prediction error PE(Xbβλ | β), especially when X⊺X is ill- conditioned or singular (Hoerl & Kennard, 1970). However, unlike the OLS estimator, the ridge estimator βλ exists even when X⊺X is singular, because (X⊺X + λIp) is necessarily non-singular when λ > 0. The ridge estimator βλ can be characterized in at least four equivalent ways (e.g., Hastie, et al. 2009). First, the ridge estimator is equivalent to the penalized least-squares estimator, βλ = arg minβ ||y −Xβ||2 + λ Pp k=1 β2 k. Second, in terms of the orthogonalized space, the ridge estimator is given by βλ = Wbαλ, with bαλ,k = {d2 k/(d2 k + λ)}bαk for k = 1, . . . , q. This shows that the ridge estimator βλ is obtained by shrinking the canonical OLS estimator bα = (bα1, . . . , bαq)⊺by the factors {d2 k/(d2 k+λ)} (resp.), and therefore applies a greater amount of shrinkage to OLS estimates bαk having relatively small eigenvalues. Third, from a Bayesian point of view, the ridge estimator βλ is the mean of the posterior distribution of β under a p-variate normal prior distribution, corresponding to normal probability density function (p.d.f.) np(β | 0, σ2λ−1Ip) for the conditional random variable β | σ2. Thus, from a Bayesian decision-theoretic perspective, βλ is the choice of point estimate of β that minimizes the posterior expected squared-error loss. Fourth, when λ = 0, the ridge estimator βλ becomes the OLS estimator bβ. In ridge regression, the quality of coeﬃcient estimates βλ and predictions hinge on the choice of the ridge parameter, λ. As a result, several methods have been proposed to estimate this parameter, based on either automatic plug-in estimation, cross-validation, information criteria optimization, or Markov chain Monte Carlo (MCMC), which we now brieﬂy review. An enormous literature on these methods has developed over the past 45 years, however, the vast majority of them rely on the OLS estimates (bβ, bσ2) (Cule & DeIorio, 2012). In keeping with the general spirit of this paper, we focus our review on ridge methods that can handle data sets where X⊺X is either positive-deﬁnite, ill-conditioned, or singular (including when p ≥n), unless indicated otherwise. The standard, Hoerl-Kennard-Baldwin (HKB) (1975) plug-in estimator for the ridge regression model is deﬁned by bλHKB = pbσ2/bβ ⊺bβ. This estimator is motivated by the fact that λHKB = pσ2/β⊺β is the choice of λ that minimizes the model’s expected prediction error when X is orthonormal (Hoerl & Kennard, 1970). The HKB estimator relies on the OLS estimate bσ2, and therefore does not exist when X⊺X is singular, including when p ≥n. 3 However, this estimator was extended to singular and p ≥n settings (Cule & DeIorio, 2013), by replacing bσ2 with the error variance estimate bσ2 r = 1 n−br||y −Xbαbr||2 obtained from a principal components regression having br ≤q = min(n, p) components with coeﬃcients α = (α1, . . . , αr)⊺. Here, br is the value of r that minimizes r−Pq k=1 d4 k/(d2 k +λr)2. Therefore br best matches the degrees of freedom for variance in the ridge regression model with ridge parameter λbr = brbσ2 br/bα⊺ br bαbr. The HKB plug-in estimator is attractive because it quickly obtains a ridge estimate bλ without iteration. However this estimator may lead to an estimate of λ that is not necessarily optimal for the given data set at hand, especially when X is not orthonormal. The method of cross-validation, for ridge regression, involves ﬁrst specifying a grid of trial values of λ, and then selecting the estimate bλ as the trial value that provides the smallest prediction error for the ridge regression model in K-fold cross-validation, usually 10-fold or n-fold (e.g., Hastie et al. 2009, Ch. 3, 7). The method takes K (at least nearly) equal-sized partitions of the n observations at random, and then for each trial value of λ, measures the model’s overall mean-squared predictive error over all K partitions. Overall mean-squared error is based on obtaining the estimate β (k) λ for one partition and measuring the predictive error",
        "metadata": {
          "paper_id": "1409.2437v5",
          "section": "introduction",
          "authors": "George Karabatsos",
          "paper_title": "Fast Marginal Likelihood Estimation of the Ridge Parameter(s) in Ridge Regression and Generalized Ridge Regression for Big Data",
          "chunk_index": 1
        },
        "id": "chunk_577"
      },
      {
        "text": "Ridge or more formally ℓ2 regularization shows up in many areas of statistics and machine learning. It is one of those essential devices that any good data scientist needs to master for their craft. In this brief ridge fest I have collected together some of the magic and beauty of ridge that my colleagues and I have encountered over the past 40 years in applied statistics. 1 Ridge and linear regression We first learn of ridge when we study linear regression. Ridge provides a remedy for an ill-conditioned X⊤X matrix. If our n×p regression matrix X has column rank less than p (or nearly so in terms of its condition number, the ratio of largest to smallest singular value), then the usual least-squares regression equation is in trouble: ˆβ = (X⊤X)−1X⊤y. (1) The poor (large) condition number of X is inherited by X⊤X, which is either singular or nearly so, and here we try to invert it. The problem is that X⊤X has some eigenvalues of zero or nearly zero, so inverting is not a good idea. So what we do is add a ridge on the diagonal — X⊤X + λI with λ > 0 — which increases all the eigenvalues by λ and takes the problem away: ˆβλ = (X⊤X + λI)−1X⊤y. (2) This is the ridge regression solution proposed by Hoerl and Kennard [1970] 50 years ago, and as we will see is alive and strong today. We can write out the optimization problem that ridge is solving, minimize β ∥y −Xβ∥2 2 + λ∥β∥2 2, (3) 1 arXiv:2006.00371v2 [stat.ME] 31 Oct 2024 where ∥· ∥2 is the ℓ2 (Euclidean) norm. Some simple matrix calculus gets us from (3) to (2). What value of λ > 0 shall we use? If our main concern is resolving the numerical issues with solving the least squares equation, then a small value suffices — say λ = 0.001 or perhaps that fraction of the largest eigenvalue of X⊤X. The ridge remedy (2) comes with consequences. Under a true linear model, the ridge estimate is biased toward zero. It also has smaller variance than the OLS estimate. Selecting λ amounts to a bias-variance trade-off. We address this further in Section 3. The ridge modification works in many situations where we fit linear models, and the effect is not as transparent as in (2). • With GLMs we model η(x) = β⊤x and E(y|x) = g(η(x)), and fit by maximum likelihood. If X is ill conditioned, this will lead to a Hessian that is flat (near zero) in some directions, and the Newton algorithm will be unstable. We can instead maximize ℓ(β; X, y) −λ∥β∥2 2, (4) which, as in (2), adds λI to the Hessian, and removes the problem. One example is logistic regression. Here even if X is well behaved, if the classes are separated in x-space, the usual maximum-likelihood estimator is undefined — some coefficients are infinite. A little bit of ridge resolves the issue. • The same is true in the Cox model, multiclass logistic regression, and any other model linear in the parameters. • In wide-data situations where p ≫n, for example in genomics where the variables are SNPs (single nucleotide polymorphisms) and can number in the millions, and in document classification in the tens of thousands. Here regularization is essential, and λ requires careful tuning. Typically we do not penalize the intercept in the linear model; in the case of least squares, we can center X and y upfront, and ignore the intercept. In other GLMs it is handled not quite as simply, but is a detail which for now we will ignore. We have expressed the ridge problem in Lagrange form in (3), as opposed to the more evocative bound form minimize β ∥y −Xβ∥2 subject to ∥β∥2 ≤c. (5) The two problems are of course equivalent: every solution ˆβλ to problem (3) is a solution to (5) with c = ∥ˆβλ∥2. The lasso [Tibshirani, 1996] uses an ℓ1 rather than an ℓ2 penalty in linear models to achieve sparsity: minimize β ∥y −Xβ∥2 2 + λ∥β∥1. (6) 2 β1 β3 β2 β1 β2 β3 β2 β1 β3 Figure 1: Constraint balls for ridge, lasso and elastic-net regularization. The sharp edges and corners of the latter two allow for variable selection as well as shrinkage. This penalty both shrinks coefficients like ridge, but also sets some to zero and thus selects. Figure 1 compares their constraint regions, which explains the ability of lasso to set coefficients to zero. The lasso has inspired ridge-like generalizations, such as the two mentioned here: • elastic net [Zou and Hastie, 2005] which mixes the ridge and lasso penalty λ \u0002 (1 −α)∥β∥2 2 + α∥β∥1 \u0003 . (7) It still selects variables like the lasso, but deals more gracefully",
        "metadata": {
          "chunk_index": 0,
          "section": "abstract",
          "authors": "Trevor Hastie",
          "paper_id": "2006.00371v2",
          "paper_title": "Ridge Regularization: an Essential Concept in Data Science"
        },
        "id": "chunk_667"
      },
      {
        "text": "fit Figure 1.4: Top panels show examples of convex (left) and nonconvex (right) sets. Middle panels show examples of convex (left) and nonconvex (right) functions. The left bottom panel illustrates the ridge estimation as a constrained estimation problem. The ellipses represent the contours of the ML loss function, with the blue dot at the center the ML estimate. The circle is the ridge parameter constraint. The red dot is the ridge estimate. It is at the intersection of the ridge constraint and the smallest contour with a non-empty intersection with the constraint. The right bottom panel shows the data corresponding to Example 1.7. The grey line represents the ‘true’ relationship, while the black line the fitted one. 20 Ridge regression The second KKT condition (the complementarity condition) requires that ν(∥ˆβ(ν)∥2 2 −c) = 0. If ν = λ and c = ∥ˆβ(λ)∥2 2, the ridge regression estimator β(λ) satisfies both KKT conditions. Hence, both problems have the same solution if c = ∥ˆβ(λ)∥2 2. The constrained estimation interpretation of the ridge regression estimator is illustrated in the left bottom panel of Figure 1.4. It shows the level sets of the sum-of-squares criterion and centered around zero the circular ridge parameter constraint, parametrized by {β : ∥β∥2 2 = β2 1 +β2 2 ≤c} for some c > 0. The ridge regression estimator is then the point where the sum-of-squares’ smallest level set hits the constraint. Exactly at that point the sum- of-squares is minimized over those β’s that live on or inside the constraint. In the high-dimensional setting the ellipsoidal level sets are degenerated. For instance, in the 2-dimensional case of the left bottom panel of Figure 1.4, the ellipsoids would then be lines but the geometric interpretation is unaltered. The ridge regression estimator is always to be found on the boundary of the ridge parameter constraint and is never an interior point. To see this, assume, for simplicity, that the X⊤X matrix is of full rank. The radius of the ridge parameter constraint can then be bounded as follows: ∥ˆβ(λ)∥2 2 = Y⊤X(X⊤X + λIpp)−2X⊤Y ≤Y⊤X(X⊤X)−2X⊤Y = ∥ˆβ ml∥2 2. The inequality in the display above follows from i) X⊤X ≻0 and λIpp ≻0, ii) X⊤X + λIpp ≻X⊤X (due to Lemma 14.2.4 of Harville, 2008), and iii) (X⊤X)−2 ≻(X⊤X + λIpp)−2 (inferring Corollary 7.7.4 of Horn and Johnson, 2012). The ridge regression estimator is thus always on the boundary or in a circular constraint centered around the origin with a radius that is smaller than the size of the maximum likelihood estimator. Moreover, the constrained estimation formulation of the ridge regression estimator then implies that the latter must be on the boundary of the constraint. The size of the spherical ridge parameter constraint shrinks monotonously as λ increases, and eventually, in the λ →∞-limit collapses to zero (as is formalized by Proposition 1.4). Proposition 1.4 The squared norm of the ridge regression estimator satisfies: i) d∥ˆβ(λ)∥2 2/dλ < 0 for λ > 0, ii) limλ→∞∥ˆβ(λ)∥2 2 = 0. Proof. For part i) we need to verify that ∥ˆβ(λ)∥2 2 > ∥ˆβ(λ + h)∥2 2 for h > 0. Hereto substitute the singular value decomposition of the design matrix, X = UxDxV⊤ x , into the display above. Then, after a little algebra, take the derivative with respect to λ, and obtain: d d λ∥ˆβ(λ)∥2 2 = −2 Xn i=1 d2 x,i(d2 x,i + λ)−3(Y⊤Ux,∗,i)2. This is negative for all λ > 0. Indeed, the parameter constraint thus becomes smaller and smaller as λ increases, and so does the size of the estimator. Part ii) follows from limλ→∞ˆβ(λ) = 0p, which has been concluded previously by other means. ■ The relevance of viewing the ridge regression estimator as the solution to a constrained estimation problem be- comes obvious when considering a typical threat to high-dimensional data analysis: overfitting. Overfitting refers to the phenomenon of modelling the noise rather than the signal. In case the true model is parsimonious (few covariates driving the response) and data on many covariates are available, it is likely that a linear combination of all covariates yields a higher likelihood than a combination of the few that are actually related to the response. As only the few covariates related to the response contain the signal, the model involving all covariates then cannot but explain more than the signal alone: it also models the error. Hence, it overfits the data. In high-dimensional settings overfitting is a real threat. The number of explanatory variables exceeds the number of observations. It is thus possible to form a linear combination of the covariates that perfectly explains the response, including the noise. Large estimates of regression coefficients are often an indication of overfitting. Augmentation of the estimation procedure with a constraint on the regression coefficients is a simple remedy to",
        "metadata": {
          "chunk_index": 3,
          "paper_title": "Lecture notes on ridge regression",
          "paper_id": "1509.09169v8",
          "section": "model",
          "authors": "Wessel N. van Wieringen"
        },
        "id": "chunk_518"
      },
      {
        "text": "E[Y⊤X(X⊤X + λIpp)−2X⊤Y] = σ2 tr \b X(X⊤X + λIpp)−2X⊤ + β⊤X⊤X(X⊤X + λIpp)−2X⊤Xβ. In the last step we have used Y ∼N (Xβ, σ2Ipp) and the expectation of the quadratic form of a multivariate random variable ε ∼N (µε, Σε) is E(ε⊤Λ ε) = tr(Λ Σε) + µ⊤ ε Λµε (cf. Mathai and Provost, 1992). The expression for the expectation of the radius of the ridge constraint can now be evaluated for the orthogonal X and the strongly, positively collinear X. It turns out that the latter is larger than the former. This results in a larger ridge constraint. For the larger ridge constraint there is a smaller level set that hits it first. The point of intersection, still on the x = y-line, is now thus closer to β and further from the origin (cf. right panel of Figure 1.8). The resulting estimate is thus larger than that from the orthogonal case. The above needs some attenuation. Among others it depends on: i) the number of covariates in each block, ii) the size of the effects, i.e. regression coefficients of each covariate, and iii) the degree of collinearity. Possibly, there are more factors influencing the behaviour of the ridge regression estimator presented in this subsection. This behaviour of ridge regression is to be understood if a certain (say) clinical outcome is to be predicted from (say) gene expression data. Genes work in concert to fulfil a certain function in the cell. Consequently, one expects their expression levels to be correlated. Indeed, gene expression studies exhibit many co-expressed genes, that is, genes with correlating transcript levels. But also in most other applications with many explanatory variables collinearity will be omnipresent and similar issues are to be considered. 1.9.3 Variance inflation factor The ridge regression estimator was introduced to resolve the undefinedness of its maximum likelihood counterpart in the face of (super)collinearity amoung the explanatory variables. The effect of collinearity on the uncertainty of estimates is often quantified by the Variance Inflation Factor (VIF). The VIF measures the change in the variance of the estimate due to the collinearity. Here we investigate how penalization affects the VIF. This requires a definition of the VIF of the ridge regression estimator. The VIF of the maximum likelihood estimator of the j-th element of the regression parameter is defined as a factor in the following factorization of the variance of ˆβj: Var(ˆβj) = n−1σ2[(X⊤X)−1]jj n−1σ2[Var(Xi,j | Xi,1, . . . , Xi,j−1, Xi,j+1, . . . , Xi,p)]−1 = n−1σ2 Var(Xi,j) · Var(Xi,j) Var(Xi,j | Xi,1, . . . , Xi,j−1, Xi,j+1, . . . , Xi,p) := n−1σ2[Var(Xi,j)]−1VIF(ˆβj), in which it assumed that the Xi,j’s are random and – using the column-wise zero ‘centered-ness’of X – that n−1X⊤X is estimator of their covariance matrix. Moreover, the identity used to arrive at the second line of the display, [(X⊤X)−1]jj = [Var(Xi,j | Xi,1, . . . , Xi,j−1, Xi,j+1, . . . , Xi,p)]−1, originates from Corollary 5.8.1 of Whittaker (1990). Thus, Var(ˆβj) factorizes into σ2[Var(Xi,j)]−1 and the variance inflation factor VIF(ˆβj). When the j-th covariate is orthogonal to the other, i.e. there is no collinearity, then the VIF’s denominator, Var(Xi,j | Xi,1, . . . , Xi,j−1, Xi,j+1, . . . , Xi,p), equals Var(Xi,j). Consequently, VIF(ˆβj) = 1. When there is collinearity among the covariates Var(Xi,j | Xi,1, . . . , Xi,j−1, Xi,j+1, . . . , Xi,p) < Var(Xi,j) and VIF(ˆβj) > 1. The VIF then inflates the variance of the estimator of βj under orthogonality – hence, the name – by a factor attributable to the collinearity. 1.9 Simulations 31 The definition of the VIF needs modification to apply to the ridge regression estimator. In Marquardt (1970) the ‘ridge VIF’ is defined analogously to the above definition of the VIF of the maximum likelihood regression estimator as: Var[ˆβj(λ)] = σ2[(X⊤X + λIpp)−1X⊤X(X⊤X + λIpp)−1]jj = n−1σ2 Var(Xi,j) · nVar(Xi,j)[(X⊤X + λIpp)−1X⊤X(X⊤X + λIpp)−1]jj := n−1σ2[Var(Xi,j)]−1VIF[ˆβj(λ)], where the factorization is forced in line with that of the ‘maximum likelihood VIF’ but lacks a similar interpre- tation. When X is orthogonal, VIF[ˆβj(λ)] = [Var(Xi,j)]2[Var(Xi,j) + λ]−2 < 1 for λ > 0. Penalization then deflates the VIF. An alternative definition of the ‘ridge VIF’ presented by Garc´ıa et al. (2015) for the ‘p = 2’-case, which they motivate from counterintuitive behaviour observed in the ‘ridge VIF’ defined by Marquardt (1970), adopts the Figure 1.9: Contour plots of ‘Marquardt VIFs’ and ‘Garcia VIFs’, left and right columns, respectively. The top panels show these VIFs against the degree of penalization (x-axis) and collinearity (y-axis) for a fixed sample size (n = 100) and dimension (p = 50). The bottom panels show these VIFs against the degree of penalization (x-axis) and the sample size (y-axis) for a fixed dimension (p = 50). 32",
        "metadata": {
          "paper_title": "Lecture notes on ridge regression",
          "paper_id": "1509.09169v8",
          "section": "model",
          "chunk_index": 11,
          "authors": "Wessel N. van Wieringen"
        },
        "id": "chunk_526"
      },
      {
        "text": "be used in combination with other loss functions, for instance that of standard generalized linear models (see Chapter 5). This computation is illustrated in Figure 1.5, which shows the substantial gain in computation time of the evaluation of the ridge regression estimator using the efficient over the naive implementation against the dimension p. Details of this figure are provided in Question 1.24. 20 40 60 80 100 0e+00 2e+05 4e+05 6e+05 8e+05 1e+06 dimension (p) computation time regular efficient Figure 1.5: Computation time of the evaluation of the ridge regression estimator using the naive and efficient implementation against the dimension p. The inversion of the p × p-dimensional matrix can be avoided in an other way. Hereto one needs the Woodbury identity. Let A, U and V be p × p-, p × n- and n × p-dimensional matrices, respectively. The (simplified form of the) Woodbury identity then is: (A + UV)−1 = A−1 −A−1U(Inn + VA−1U)−1VA−1. Application of the Woodbury identity to the matrix inverse in the ridge estimator of the regression parameter gives: ˆβ(λ) = (X⊤X + λIpp)−1X⊤Y = [λ−1Ipp −λ−2X⊤(Inn + λ−1XX⊤)−1X]X⊤Y = λ−1X⊤(Inn + λ−1XX⊤)−1Y. (1.12) The inversion of the p × p-dimensional matrix X⊤X + λIpp is thus replaced by that of the n × n-dimensional matrix Inn + λ−1XX⊤. In addition, this expression of the ridge regression estimator avoids the singular value decomposition of X, which may in some cases introduce additional numerical errors (e.g. at the level of machine precision). 1.8 Penalty parameter selection Throughout the introduction of the ridge regression estimator and the subsequent discussion of its properties, we considered the penalty parameter known or ‘given’. In practice, it is unknown and the user needs to make an informed decision on its value. We present several strategies to facilitate such a decision. Prior to that, we discuss 1.8 Penalty parameter selection 23 some sanity requirements one may wish to impose on the ridge regression estimator. Those requirements do not yield a specific choice of the penalty parameter but they specify a domain of sensible penalty parameter values. The evaluation of the ridge regression estimator may be subject to numerical inaccuracy, which ideally is avoided. This numerical inaccuracy results from the ill-conditionedness of X⊤X + λIpp. A matrix is ill- conditioned if its condition number is high. The condition number of a square positive definite matrix A is the ratio of its largest and smallest eigenvalue. If the smallest eigenvalue is zero, the conditional number is unde- fined and so is A−1. Furthermore, a high condition number is indicative of the loss (on a log-scale) in numerical accuracy of the evaluation of A−1. To ensure the numerical accurate evaluation of the ridge regression estimator, the choice of the penalty parameter is thus to be restricted to a subset of the positive real numbers such that it yields a well-conditioned matrix X⊤X + λIpp. Clearly, the penalty parameter λ should then not be too close to zero. There is however no consensus on the criteria on the condition number for a matrix to be well-defined. This depends among others on how much numerical inaccuracy is tolerated by the context. Of course, as pointed out in Section 1.7, inversion of the X⊤X + λIpp matrix can be circumvented. One then still needs to ensure the well- conditionedness of λ−1XX⊤+ Inn, which too results in a lower bound for the penalty parameter. Practically, following Peeters et al. (2019) (who do so in a different context), we suggest to generate a conditional number plot. It plots (on some convenient scale) the condition number of the matrix X⊤X+λIpp against the penalty parameter λ. From this plot, we identify the domain of the penalty parameter value associated with well-conditionedness. To guide in this choice, Peeters et al. (2019) overlay this plot with a curve indicative of the numerical inaccurary. Traditional text books on regression analysis suggest, in order to prevent over-fitting, to limit the number of covariates of the model. This ought to leave enough degrees of freedom to estimate the error and facilitate proper inference. While ridge regression is commonly used for prediction purposes and inference need not be the objective, over-fitting is certainly to be avoided. This can be achieved by limiting the degrees of freedom spent on the estimation of the regression parameter (Harrell, 2001). We thus follow Saleh et al. (2019), who illustrate this in the ridge regression context, and use the degrees of freedom to bound the search domain of the penalty parameter. This requires the specification of a maximum degrees of freedom, denoted by ν with 0 ≤ν ≤n, one wishes to spend on the construction of the ridge regression estimator. Now choose λ such that df[ ˆβ(λ)] ≤ν. To find the bound on the penalty parameter, note that df[ ˆβ(λ)] = tr[H(λ)]",
        "metadata": {
          "authors": "Wessel N. van Wieringen",
          "section": "model",
          "paper_id": "1509.09169v8",
          "chunk_index": 5,
          "paper_title": "Lecture notes on ridge regression"
        },
        "id": "chunk_520"
      }
    ]
  }
}